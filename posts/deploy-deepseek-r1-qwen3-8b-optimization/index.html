<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>DeepSeek-R1-0528-Qwen3-8B部署优化实践 | Code Whispers</title><meta name=keywords content="LLM,DeepSeek,Qwen3,vLLM,性能优化,Docker"><meta name=description content="DeepSeek-R1-0528-Qwen3-8B部署优化实践：性能与稳定性的平衡艺术
在AI大模型部署领域，本文详细记录对DeepSeek-R1-0528-Qwen3-8B模型使用vLLM进行部署优化的全过程，重点关注上下文窗口长度与硬件资源利用的平衡调优。
环境与基础设施
我们的部署环境具备以下配置：

GPU: 4 x NVIDIA RTX 2080 Ti（每张22GB显存，总计88GB显存）

架构: Turing
计算能力: 7.5


CPU: 56核
内存: 512GB RAM
存储: 2TB SSD
操作系统: Ubuntu 24.04
容器镜像: vllm/vllm-openai:v0.8.5
NVIDIA驱动: 570.153.02（CUDA 12.8）

优化前的部署脚本分析
我们最初的部署脚本如下：
docker run \
  -d \
  --gpus all \
  --name coder \
  --shm-size 16g \
  --ulimit memlock=-1 \
  --restart always \
  --ipc=host \
  -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models \
  -p 8000:8000 \
  -e CUDA_MODULE_LOADING=LAZY \
  vllm/vllm-openai:v0.8.5 \
  --model /models \
  --served-model-name coder \
  --tensor-parallel-size 4 \
  --gpu-memory-utilization 0.93 \
  --dtype float16 \
  --max-model-len 65536 \
  --trust-remote-code \
  --load-format safetensors \
  --disable-custom-all-reduce
通过分析，我们发现几个可以优化的关键点："><meta name=author content="Jacky Pan"><link rel=canonical href=https://jackypanster.github.io/ai-stream/posts/deploy-deepseek-r1-qwen3-8b-optimization/><link crossorigin=anonymous href=/ai-stream/assets/css/stylesheet.93f625d739f1d6a5c6f20c146bc6a8d26b233492b34b2220c54b12fd46a04ded.css integrity="sha256-k/Yl1znx1qXG8gwUa8ao0msjNJKzSyIgxUsS/UagTe0=" rel="preload stylesheet" as=style><link rel=icon href=https://jackypanster.github.io/ai-stream/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://jackypanster.github.io/ai-stream/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://jackypanster.github.io/ai-stream/favicon-32x32.png><link rel=apple-touch-icon href=https://jackypanster.github.io/ai-stream/apple-touch-icon.png><link rel=mask-icon href=https://jackypanster.github.io/ai-stream/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://jackypanster.github.io/ai-stream/posts/deploy-deepseek-r1-qwen3-8b-optimization/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://jackypanster.github.io/ai-stream/posts/deploy-deepseek-r1-qwen3-8b-optimization/"><meta property="og:site_name" content="Code Whispers"><meta property="og:title" content="DeepSeek-R1-0528-Qwen3-8B部署优化实践"><meta property="og:description" content="DeepSeek-R1-0528-Qwen3-8B部署优化实践：性能与稳定性的平衡艺术 在AI大模型部署领域，本文详细记录对DeepSeek-R1-0528-Qwen3-8B模型使用vLLM进行部署优化的全过程，重点关注上下文窗口长度与硬件资源利用的平衡调优。
环境与基础设施 我们的部署环境具备以下配置：
GPU: 4 x NVIDIA RTX 2080 Ti（每张22GB显存，总计88GB显存） 架构: Turing 计算能力: 7.5 CPU: 56核 内存: 512GB RAM 存储: 2TB SSD 操作系统: Ubuntu 24.04 容器镜像: vllm/vllm-openai:v0.8.5 NVIDIA驱动: 570.153.02（CUDA 12.8） 优化前的部署脚本分析 我们最初的部署脚本如下：
docker run \ -d \ --gpus all \ --name coder \ --shm-size 16g \ --ulimit memlock=-1 \ --restart always \ --ipc=host \ -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models \ -p 8000:8000 \ -e CUDA_MODULE_LOADING=LAZY \ vllm/vllm-openai:v0.8.5 \ --model /models \ --served-model-name coder \ --tensor-parallel-size 4 \ --gpu-memory-utilization 0.93 \ --dtype float16 \ --max-model-len 65536 \ --trust-remote-code \ --load-format safetensors \ --disable-custom-all-reduce 通过分析，我们发现几个可以优化的关键点："><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-07T17:50:00+08:00"><meta property="article:modified_time" content="2025-06-07T17:50:00+08:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="DeepSeek"><meta property="article:tag" content="Qwen3"><meta property="article:tag" content="VLLM"><meta property="article:tag" content="性能优化"><meta property="article:tag" content="Docker"><meta name=twitter:card content="summary"><meta name=twitter:title content="DeepSeek-R1-0528-Qwen3-8B部署优化实践"><meta name=twitter:description content="DeepSeek-R1-0528-Qwen3-8B部署优化实践：性能与稳定性的平衡艺术
在AI大模型部署领域，本文详细记录对DeepSeek-R1-0528-Qwen3-8B模型使用vLLM进行部署优化的全过程，重点关注上下文窗口长度与硬件资源利用的平衡调优。
环境与基础设施
我们的部署环境具备以下配置：

GPU: 4 x NVIDIA RTX 2080 Ti（每张22GB显存，总计88GB显存）

架构: Turing
计算能力: 7.5


CPU: 56核
内存: 512GB RAM
存储: 2TB SSD
操作系统: Ubuntu 24.04
容器镜像: vllm/vllm-openai:v0.8.5
NVIDIA驱动: 570.153.02（CUDA 12.8）

优化前的部署脚本分析
我们最初的部署脚本如下：
docker run \
  -d \
  --gpus all \
  --name coder \
  --shm-size 16g \
  --ulimit memlock=-1 \
  --restart always \
  --ipc=host \
  -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models \
  -p 8000:8000 \
  -e CUDA_MODULE_LOADING=LAZY \
  vllm/vllm-openai:v0.8.5 \
  --model /models \
  --served-model-name coder \
  --tensor-parallel-size 4 \
  --gpu-memory-utilization 0.93 \
  --dtype float16 \
  --max-model-len 65536 \
  --trust-remote-code \
  --load-format safetensors \
  --disable-custom-all-reduce
通过分析，我们发现几个可以优化的关键点："><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://jackypanster.github.io/ai-stream/posts/"},{"@type":"ListItem","position":2,"name":"DeepSeek-R1-0528-Qwen3-8B部署优化实践","item":"https://jackypanster.github.io/ai-stream/posts/deploy-deepseek-r1-qwen3-8b-optimization/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"DeepSeek-R1-0528-Qwen3-8B部署优化实践","name":"DeepSeek-R1-0528-Qwen3-8B部署优化实践","description":"DeepSeek-R1-0528-Qwen3-8B部署优化实践：性能与稳定性的平衡艺术 在AI大模型部署领域，本文详细记录对DeepSeek-R1-0528-Qwen3-8B模型使用vLLM进行部署优化的全过程，重点关注上下文窗口长度与硬件资源利用的平衡调优。\n环境与基础设施 我们的部署环境具备以下配置：\nGPU: 4 x NVIDIA RTX 2080 Ti（每张22GB显存，总计88GB显存） 架构: Turing 计算能力: 7.5 CPU: 56核 内存: 512GB RAM 存储: 2TB SSD 操作系统: Ubuntu 24.04 容器镜像: vllm/vllm-openai:v0.8.5 NVIDIA驱动: 570.153.02（CUDA 12.8） 优化前的部署脚本分析 我们最初的部署脚本如下：\ndocker run \\ -d \\ --gpus all \\ --name coder \\ --shm-size 16g \\ --ulimit memlock=-1 \\ --restart always \\ --ipc=host \\ -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models \\ -p 8000:8000 \\ -e CUDA_MODULE_LOADING=LAZY \\ vllm/vllm-openai:v0.8.5 \\ --model /models \\ --served-model-name coder \\ --tensor-parallel-size 4 \\ --gpu-memory-utilization 0.93 \\ --dtype float16 \\ --max-model-len 65536 \\ --trust-remote-code \\ --load-format safetensors \\ --disable-custom-all-reduce 通过分析，我们发现几个可以优化的关键点：\n","keywords":["LLM","DeepSeek","Qwen3","vLLM","性能优化","Docker"],"articleBody":"DeepSeek-R1-0528-Qwen3-8B部署优化实践：性能与稳定性的平衡艺术 在AI大模型部署领域，本文详细记录对DeepSeek-R1-0528-Qwen3-8B模型使用vLLM进行部署优化的全过程，重点关注上下文窗口长度与硬件资源利用的平衡调优。\n环境与基础设施 我们的部署环境具备以下配置：\nGPU: 4 x NVIDIA RTX 2080 Ti（每张22GB显存，总计88GB显存） 架构: Turing 计算能力: 7.5 CPU: 56核 内存: 512GB RAM 存储: 2TB SSD 操作系统: Ubuntu 24.04 容器镜像: vllm/vllm-openai:v0.8.5 NVIDIA驱动: 570.153.02（CUDA 12.8） 优化前的部署脚本分析 我们最初的部署脚本如下：\ndocker run \\ -d \\ --gpus all \\ --name coder \\ --shm-size 16g \\ --ulimit memlock=-1 \\ --restart always \\ --ipc=host \\ -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models \\ -p 8000:8000 \\ -e CUDA_MODULE_LOADING=LAZY \\ vllm/vllm-openai:v0.8.5 \\ --model /models \\ --served-model-name coder \\ --tensor-parallel-size 4 \\ --gpu-memory-utilization 0.93 \\ --dtype float16 \\ --max-model-len 65536 \\ --trust-remote-code \\ --load-format safetensors \\ --disable-custom-all-reduce 通过分析，我们发现几个可以优化的关键点：\n共享内存：16GB可能不足以支持高并发请求 交换空间：未配置SSD交换空间支持 批处理能力：未设置--max-num-batched-tokens参数 CUDA图形优化：未使用--enforce-eager提高稳定性 深入优化策略 1. 内存与计算资源分配 对于RTX 2080 Ti这类Turing架构GPU，我们需要特别注意显存分配与并行策略：\n共享内存扩展：将--shm-size从16g增加到64g，充分利用512GB系统内存 显存利用率：维持--gpu-memory-utilization 0.93的激进但可控设置 张量并行化：保持--tensor-parallel-size 4充分利用所有GPU 批处理支持：添加--max-num-batched-tokens 8192提高吞吐量 2. 稳定性与效率平衡 CUDA执行模式：添加--enforce-eager参数，避免CUDA图捕获可能导致的OOM问题 交换空间支持：添加--swap-space 32参数，为处理长上下文提供额外内存保障 all-reduce优化：移除--disable-custom-all-reduce参数（注：日志显示系统自动禁用） 3. 上下文长度设计 虽然我们最终保留了--max-model-len 65536设置，但在生产环境中应当根据具体使用场景和稳定性需求考虑降至32768。对于大多数应用场景，这个长度已经足够，并且能提供更好的性能和稳定性平衡。\n优化后的部署脚本 经过一系列优化，我们的最终部署脚本如下：\ndocker run \\ -d \\ --gpus all \\ --name coder \\ --shm-size 64g \\ --ulimit memlock=-1 \\ --restart always \\ --ipc=host \\ -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models \\ -p 8000:8000 \\ -e CUDA_MODULE_LOADING=LAZY \\ vllm/vllm-openai:v0.8.5 \\ --model /models \\ --served-model-name coder \\ --tensor-parallel-size 4 \\ --gpu-memory-utilization 0.93 \\ --dtype float16 \\ --max-model-len 65536 \\ --trust-remote-code \\ --load-format safetensors \\ --swap-space 32 \\ --enforce-eager \\ --max-num-batched-tokens 8192 \\ --chat-template /models/qwen3_programming.jinja 性能与资源分析 部署后，通过日志分析我们得到以下性能指标：\nMemory profiling takes 5.76 seconds the current vLLM instance can use total_gpu_memory (21.48GiB) x gpu_memory_utilization (0.93) = 19.98GiB model weights take 3.85GiB; non_torch_memory takes 0.20GiB; PyTorch activation peak memory takes 1.45GiB; the rest of the memory reserved for KV Cache is 14.49GiB. 关键性能发现：\nKV缓存空间：14.49GiB，足够支持65536 token的上下文处理 最大并发能力：可同时处理约6.44个最大长度（65536 tokens）的请求 初始化时间：31.86秒，相比未优化配置有所改善 实用部署建议 根据我们的实践经验，提供以下部署建议：\n上下文长度选择\n对于追求稳定性的生产环境：使用--max-model-len 32768 对于需要极限性能的场景：可尝试--max-model-len 65536但需密切监控稳定性 显存利用率调优\n稳定性优先：--gpu-memory-utilization 0.9 性能优先：--gpu-memory-utilization 0.93或更高（需谨慎） 批处理参数优化\n对于多用户场景：增加--max-num-batched-tokens至8192或更高 对于单一复杂任务：可适当降低此参数，专注单任务性能 硬件资源分配\n共享内存与系统内存比例：建议1:8左右（如512GB系统内存配置64GB共享内存） 交换空间设置：根据SSD速度和容量，可设置为显存总量的1/3至1/2 排障与验证 每次修改配置后，通过以下命令验证部署状态：\ncurl http://localhost:8000/v1/models 验证结果显示模型已成功部署，并返回了以下实际输出：\n{ \"object\": \"list\", \"data\": [ { \"id\": \"coder\", \"object\": \"model\", \"created\": 1749289780, \"owned_by\": \"vllm\", \"root\": \"/models\", \"parent\": null, \"max_model_len\": 65536, \"permission\": [ { \"id\": \"modelperm-ee339bc1702c402f8ae06ea2f1b05c7c\", \"object\": \"model_permission\", \"created\": 1749289780, \"allow_create_engine\": false, \"allow_sampling\": true, \"allow_logprobs\": true, \"allow_search_indices\": false, \"allow_view\": true, \"allow_fine_tuning\": false, \"organization\": \"*\", \"group\": null, \"is_blocking\": false } ] } ] } 从返回的JSON响应中，我们可以确认模型部署成功并解读以下关键信息：\nid: “coder” - 确认我们的模型服务名称已正确设置 max_model_len: 65536 - 验证了我们设置的上下文窗口长度为65536 tokens owned_by: “vllm” - 表明模型由vLLM服务管理 permission对象中： allow_sampling: true - 支持采样生成（temperature、top_p等参数） allow_logprobs: true - 支持输出token概率 organization: “*” - 允许所有组织访问模型 这些参数确认了我们的部署配置已经正确应用，且模型服务已准备好接收推理请求。\n专用编程提示词模板 由于DeepSeek-R1-0528-Qwen3-8B模型特别适合编程任务，我们在部署中加入了专门的提示词模板来优化其编程能力。我们已经通过--chat-template参数指定了模板路径，模板内容如下：\n{# Enhanced template for Qwen3 optimized for programming tasks #} {% if messages[0]['role'] == 'system' %} {% set loop_messages = messages[1:] %} {% set system_message = messages[0]['content'] %} {% else %} {% set loop_messages = messages %} {% set system_message = \"You are a programming assistant specialized in writing clean, efficient, and well-documented code. Provide direct code solutions without unnecessary explanations unless requested. Focus on best practices, optimal algorithms, and proper error handling. When multiple approaches exist, choose the most efficient one by default. Always include necessary imports and dependencies.\" %} {% endif %} {# Always include system message for programming optimization #} \u003c|im_start|\u003esystem {{ system_message }}\u003c|im_end|\u003e {% for message in loop_messages %} {% if message['role'] == 'user' %} \u003c|im_start|\u003euser {{ message['content'] }}\u003c|im_end|\u003e {% elif message['role'] == 'assistant' %} \u003c|im_start|\u003eassistant {{ message['content'] }}\u003c|im_end|\u003e {% elif message['role'] == 'tool' %} \u003c|im_start|\u003etool {{ message['content'] }}\u003c|im_end|\u003e {% else %} \u003c|im_start|\u003e{{ message['role'] }} {{ message['content'] }}\u003c|im_end|\u003e {% endif %} {% endfor %} {% if add_generation_prompt %} \u003c|im_start|\u003eassistant {% endif %} 此模板具有以下特性：\n专业编程指令：默认系统提示词专门针对编程任务优化，强调代码质量、效率和文档 直接输出：倾向于直接提供代码解决方案，减少不必要的解释（除非特别要求） 标准化格式：使用\u003c|im_start|\u003e和\u003c|im_end|\u003e标记清晰界定不同角色的消息 灵活性：允许覆盖默认系统提示词，以适应特定编程场景 在实际使用中，可以将该模板与vLLM的API调用结合，例如：\nimport requests url = \"http://localhost:8000/v1/chat/completions\" headers = {\"Content-Type\": \"application/json\"} payload = { \"model\": \"coder\", \"messages\": [ {\"role\": \"user\", \"content\": \"写一个Python函数计算斐波那契数列的第n项，要求使用动态规划优化性能\"} ], \"temperature\": 0.2, \"response_format\": {\"type\": \"text\"} } response = requests.post(url, headers=headers, json=payload) print(response.json()) 通过这种方式，我们可以充分发挥模型在编程领域的专长，获得更高质量、更符合工程实践的代码输出。\n结论与未来方向 通过精心调整vLLM参数，我们成功实现了DeepSeek-R1-0528-Qwen3-8B模型的高效部署，在有限的RTX 2080 Ti显卡上实现了最大化的性能和上下文长度。\n未来的优化方向可以探索：\n进一步量化研究：探索int8量化对性能和质量的影响 调度策略优化：通过--scheduler-delay-factor和--preemption-mode参数优化多用户场景 自动扩缩容方案：根据负载动态调整GPU分配 希望这份部署优化实践能为更多工程师提供参考，在大模型部署中找到性能与稳定性的最佳平衡点。\n参考资料 vLLM官方文档 Qwen3系列模型说明 DeepSeek R1模型系列介绍 ","wordCount":"561","inLanguage":"en","datePublished":"2025-06-07T17:50:00+08:00","dateModified":"2025-06-07T17:50:00+08:00","author":{"@type":"Person","name":"Jacky Pan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://jackypanster.github.io/ai-stream/posts/deploy-deepseek-r1-qwen3-8b-optimization/"},"publisher":{"@type":"Organization","name":"Code Whispers","logo":{"@type":"ImageObject","url":"https://jackypanster.github.io/ai-stream/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://jackypanster.github.io/ai-stream/ accesskey=h title="Code Whispers (Alt + H)">Code Whispers</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://jackypanster.github.io/ai-stream/categories/ title=分类><span>分类</span></a></li><li><a href=https://jackypanster.github.io/ai-stream/tags/ title=标签><span>标签</span></a></li><li><a href=https://jackypanster.github.io/ai-stream/search/ title="搜索 (Alt + /)" accesskey=/><span>搜索</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://jackypanster.github.io/ai-stream/>Home</a>&nbsp;»&nbsp;<a href=https://jackypanster.github.io/ai-stream/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">DeepSeek-R1-0528-Qwen3-8B部署优化实践</h1><div class=post-meta><span title='2025-06-07 17:50:00 +0800 +0800'>June 7, 2025</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;561 words&nbsp;·&nbsp;Jacky Pan</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#deepseek-r1-0528-qwen3-8b%e9%83%a8%e7%bd%b2%e4%bc%98%e5%8c%96%e5%ae%9e%e8%b7%b5%e6%80%a7%e8%83%bd%e4%b8%8e%e7%a8%b3%e5%ae%9a%e6%80%a7%e7%9a%84%e5%b9%b3%e8%a1%a1%e8%89%ba%e6%9c%af aria-label=DeepSeek-R1-0528-Qwen3-8B部署优化实践：性能与稳定性的平衡艺术>DeepSeek-R1-0528-Qwen3-8B部署优化实践：性能与稳定性的平衡艺术</a><ul><li><a href=#%e7%8e%af%e5%a2%83%e4%b8%8e%e5%9f%ba%e7%a1%80%e8%ae%be%e6%96%bd aria-label=环境与基础设施>环境与基础设施</a></li><li><a href=#%e4%bc%98%e5%8c%96%e5%89%8d%e7%9a%84%e9%83%a8%e7%bd%b2%e8%84%9a%e6%9c%ac%e5%88%86%e6%9e%90 aria-label=优化前的部署脚本分析>优化前的部署脚本分析</a></li><li><a href=#%e6%b7%b1%e5%85%a5%e4%bc%98%e5%8c%96%e7%ad%96%e7%95%a5 aria-label=深入优化策略>深入优化策略</a><ul><li><a href=#1-%e5%86%85%e5%ad%98%e4%b8%8e%e8%ae%a1%e7%ae%97%e8%b5%84%e6%ba%90%e5%88%86%e9%85%8d aria-label="1. 内存与计算资源分配">1. 内存与计算资源分配</a></li><li><a href=#2-%e7%a8%b3%e5%ae%9a%e6%80%a7%e4%b8%8e%e6%95%88%e7%8e%87%e5%b9%b3%e8%a1%a1 aria-label="2. 稳定性与效率平衡">2. 稳定性与效率平衡</a></li><li><a href=#3-%e4%b8%8a%e4%b8%8b%e6%96%87%e9%95%bf%e5%ba%a6%e8%ae%be%e8%ae%a1 aria-label="3. 上下文长度设计">3. 上下文长度设计</a></li></ul></li><li><a href=#%e4%bc%98%e5%8c%96%e5%90%8e%e7%9a%84%e9%83%a8%e7%bd%b2%e8%84%9a%e6%9c%ac aria-label=优化后的部署脚本>优化后的部署脚本</a></li><li><a href=#%e6%80%a7%e8%83%bd%e4%b8%8e%e8%b5%84%e6%ba%90%e5%88%86%e6%9e%90 aria-label=性能与资源分析>性能与资源分析</a></li><li><a href=#%e5%ae%9e%e7%94%a8%e9%83%a8%e7%bd%b2%e5%bb%ba%e8%ae%ae aria-label=实用部署建议>实用部署建议</a></li><li><a href=#%e6%8e%92%e9%9a%9c%e4%b8%8e%e9%aa%8c%e8%af%81 aria-label=排障与验证>排障与验证</a></li><li><a href=#%e4%b8%93%e7%94%a8%e7%bc%96%e7%a8%8b%e6%8f%90%e7%a4%ba%e8%af%8d%e6%a8%a1%e6%9d%bf aria-label=专用编程提示词模板>专用编程提示词模板</a></li><li><a href=#%e7%bb%93%e8%ae%ba%e4%b8%8e%e6%9c%aa%e6%9d%a5%e6%96%b9%e5%90%91 aria-label=结论与未来方向>结论与未来方向</a></li><li><a href=#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99 aria-label=参考资料>参考资料</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=deepseek-r1-0528-qwen3-8b部署优化实践性能与稳定性的平衡艺术>DeepSeek-R1-0528-Qwen3-8B部署优化实践：性能与稳定性的平衡艺术<a hidden class=anchor aria-hidden=true href=#deepseek-r1-0528-qwen3-8b部署优化实践性能与稳定性的平衡艺术>#</a></h1><p>在AI大模型部署领域，本文详细记录对DeepSeek-R1-0528-Qwen3-8B模型使用vLLM进行部署优化的全过程，重点关注上下文窗口长度与硬件资源利用的平衡调优。</p><h2 id=环境与基础设施>环境与基础设施<a hidden class=anchor aria-hidden=true href=#环境与基础设施>#</a></h2><p>我们的部署环境具备以下配置：</p><ul><li><strong>GPU</strong>: 4 x NVIDIA RTX 2080 Ti（每张22GB显存，总计88GB显存）<ul><li>架构: Turing</li><li>计算能力: 7.5</li></ul></li><li><strong>CPU</strong>: 56核</li><li><strong>内存</strong>: 512GB RAM</li><li><strong>存储</strong>: 2TB SSD</li><li><strong>操作系统</strong>: Ubuntu 24.04</li><li><strong>容器镜像</strong>: <code>vllm/vllm-openai:v0.8.5</code></li><li><strong>NVIDIA驱动</strong>: 570.153.02（CUDA 12.8）</li></ul><h2 id=优化前的部署脚本分析>优化前的部署脚本分析<a hidden class=anchor aria-hidden=true href=#优化前的部署脚本分析>#</a></h2><p>我们最初的部署脚本如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -d <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --gpus all <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --name coder <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --shm-size 16g <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --ulimit memlock<span style=color:#f92672>=</span>-1 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --restart always <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --ipc<span style=color:#f92672>=</span>host <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -p 8000:8000 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -e CUDA_MODULE_LOADING<span style=color:#f92672>=</span>LAZY <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  vllm/vllm-openai:v0.8.5 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --model /models <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --served-model-name coder <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --tensor-parallel-size <span style=color:#ae81ff>4</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --gpu-memory-utilization 0.93 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --dtype float16 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --max-model-len <span style=color:#ae81ff>65536</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --trust-remote-code <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --load-format safetensors <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --disable-custom-all-reduce
</span></span></code></pre></div><p>通过分析，我们发现几个可以优化的关键点：</p><ol><li><strong>共享内存</strong>：16GB可能不足以支持高并发请求</li><li><strong>交换空间</strong>：未配置SSD交换空间支持</li><li><strong>批处理能力</strong>：未设置<code>--max-num-batched-tokens</code>参数</li><li><strong>CUDA图形优化</strong>：未使用<code>--enforce-eager</code>提高稳定性</li></ol><h2 id=深入优化策略>深入优化策略<a hidden class=anchor aria-hidden=true href=#深入优化策略>#</a></h2><h3 id=1-内存与计算资源分配>1. 内存与计算资源分配<a hidden class=anchor aria-hidden=true href=#1-内存与计算资源分配>#</a></h3><p>对于RTX 2080 Ti这类Turing架构GPU，我们需要特别注意显存分配与并行策略：</p><ul><li><strong>共享内存扩展</strong>：将<code>--shm-size</code>从16g增加到64g，充分利用512GB系统内存</li><li><strong>显存利用率</strong>：维持<code>--gpu-memory-utilization 0.93</code>的激进但可控设置</li><li><strong>张量并行化</strong>：保持<code>--tensor-parallel-size 4</code>充分利用所有GPU</li><li><strong>批处理支持</strong>：添加<code>--max-num-batched-tokens 8192</code>提高吞吐量</li></ul><h3 id=2-稳定性与效率平衡>2. 稳定性与效率平衡<a hidden class=anchor aria-hidden=true href=#2-稳定性与效率平衡>#</a></h3><ul><li><strong>CUDA执行模式</strong>：添加<code>--enforce-eager</code>参数，避免CUDA图捕获可能导致的OOM问题</li><li><strong>交换空间支持</strong>：添加<code>--swap-space 32</code>参数，为处理长上下文提供额外内存保障</li><li><strong>all-reduce优化</strong>：移除<code>--disable-custom-all-reduce</code>参数（注：日志显示系统自动禁用）</li></ul><h3 id=3-上下文长度设计>3. 上下文长度设计<a hidden class=anchor aria-hidden=true href=#3-上下文长度设计>#</a></h3><p>虽然我们最终保留了<code>--max-model-len 65536</code>设置，但在生产环境中应当根据具体使用场景和稳定性需求考虑降至32768。对于大多数应用场景，这个长度已经足够，并且能提供更好的性能和稳定性平衡。</p><h2 id=优化后的部署脚本>优化后的部署脚本<a hidden class=anchor aria-hidden=true href=#优化后的部署脚本>#</a></h2><p>经过一系列优化，我们的最终部署脚本如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -d <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --gpus all <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --name coder <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --shm-size 64g <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --ulimit memlock<span style=color:#f92672>=</span>-1 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --restart always <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --ipc<span style=color:#f92672>=</span>host <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -p 8000:8000 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -e CUDA_MODULE_LOADING<span style=color:#f92672>=</span>LAZY <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  vllm/vllm-openai:v0.8.5 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --model /models <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --served-model-name coder <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --tensor-parallel-size <span style=color:#ae81ff>4</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --gpu-memory-utilization 0.93 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --dtype float16 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --max-model-len <span style=color:#ae81ff>65536</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --trust-remote-code <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --load-format safetensors <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --swap-space <span style=color:#ae81ff>32</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --enforce-eager <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --max-num-batched-tokens <span style=color:#ae81ff>8192</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --chat-template /models/qwen3_programming.jinja
</span></span></code></pre></div><h2 id=性能与资源分析>性能与资源分析<a hidden class=anchor aria-hidden=true href=#性能与资源分析>#</a></h2><p>部署后，通过日志分析我们得到以下性能指标：</p><pre tabindex=0><code>Memory profiling takes 5.76 seconds
the current vLLM instance can use total_gpu_memory (21.48GiB) x gpu_memory_utilization (0.93) = 19.98GiB
model weights take 3.85GiB; non_torch_memory takes 0.20GiB; PyTorch activation peak memory takes 1.45GiB; the rest of the memory reserved for KV Cache is 14.49GiB.
</code></pre><p>关键性能发现：</p><ul><li><strong>KV缓存空间</strong>：14.49GiB，足够支持65536 token的上下文处理</li><li><strong>最大并发能力</strong>：可同时处理约6.44个最大长度（65536 tokens）的请求</li><li><strong>初始化时间</strong>：31.86秒，相比未优化配置有所改善</li></ul><h2 id=实用部署建议>实用部署建议<a hidden class=anchor aria-hidden=true href=#实用部署建议>#</a></h2><p>根据我们的实践经验，提供以下部署建议：</p><ol><li><p><strong>上下文长度选择</strong></p><ul><li>对于追求稳定性的生产环境：使用<code>--max-model-len 32768</code></li><li>对于需要极限性能的场景：可尝试<code>--max-model-len 65536</code>但需密切监控稳定性</li></ul></li><li><p><strong>显存利用率调优</strong></p><ul><li>稳定性优先：<code>--gpu-memory-utilization 0.9</code></li><li>性能优先：<code>--gpu-memory-utilization 0.93</code>或更高（需谨慎）</li></ul></li><li><p><strong>批处理参数优化</strong></p><ul><li>对于多用户场景：增加<code>--max-num-batched-tokens</code>至8192或更高</li><li>对于单一复杂任务：可适当降低此参数，专注单任务性能</li></ul></li><li><p><strong>硬件资源分配</strong></p><ul><li>共享内存与系统内存比例：建议1:8左右（如512GB系统内存配置64GB共享内存）</li><li>交换空间设置：根据SSD速度和容量，可设置为显存总量的1/3至1/2</li></ul></li></ol><h2 id=排障与验证>排障与验证<a hidden class=anchor aria-hidden=true href=#排障与验证>#</a></h2><p>每次修改配置后，通过以下命令验证部署状态：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl http://localhost:8000/v1/models
</span></span></code></pre></div><p>验证结果显示模型已成功部署，并返回了以下实际输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;object&#34;</span>: <span style=color:#e6db74>&#34;list&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;data&#34;</span>: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#e6db74>&#34;coder&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;object&#34;</span>: <span style=color:#e6db74>&#34;model&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;created&#34;</span>: <span style=color:#ae81ff>1749289780</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;owned_by&#34;</span>: <span style=color:#e6db74>&#34;vllm&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;root&#34;</span>: <span style=color:#e6db74>&#34;/models&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;parent&#34;</span>: <span style=color:#66d9ef>null</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;max_model_len&#34;</span>: <span style=color:#ae81ff>65536</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;permission&#34;</span>: [
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#e6db74>&#34;modelperm-ee339bc1702c402f8ae06ea2f1b05c7c&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;object&#34;</span>: <span style=color:#e6db74>&#34;model_permission&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;created&#34;</span>: <span style=color:#ae81ff>1749289780</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;allow_create_engine&#34;</span>: <span style=color:#66d9ef>false</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;allow_sampling&#34;</span>: <span style=color:#66d9ef>true</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;allow_logprobs&#34;</span>: <span style=color:#66d9ef>true</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;allow_search_indices&#34;</span>: <span style=color:#66d9ef>false</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;allow_view&#34;</span>: <span style=color:#66d9ef>true</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;allow_fine_tuning&#34;</span>: <span style=color:#66d9ef>false</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;organization&#34;</span>: <span style=color:#e6db74>&#34;*&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;group&#34;</span>: <span style=color:#66d9ef>null</span>,
</span></span><span style=display:flex><span>          <span style=color:#f92672>&#34;is_blocking&#34;</span>: <span style=color:#66d9ef>false</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>      ]
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>从返回的JSON响应中，我们可以确认模型部署成功并解读以下关键信息：</p><ul><li><strong>id</strong>: &ldquo;coder&rdquo; - 确认我们的模型服务名称已正确设置</li><li><strong>max_model_len</strong>: 65536 - 验证了我们设置的上下文窗口长度为65536 tokens</li><li><strong>owned_by</strong>: &ldquo;vllm&rdquo; - 表明模型由vLLM服务管理</li><li><strong>permission</strong>对象中：<ul><li><strong>allow_sampling</strong>: true - 支持采样生成（temperature、top_p等参数）</li><li><strong>allow_logprobs</strong>: true - 支持输出token概率</li><li><strong>organization</strong>: &ldquo;*&rdquo; - 允许所有组织访问模型</li></ul></li></ul><p>这些参数确认了我们的部署配置已经正确应用，且模型服务已准备好接收推理请求。</p><h2 id=专用编程提示词模板>专用编程提示词模板<a hidden class=anchor aria-hidden=true href=#专用编程提示词模板>#</a></h2><p>由于DeepSeek-R1-0528-Qwen3-8B模型特别适合编程任务，我们在部署中加入了专门的提示词模板来优化其编程能力。我们已经通过<code>--chat-template</code>参数指定了模板路径，模板内容如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-jinja data-lang=jinja><span style=display:flex><span><span style=color:#75715e>{# Enhanced template for Qwen3 optimized for programming tasks #}</span>
</span></span><span style=display:flex><span><span style=color:#75715e>{%</span> <span style=color:#66d9ef>if</span> messages<span style=color:#f92672>[</span><span style=color:#ae81ff>0</span><span style=color:#f92672>][</span><span style=color:#e6db74>&#39;role&#39;</span><span style=color:#f92672>]</span> <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;system&#39;</span> <span style=color:#75715e>%}</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>{%</span> <span style=color:#66d9ef>set</span> loop_messages <span style=color:#f92672>=</span> messages<span style=color:#f92672>[</span><span style=color:#ae81ff>1</span><span style=color:#f92672>:]</span> <span style=color:#75715e>%}</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>{%</span> <span style=color:#66d9ef>set</span> system_message <span style=color:#f92672>=</span> messages<span style=color:#f92672>[</span><span style=color:#ae81ff>0</span><span style=color:#f92672>][</span><span style=color:#e6db74>&#39;content&#39;</span><span style=color:#f92672>]</span> <span style=color:#75715e>%}</span>
</span></span><span style=display:flex><span><span style=color:#75715e>{%</span> <span style=color:#66d9ef>else</span> <span style=color:#75715e>%}</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>{%</span> <span style=color:#66d9ef>set</span> loop_messages <span style=color:#f92672>=</span> messages <span style=color:#75715e>%}</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>{%</span> <span style=color:#66d9ef>set</span> system_message <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;You are a programming assistant specialized in writing clean, efficient, and well-documented code. Provide direct code solutions without unnecessary explanations unless requested. Focus on best practices, optimal algorithms, and proper error handling. When multiple approaches exist, choose the most efficient one by default. Always include necessary imports and dependencies.&#34;</span> <span style=color:#75715e>%}</span>
</span></span><span style=display:flex><span><span style=color:#75715e>{%</span> <span style=color:#66d9ef>endif</span> <span style=color:#75715e>%}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>{# Always include system message for programming optimization #}</span>
</span></span><span style=display:flex><span>&lt;|im_start|&gt;system
</span></span><span style=display:flex><span><span style=color:#75715e>{{</span> system_message <span style=color:#75715e>}}</span>&lt;|im_end|&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>{%</span> <span style=color:#66d9ef>for</span> message <span style=color:#66d9ef>in</span> loop_messages <span style=color:#75715e>%}</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>{%</span> <span style=color:#66d9ef>if</span> message<span style=color:#f92672>[</span><span style=color:#e6db74>&#39;role&#39;</span><span style=color:#f92672>]</span> <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;user&#39;</span> <span style=color:#75715e>%}</span>
</span></span><span style=display:flex><span>&lt;|im_start|&gt;user
</span></span><span style=display:flex><span><span style=color:#75715e>{{</span> message<span style=color:#f92672>[</span><span style=color:#e6db74>&#39;content&#39;</span><span style=color:#f92672>]</span> <span style=color:#75715e>}}</span>&lt;|im_end|&gt;
</span></span><span style=display:flex><span>    <span style=color:#75715e>{%</span> <span style=color:#66d9ef>elif</span> message<span style=color:#f92672>[</span><span style=color:#e6db74>&#39;role&#39;</span><span style=color:#f92672>]</span> <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;assistant&#39;</span> <span style=color:#75715e>%}</span>
</span></span><span style=display:flex><span>&lt;|im_start|&gt;assistant
</span></span><span style=display:flex><span><span style=color:#75715e>{{</span> message<span style=color:#f92672>[</span><span style=color:#e6db74>&#39;content&#39;</span><span style=color:#f92672>]</span> <span style=color:#75715e>}}</span>&lt;|im_end|&gt;
</span></span><span style=display:flex><span>    <span style=color:#75715e>{%</span> <span style=color:#66d9ef>elif</span> message<span style=color:#f92672>[</span><span style=color:#e6db74>&#39;role&#39;</span><span style=color:#f92672>]</span> <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;tool&#39;</span> <span style=color:#75715e>%}</span>
</span></span><span style=display:flex><span>&lt;|im_start|&gt;tool
</span></span><span style=display:flex><span><span style=color:#75715e>{{</span> message<span style=color:#f92672>[</span><span style=color:#e6db74>&#39;content&#39;</span><span style=color:#f92672>]</span> <span style=color:#75715e>}}</span>&lt;|im_end|&gt;
</span></span><span style=display:flex><span>    <span style=color:#75715e>{%</span> <span style=color:#66d9ef>else</span> <span style=color:#75715e>%}</span>
</span></span><span style=display:flex><span>&lt;|im_start|&gt;<span style=color:#75715e>{{</span> message<span style=color:#f92672>[</span><span style=color:#e6db74>&#39;role&#39;</span><span style=color:#f92672>]</span> <span style=color:#75715e>}}</span>
</span></span><span style=display:flex><span><span style=color:#75715e>{{</span> message<span style=color:#f92672>[</span><span style=color:#e6db74>&#39;content&#39;</span><span style=color:#f92672>]</span> <span style=color:#75715e>}}</span>&lt;|im_end|&gt;
</span></span><span style=display:flex><span>    <span style=color:#75715e>{%</span> <span style=color:#66d9ef>endif</span> <span style=color:#75715e>%}</span>
</span></span><span style=display:flex><span><span style=color:#75715e>{%</span> <span style=color:#66d9ef>endfor</span> <span style=color:#75715e>%}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>{%</span> <span style=color:#66d9ef>if</span> add_generation_prompt <span style=color:#75715e>%}</span>
</span></span><span style=display:flex><span>&lt;|im_start|&gt;assistant
</span></span><span style=display:flex><span><span style=color:#75715e>{%</span> <span style=color:#66d9ef>endif</span> <span style=color:#75715e>%}</span>
</span></span></code></pre></div><p>此模板具有以下特性：</p><ol><li><strong>专业编程指令</strong>：默认系统提示词专门针对编程任务优化，强调代码质量、效率和文档</li><li><strong>直接输出</strong>：倾向于直接提供代码解决方案，减少不必要的解释（除非特别要求）</li><li><strong>标准化格式</strong>：使用<code>&lt;|im_start|></code>和<code>&lt;|im_end|></code>标记清晰界定不同角色的消息</li><li><strong>灵活性</strong>：允许覆盖默认系统提示词，以适应特定编程场景</li></ol><p>在实际使用中，可以将该模板与vLLM的API调用结合，例如：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> requests
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;http://localhost:8000/v1/chat/completions&#34;</span>
</span></span><span style=display:flex><span>headers <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;Content-Type&#34;</span>: <span style=color:#e6db74>&#34;application/json&#34;</span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>payload <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;model&#34;</span>: <span style=color:#e6db74>&#34;coder&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;messages&#34;</span>: [
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;写一个Python函数计算斐波那契数列的第n项，要求使用动态规划优化性能&#34;</span>}
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;temperature&#34;</span>: <span style=color:#ae81ff>0.2</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;response_format&#34;</span>: {<span style=color:#e6db74>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;text&#34;</span>}
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>response <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>post(url, headers<span style=color:#f92672>=</span>headers, json<span style=color:#f92672>=</span>payload)
</span></span><span style=display:flex><span>print(response<span style=color:#f92672>.</span>json())
</span></span></code></pre></div><p>通过这种方式，我们可以充分发挥模型在编程领域的专长，获得更高质量、更符合工程实践的代码输出。</p><h2 id=结论与未来方向>结论与未来方向<a hidden class=anchor aria-hidden=true href=#结论与未来方向>#</a></h2><p>通过精心调整vLLM参数，我们成功实现了DeepSeek-R1-0528-Qwen3-8B模型的高效部署，在有限的RTX 2080 Ti显卡上实现了最大化的性能和上下文长度。</p><p>未来的优化方向可以探索：</p><ol><li><strong>进一步量化研究</strong>：探索int8量化对性能和质量的影响</li><li><strong>调度策略优化</strong>：通过<code>--scheduler-delay-factor</code>和<code>--preemption-mode</code>参数优化多用户场景</li><li><strong>自动扩缩容方案</strong>：根据负载动态调整GPU分配</li></ol><p>希望这份部署优化实践能为更多工程师提供参考，在大模型部署中找到性能与稳定性的最佳平衡点。</p><h2 id=参考资料>参考资料<a hidden class=anchor aria-hidden=true href=#参考资料>#</a></h2><ol><li><a href=https://docs.vllm.ai/>vLLM官方文档</a></li><li><a href=https://github.com/QwenLM/Qwen>Qwen3系列模型说明</a></li><li><a href=https://github.com/deepseek-ai>DeepSeek R1模型系列介绍</a></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://jackypanster.github.io/ai-stream/tags/llm/>LLM</a></li><li><a href=https://jackypanster.github.io/ai-stream/tags/deepseek/>DeepSeek</a></li><li><a href=https://jackypanster.github.io/ai-stream/tags/qwen3/>Qwen3</a></li><li><a href=https://jackypanster.github.io/ai-stream/tags/vllm/>VLLM</a></li><li><a href=https://jackypanster.github.io/ai-stream/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/>性能优化</a></li><li><a href=https://jackypanster.github.io/ai-stream/tags/docker/>Docker</a></li></ul><nav class=paginav><a class=prev href=https://jackypanster.github.io/ai-stream/posts/apparmor-troubleshooting/><span class=title>« Prev</span><br><span>AppArmor配置残留问题排查与彻底解决：从报错到系统净化的完整实践</span>
</a><a class=next href=https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-part2/><span class=title>Next »</span><br><span>Qwen3-30B 技术优化实践（二）：思考模式控制与15-20%性能提升</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-R1-0528-Qwen3-8B部署优化实践 on x" href="https://x.com/intent/tweet/?text=DeepSeek-R1-0528-Qwen3-8B%e9%83%a8%e7%bd%b2%e4%bc%98%e5%8c%96%e5%ae%9e%e8%b7%b5&amp;url=https%3a%2f%2fjackypanster.github.io%2fai-stream%2fposts%2fdeploy-deepseek-r1-qwen3-8b-optimization%2f&amp;hashtags=LLM%2cDeepSeek%2cQwen3%2cvLLM%2c%e6%80%a7%e8%83%bd%e4%bc%98%e5%8c%96%2cDocker"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-R1-0528-Qwen3-8B部署优化实践 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjackypanster.github.io%2fai-stream%2fposts%2fdeploy-deepseek-r1-qwen3-8b-optimization%2f&amp;title=DeepSeek-R1-0528-Qwen3-8B%e9%83%a8%e7%bd%b2%e4%bc%98%e5%8c%96%e5%ae%9e%e8%b7%b5&amp;summary=DeepSeek-R1-0528-Qwen3-8B%e9%83%a8%e7%bd%b2%e4%bc%98%e5%8c%96%e5%ae%9e%e8%b7%b5&amp;source=https%3a%2f%2fjackypanster.github.io%2fai-stream%2fposts%2fdeploy-deepseek-r1-qwen3-8b-optimization%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-R1-0528-Qwen3-8B部署优化实践 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fjackypanster.github.io%2fai-stream%2fposts%2fdeploy-deepseek-r1-qwen3-8b-optimization%2f&title=DeepSeek-R1-0528-Qwen3-8B%e9%83%a8%e7%bd%b2%e4%bc%98%e5%8c%96%e5%ae%9e%e8%b7%b5"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-R1-0528-Qwen3-8B部署优化实践 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fjackypanster.github.io%2fai-stream%2fposts%2fdeploy-deepseek-r1-qwen3-8b-optimization%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-R1-0528-Qwen3-8B部署优化实践 on whatsapp" href="https://api.whatsapp.com/send?text=DeepSeek-R1-0528-Qwen3-8B%e9%83%a8%e7%bd%b2%e4%bc%98%e5%8c%96%e5%ae%9e%e8%b7%b5%20-%20https%3a%2f%2fjackypanster.github.io%2fai-stream%2fposts%2fdeploy-deepseek-r1-qwen3-8b-optimization%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-R1-0528-Qwen3-8B部署优化实践 on telegram" href="https://telegram.me/share/url?text=DeepSeek-R1-0528-Qwen3-8B%e9%83%a8%e7%bd%b2%e4%bc%98%e5%8c%96%e5%ae%9e%e8%b7%b5&amp;url=https%3a%2f%2fjackypanster.github.io%2fai-stream%2fposts%2fdeploy-deepseek-r1-qwen3-8b-optimization%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-R1-0528-Qwen3-8B部署优化实践 on ycombinator" href="https://news.ycombinator.com/submitlink?t=DeepSeek-R1-0528-Qwen3-8B%e9%83%a8%e7%bd%b2%e4%bc%98%e5%8c%96%e5%ae%9e%e8%b7%b5&u=https%3a%2f%2fjackypanster.github.io%2fai-stream%2fposts%2fdeploy-deepseek-r1-qwen3-8b-optimization%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://jackypanster.github.io/ai-stream/>Code Whispers</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>