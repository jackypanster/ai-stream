<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Posts on AI 避难所</title><link>https://jackypanster.github.io/ai-stream/posts/</link><description>Recent content in Posts on AI 避难所</description><generator>Hugo -- 0.147.4</generator><language>en-us</language><lastBuildDate>Wed, 21 May 2025 15:43:08 +0800</lastBuildDate><atom:link href="https://jackypanster.github.io/ai-stream/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>基于 FunAudioLLM/SenseVoiceSmall 搭建高效语音转录服务的实践之路</title><link>https://jackypanster.github.io/ai-stream/posts/howto-use-sensevoicesmall/</link><pubDate>Wed, 21 May 2025 15:43:08 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/howto-use-sensevoicesmall/</guid><description>&lt;h2 id="项目概述">项目概述&lt;/h2>
&lt;p>实现一个语音转录文本（ASR）的服务，目标是能够高效地将用户上传的音频文件转换为文字。出于中文语音的考虑，选择了来自 &lt;code>FunAudioLLM&lt;/code> 的 &lt;code>SenseVoiceSmall&lt;/code> 模型，它以其多语种支持、高效率以及集成的语音理解能力（如情感识别、事件检测）吸引了我。本文将详细记录从环境配置、核心功能实现到踩坑解决的全过程，并分享一些关于模型选型的思考。&lt;/p>
&lt;p>完整代码已开源在 GitHub 仓库：&lt;a href="https://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall">https://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall&lt;/a>&lt;/p>
&lt;p>项目需求文档（&lt;code>prd.md&lt;/code>）关键信息如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>模型&lt;/strong>: FunAudioLLM/SenseVoice (具体为 &lt;code>SenseVoiceSmall&lt;/code>)&lt;/li>
&lt;li>&lt;strong>本地模型路径&lt;/strong>: &lt;code>/home/llm/model/iic/SenseVoiceSmall&lt;/code> (从 ModelScope 下载)&lt;/li>
&lt;li>&lt;strong>API框架&lt;/strong>: FastAPI&lt;/li>
&lt;li>&lt;strong>Python环境管理&lt;/strong>: &lt;code>uv&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="环境配置">环境配置&lt;/h2>
&lt;p>为了保持开发环境的纯净和高效，采用了 &lt;code>uv&lt;/code> 来管理 Python 依赖。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>创建虚拟环境&lt;/strong> (如果尚未创建):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>uv venv .venv
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>source .venv/bin/activate
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>&lt;strong>安装核心依赖&lt;/strong>:
初始的 &lt;code>requirements.txt&lt;/code> 包含了 &lt;code>fastapi&lt;/code>, &lt;code>uvicorn&lt;/code>, &lt;code>python-multipart&lt;/code> 等基础库。后续根据模型加载和处理的需求，逐步添加了 &lt;code>torch&lt;/code>, &lt;code>torchaudio&lt;/code>, &lt;code>numpy&lt;/code>, &lt;code>transformers&lt;/code>, &lt;code>sentencepiece&lt;/code>, 以及最终解决模型加载问题的核心库 &lt;code>funasr&lt;/code>。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>uv pip install -r requirements.txt
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;h2 id="核心功能实现概览">核心功能实现概览&lt;/h2>
&lt;h3 id="项目结构">项目结构&lt;/h3>
&lt;p>项目的主要结构包括：&lt;/p>
&lt;ul>
&lt;li>&lt;code>app/main.py&lt;/code>: FastAPI 应用入口，定义 API 路由和应用生命周期事件（如模型加载）。&lt;/li>
&lt;li>&lt;code>app/models/sensevoice_loader.py&lt;/code>: 负责加载 &lt;code>SenseVoiceSmall&lt;/code> 模型，采用单例模式。&lt;/li>
&lt;li>&lt;code>app/services/asr_service.py&lt;/code>: 封装语音处理和模型推理的核心逻辑。&lt;/li>
&lt;li>&lt;code>app/schemas.py&lt;/code>: 定义 API 的请求和响应数据模型 (Pydantic models)。&lt;/li>
&lt;/ul>
&lt;h3 id="api-端点">API 端点&lt;/h3>
&lt;p>关键的 API 端点设计为：&lt;/p></description><content:encoded><![CDATA[<h2 id="项目概述">项目概述</h2>
<p>实现一个语音转录文本（ASR）的服务，目标是能够高效地将用户上传的音频文件转换为文字。出于中文语音的考虑，选择了来自 <code>FunAudioLLM</code> 的 <code>SenseVoiceSmall</code> 模型，它以其多语种支持、高效率以及集成的语音理解能力（如情感识别、事件检测）吸引了我。本文将详细记录从环境配置、核心功能实现到踩坑解决的全过程，并分享一些关于模型选型的思考。</p>
<p>完整代码已开源在 GitHub 仓库：<a href="https://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall">https://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall</a></p>
<p>项目需求文档（<code>prd.md</code>）关键信息如下：</p>
<ul>
<li><strong>模型</strong>: FunAudioLLM/SenseVoice (具体为 <code>SenseVoiceSmall</code>)</li>
<li><strong>本地模型路径</strong>: <code>/home/llm/model/iic/SenseVoiceSmall</code> (从 ModelScope 下载)</li>
<li><strong>API框架</strong>: FastAPI</li>
<li><strong>Python环境管理</strong>: <code>uv</code></li>
</ul>
<h2 id="环境配置">环境配置</h2>
<p>为了保持开发环境的纯净和高效，采用了 <code>uv</code> 来管理 Python 依赖。</p>
<ol>
<li>
<p><strong>创建虚拟环境</strong> (如果尚未创建):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>uv venv .venv
</span></span><span style="display:flex;"><span>source .venv/bin/activate
</span></span></code></pre></div></li>
<li>
<p><strong>安装核心依赖</strong>:
初始的 <code>requirements.txt</code> 包含了 <code>fastapi</code>, <code>uvicorn</code>, <code>python-multipart</code> 等基础库。后续根据模型加载和处理的需求，逐步添加了 <code>torch</code>, <code>torchaudio</code>, <code>numpy</code>, <code>transformers</code>, <code>sentencepiece</code>, 以及最终解决模型加载问题的核心库 <code>funasr</code>。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>uv pip install -r requirements.txt
</span></span></code></pre></div></li>
</ol>
<h2 id="核心功能实现概览">核心功能实现概览</h2>
<h3 id="项目结构">项目结构</h3>
<p>项目的主要结构包括：</p>
<ul>
<li><code>app/main.py</code>: FastAPI 应用入口，定义 API 路由和应用生命周期事件（如模型加载）。</li>
<li><code>app/models/sensevoice_loader.py</code>: 负责加载 <code>SenseVoiceSmall</code> 模型，采用单例模式。</li>
<li><code>app/services/asr_service.py</code>: 封装语音处理和模型推理的核心逻辑。</li>
<li><code>app/schemas.py</code>: 定义 API 的请求和响应数据模型 (Pydantic models)。</li>
</ul>
<h3 id="api-端点">API 端点</h3>
<p>关键的 API 端点设计为：</p>
<h4 id="post-asr_pure">POST /asr_pure</h4>
<ul>
<li><strong>Content-Type</strong>: <code>multipart/form-data</code></li>
<li><strong>Body</strong>: <code>file</code> (音频文件)</li>
</ul>
<p>返回转录后的文本及处理时间。</p>
<h2 id="踩坑与解决之路模型加载的曲折历程">踩坑与解决之路：模型加载的曲折历程</h2>
<p>在项目推进过程中，模型加载部分是遇到问题最多的地方，也是收获最多的地方。</p>
<h3 id="坑1hugging-face-autoclass-的-unrecognized-model">坑1：Hugging Face <code>AutoClass</code> 的 &ldquo;Unrecognized model&rdquo;</h3>
<p>最初，尝试使用 Hugging Face <code>transformers</code> 库通用的 <code>AutoProcessor.from_pretrained()</code> 和 <code>AutoModelForSpeechSeq2Seq.from_pretrained()</code> 来加载本地的 <code>SenseVoiceSmall</code> 模型文件。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># app/models/sensevoice_loader.py (早期尝试)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ...</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># self.processor = AutoProcessor.from_pretrained(MODEL_PATH)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># self.model = AutoModelForSpeechSeq2Seq.from_pretrained(MODEL_PATH)</span>
</span></span></code></pre></div><p>然而，服务启动时立即报错：</p>
<pre tabindex="0"><code>ValueError: Unrecognized model in /home/llm/model/iic/SenseVoiceSmall. Should have a model_type key in its config.json...
</code></pre><p>这个错误表明 <code>transformers</code> 的自动发现机制无法识别模型类型，通常是因为模型目录下的 <code>config.json</code> 文件缺少 <code>model_type</code> 字段，或者该模型需要特定的加载类。</p>
<h3 id="坑2转向-funasr-与-trust_remote_code-的初步探索">坑2：转向 <code>funasr</code> 与 <code>trust_remote_code</code> 的初步探索</h3>
<p>查阅 <code>FunAudioLLM/SenseVoice</code> 的官方文档后发现，推荐使用 <code>funasr</code> 库的 <code>AutoModel</code> 来加载 <code>SenseVoice</code> 系列模型。于是调整了代码：</p>
<ol>
<li><strong>添加 <code>funasr</code> 到 <code>requirements.txt</code></strong>。</li>
<li><strong>修改 <code>SenseVoiceLoader</code></strong>:
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># app/models/sensevoice_loader.py (引入 funasr)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> funasr <span style="color:#f92672">import</span> AutoModel
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ...</span>
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> AutoModel(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span>FUNASR_MODEL_NAME_OR_PATH, <span style="color:#75715e"># 即本地路径</span>
</span></span><span style="display:flex;"><span>    trust_remote_code<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    device<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>device
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div>同时，<code>asr_service.py</code> 中的推理逻辑也相应调整为调用 <code>funasr</code> 模型对象的 <code>.generate()</code> 方法。</li>
</ol>
<p>本以为这样能解决问题，但启动时又遇到了新的日志：</p>
<pre tabindex="0"><code>Loading remote code failed: model, No module named &#39;model&#39;
</code></pre><p>尽管这条日志出现，但后续的 API 调用测试居然成功了！这让我非常困惑。</p>
<h3 id="坑3remote_code-参数与-modelpy-文件的幻影">坑3：<code>remote_code</code> 参数与 <code>model.py</code> 文件的“幻影”</h3>
<p>深入研究 <code>funasr</code> 和 <code>SenseVoice</code> 的文档，注意到对于包含自定义代码（如 <code>model.py</code>）的模型，除了 <code>trust_remote_code=True</code>，有时还需要明确指定 <code>remote_code</code> 参数。</p>
<p>我检查了 Hugging Face 仓库 <code>FunAudioLLM/SenseVoiceSmall</code> (<a href="https://huggingface.co/FunAudioLLM/SenseVoiceSmall/tree/main">https://huggingface.co/FunAudioLLM/SenseVoiceSmall/tree/main</a>)，发现其文件列表中确实包含一个 <code>model.py</code>。因此，我尝试在 <code>AutoModel</code> 调用中加入 <code>remote_code=&quot;model.py&quot;</code>。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># app/models/sensevoice_loader.py (尝试指定 remote_code)</span>
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> AutoModel(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span>FUNASR_MODEL_NAME_OR_PATH,
</span></span><span style="display:flex;"><span>    trust_remote_code<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    remote_code<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;model.py&#34;</span>, <span style="color:#75715e"># &lt;--- 新增</span>
</span></span><span style="display:flex;"><span>    device<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>device
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>结果，<code>No module named 'model'</code> 的错误依旧。</p>
<h3 id="解决方案澄清-modelscope-与-hugging-face-的模型文件差异">解决方案：澄清 ModelScope 与 Hugging Face 的模型文件差异</h3>
<p>本地模型 <code>/home/llm/model/iic/SenseVoiceSmall</code> 是从 <strong>ModelScope</strong> (<a href="https://www.modelscope.cn/models/iic/SenseVoiceSmall/files">https://www.modelscope.cn/models/iic/SenseVoiceSmall/files</a>) 下载的，而非直接 clone Hugging Face 的仓库。通过 <code>ls -al /home/llm/model/iic/SenseVoiceSmall/</code> 查看本地文件，<strong>发现确实没有 <code>model.py</code> 文件！</strong></p>
<p>这解释了为什么指定 <code>remote_code=&quot;model.py&quot;</code> 依然报错。ModelScope 提供的模型包可能与 Hugging Face 仓库中的文件结构不完全一致，特别是对于这种依赖 <code>funasr</code> 特定加载方式的模型。</p>
<p><strong>最终的正确配置</strong>：移除 <code>remote_code</code> 参数，但保留 <code>trust_remote_code=True</code>。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># app/models/sensevoice_loader.py (最终正确配置)</span>
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> AutoModel(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span>FUNASR_MODEL_NAME_OR_PATH,
</span></span><span style="display:flex;"><span>    trust_remote_code<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, <span style="color:#75715e"># 保留，funasr 可能仍需此权限处理 ModelScope 模型</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># remote_code=&#34;model.py&#34;, # 移除，因为本地 ModelScope 版本无此文件</span>
</span></span><span style="display:flex;"><span>    device<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>device
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>这样修改后，服务启动时仍然会打印 <code>Loading remote code failed: model, No module named 'model'</code>，但 API 调用完全正常！</p>
<p><strong>原因分析</strong>：<code>funasr</code> 在 <code>trust_remote_code=True</code> 时，会优先尝试加载自定义代码。如果本地模型路径（如从 ModelScope 下载的）没有 <code>model.py</code>，这个尝试会失败并打印日志。但随后，<code>funasr</code> 能够识别出这是一个有效的 ModelScope 模型路径，并转用其内部的标准加载流程成功加载模型。因此，该日志在这种情况下是良性的。</p>
<h2 id="模型对比与选型思考">模型对比与选型思考</h2>
<p>在解决问题的过程中，也探讨了 <code>FunAudioLLM/SenseVoiceSmall</code> 与其他主流 ASR 模型的对比：</p>
<ul>
<li>
<p><strong>OpenAI Whisper 系列</strong> (如 <code>whisper-large-v3</code>):</p>
<ul>
<li><strong>优势</strong>: 极高的准确率，强大的多语言能力，庞大的社区。</li>
<li><strong>劣势</strong>: 推理速度相对较慢（尤其大模型），不直接提供情感/事件检测。</li>
</ul>
</li>
<li>
<p><strong>Wav2Vec2 系列</strong>:</p>
<ul>
<li><strong>优势</strong>: 自监督学习典范，大量特定语言微调模型。</li>
<li><strong>劣势</strong>: 基础模型功能相对单一。</li>
</ul>
</li>
</ul>
<h3 id="sensevoicesmall"><strong><code>SenseVoiceSmall</code> 的核心优势</strong></h3>
<ol>
<li>
<p><strong>高效推理</strong>：其模型卡声称采用非自回归端到端框架，比 Whisper-Large 快15倍。这对于需要低延迟的应用至关重要。</p>
</li>
<li>
<p><strong>多任务集成</strong>：内置 ASR、LID（语种识别）、SER（情感识别）、AED（事件检测）。如果应用场景需要这些附加信息，<code>SenseVoiceSmall</code> 提供了一站式解决方案。</p>
</li>
<li>
<p><strong>特定语言优化</strong>：在中文、粤语等语言上表现突出。</p>
</li>
</ol>
<h3 id="结论"><strong>结论</strong></h3>
<p>没有绝对的“最好”，只有“最适合”。</p>
<ul>
<li>若追求极致准确性和最广语言覆盖，且对延迟不敏感，Whisper 仍是首选。</li>
<li>若对<strong>推理效率、集成的多任务语音理解（特别是情感/事件）或中文等特定场景有高要求</strong>，<code>SenseVoiceSmall</code> 是一个极具竞争力的选择。</li>
</ul>
<p>目前选择的 <code>SenseVoiceSmall</code>，尤其是在确认了其 ModelScope 版本能够顺畅运行后，对于我的项目目标来说是一个合适的起点。</p>
<h2 id="当前状态与展望">当前状态与展望</h2>
<p>目前，基于 <code>FunAudioLLM/SenseVoiceSmall</code> 和 FastAPI 的语音转录服务已成功搭建并能正确处理请求。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>$ curl -X POST <span style="color:#e6db74">&#34;http://&lt;your_server_ip&gt;:8888/asr_pure&#34;</span> -F <span style="color:#e6db74">&#34;file=@test_audio.wav&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">{</span><span style="color:#e6db74">&#34;text&#34;</span>:<span style="color:#e6db74">&#34;太好了，那接下来咱们可以试试其他功能了。比如说你想测试一下语音合成的效果怎么样，或者是看看有没有什么新的语音处理功能出来啦。😔&#34;</span>,<span style="color:#e6db74">&#34;status&#34;</span>:<span style="color:#e6db74">&#34;success&#34;</span>,<span style="color:#e6db74">&#34;processing_time_ms&#34;</span>:503.39...<span style="color:#f92672">}</span>
</span></span></code></pre></div><h3 id="后续可优化的方向"><strong>后续可优化的方向</strong></h3>
<ul>
<li><strong>性能优化</strong>：进一步测试并发处理能力，考虑多 worker 配置。</li>
<li><strong>错误处理与日志</strong>：完善更细致的错误捕获和日志记录。</li>
<li><strong>功能扩展</strong>：如果需要，可以利用 <code>SenseVoiceSmall</code> 的情感识别和事件检测能力。</li>
<li><strong>VAD 集成</strong>：对于长音频，考虑在 <code>funasr.AutoModel</code> 加载时集成 VAD (Voice Activity Detection) 功能，以实现自动分段处理，提升长音频处理的稳定性和效率。</li>
<li><strong>异步处理与队列</strong>：对于高并发场景，引入消息队列和异步任务处理。</li>
</ul>
]]></content:encoded></item><item><title>如何使用Qwen2.5-Omni实现文本转语音(TTS)和语音转文本(ASR)</title><link>https://jackypanster.github.io/ai-stream/posts/how-to-use-qwen-omni-tts-asr/</link><pubDate>Tue, 20 May 2025 20:24:06 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/how-to-use-qwen-omni-tts-asr/</guid><description>&lt;h1 id="如何使用qwen25-omni实现文本转语音tts和语音转文本asr">如何使用Qwen2.5-Omni实现文本转语音(TTS)和语音转文本(ASR)&lt;/h1>
&lt;h2 id="项目概述">项目概述&lt;/h2>
&lt;p>本项目基于Qwen2.5-Omni-7B模型，实现了两个核心功能：&lt;/p>
&lt;ol>
&lt;li>文本转语音（TTS）：将输入文本转换为自然流畅的语音&lt;/li>
&lt;li>语音转文本（ASR）：将语音文件转换为文本，支持标准ASR和纯ASR两种模式&lt;/li>
&lt;/ol>
&lt;p>项目地址：&lt;a href="https://github.com/jackypanster/qwen-omni">https://github.com/jackypanster/qwen-omni&lt;/a>&lt;/p>
&lt;h2 id="环境配置">环境配置&lt;/h2>
&lt;p>推荐使用conda管理Python环境，确保依赖安装的稳定性：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 创建并激活环境&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>conda create -n qwen-tts python&lt;span style="color:#f92672">=&lt;/span>3.10
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>conda activate qwen-tts
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 安装PyTorch（GPU版本）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>conda install pytorch&lt;span style="color:#f92672">=&lt;/span>2.5.1 pytorch-cuda&lt;span style="color:#f92672">=&lt;/span>12.1 -c pytorch -c nvidia
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>conda install torchvision torchaudio -c pytorch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 安装其他依赖&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>conda install streamlit python-soundfile -c conda-forge
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pip install git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pip install qwen-omni-utils
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="核心功能实现">核心功能实现&lt;/h2>
&lt;h3 id="1-文本转语音tts">1. 文本转语音（TTS）&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">text_to_speech&lt;/span>(text_input, output_audio_path&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;output/output.wav&amp;#34;&lt;/span>, speaker&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Chelsie&amp;#34;&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 加载模型和处理器&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model &lt;span style="color:#f92672">=&lt;/span> Qwen2_5OmniForConditionalGeneration&lt;span style="color:#f92672">.&lt;/span>from_pretrained(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model_path,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config&lt;span style="color:#f92672">=&lt;/span>config,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> torch_dtype&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;auto&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> device_map&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;auto&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> processor &lt;span style="color:#f92672">=&lt;/span> Qwen2_5OmniProcessor&lt;span style="color:#f92672">.&lt;/span>from_pretrained(model_path)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 构造对话&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> conversation &lt;span style="color:#f92672">=&lt;/span> [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {&lt;span style="color:#e6db74">&amp;#34;role&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;system&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;content&amp;#34;&lt;/span>: [{&lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;You are Qwen...&amp;#34;&lt;/span>}]},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {&lt;span style="color:#e6db74">&amp;#34;role&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;user&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;content&amp;#34;&lt;/span>: [{&lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>: text_input}]}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 生成语音&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">with&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>no_grad():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text_ids, audio &lt;span style="color:#f92672">=&lt;/span> model&lt;span style="color:#f92672">.&lt;/span>generate(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">**&lt;/span>inputs,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> speaker&lt;span style="color:#f92672">=&lt;/span>speaker,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> do_sample&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> temperature&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.8&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> top_p&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.95&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> max_new_tokens&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1024&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="2-语音转文本asr">2. 语音转文本（ASR）&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">audio_to_text&lt;/span>(audio_path: str) &lt;span style="color:#f92672">-&amp;gt;&lt;/span> str:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 标准ASR模式&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> conversation &lt;span style="color:#f92672">=&lt;/span> [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {&lt;span style="color:#e6db74">&amp;#34;role&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;system&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;content&amp;#34;&lt;/span>: [{&lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;你是Qwen...&amp;#34;&lt;/span>}]},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {&lt;span style="color:#e6db74">&amp;#34;role&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;user&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;content&amp;#34;&lt;/span>: [{&lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;audio&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;audio&amp;#34;&lt;/span>: audio_path}]}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 生成文本&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">with&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>no_grad():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text_ids &lt;span style="color:#f92672">=&lt;/span> model&lt;span style="color:#f92672">.&lt;/span>generate(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">**&lt;/span>inputs,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> do_sample&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> max_new_tokens&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1024&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> return_audio&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="web界面实现">Web界面实现&lt;/h2>
&lt;p>使用Streamlit构建了简洁的Web界面：&lt;/p></description><content:encoded><![CDATA[<h1 id="如何使用qwen25-omni实现文本转语音tts和语音转文本asr">如何使用Qwen2.5-Omni实现文本转语音(TTS)和语音转文本(ASR)</h1>
<h2 id="项目概述">项目概述</h2>
<p>本项目基于Qwen2.5-Omni-7B模型，实现了两个核心功能：</p>
<ol>
<li>文本转语音（TTS）：将输入文本转换为自然流畅的语音</li>
<li>语音转文本（ASR）：将语音文件转换为文本，支持标准ASR和纯ASR两种模式</li>
</ol>
<p>项目地址：<a href="https://github.com/jackypanster/qwen-omni">https://github.com/jackypanster/qwen-omni</a></p>
<h2 id="环境配置">环境配置</h2>
<p>推荐使用conda管理Python环境，确保依赖安装的稳定性：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># 创建并激活环境</span>
</span></span><span style="display:flex;"><span>conda create -n qwen-tts python<span style="color:#f92672">=</span>3.10
</span></span><span style="display:flex;"><span>conda activate qwen-tts
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 安装PyTorch（GPU版本）</span>
</span></span><span style="display:flex;"><span>conda install pytorch<span style="color:#f92672">=</span>2.5.1 pytorch-cuda<span style="color:#f92672">=</span>12.1 -c pytorch -c nvidia
</span></span><span style="display:flex;"><span>conda install torchvision torchaudio -c pytorch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 安装其他依赖</span>
</span></span><span style="display:flex;"><span>conda install streamlit python-soundfile -c conda-forge
</span></span><span style="display:flex;"><span>pip install git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview
</span></span><span style="display:flex;"><span>pip install qwen-omni-utils
</span></span></code></pre></div><h2 id="核心功能实现">核心功能实现</h2>
<h3 id="1-文本转语音tts">1. 文本转语音（TTS）</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">text_to_speech</span>(text_input, output_audio_path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;output/output.wav&#34;</span>, speaker<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Chelsie&#34;</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 加载模型和处理器</span>
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> Qwen2_5OmniForConditionalGeneration<span style="color:#f92672">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>        model_path, 
</span></span><span style="display:flex;"><span>        config<span style="color:#f92672">=</span>config, 
</span></span><span style="display:flex;"><span>        torch_dtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;auto&#34;</span>, 
</span></span><span style="display:flex;"><span>        device_map<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;auto&#34;</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    processor <span style="color:#f92672">=</span> Qwen2_5OmniProcessor<span style="color:#f92672">.</span>from_pretrained(model_path)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 构造对话</span>
</span></span><span style="display:flex;"><span>    conversation <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>        {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;system&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: [{<span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;text&#34;</span>, <span style="color:#e6db74">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;You are Qwen...&#34;</span>}]},
</span></span><span style="display:flex;"><span>        {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: [{<span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;text&#34;</span>, <span style="color:#e6db74">&#34;text&#34;</span>: text_input}]}
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 生成语音</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        text_ids, audio <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>generate(
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">**</span>inputs,
</span></span><span style="display:flex;"><span>            speaker<span style="color:#f92672">=</span>speaker,
</span></span><span style="display:flex;"><span>            do_sample<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>            temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">0.8</span>,
</span></span><span style="display:flex;"><span>            top_p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.95</span>,
</span></span><span style="display:flex;"><span>            max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>
</span></span><span style="display:flex;"><span>        )
</span></span></code></pre></div><h3 id="2-语音转文本asr">2. 语音转文本（ASR）</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">audio_to_text</span>(audio_path: str) <span style="color:#f92672">-&gt;</span> str:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 标准ASR模式</span>
</span></span><span style="display:flex;"><span>    conversation <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>        {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;system&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: [{<span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;text&#34;</span>, <span style="color:#e6db74">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;你是Qwen...&#34;</span>}]},
</span></span><span style="display:flex;"><span>        {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: [{<span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;audio&#34;</span>, <span style="color:#e6db74">&#34;audio&#34;</span>: audio_path}]}
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 生成文本</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        text_ids <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>generate(
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">**</span>inputs,
</span></span><span style="display:flex;"><span>            do_sample<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>,
</span></span><span style="display:flex;"><span>            max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>,
</span></span><span style="display:flex;"><span>            return_audio<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>        )
</span></span></code></pre></div><h2 id="web界面实现">Web界面实现</h2>
<p>使用Streamlit构建了简洁的Web界面：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 文本输入</span>
</span></span><span style="display:flex;"><span>text_input <span style="color:#f92672">=</span> st<span style="color:#f92672">.</span>text_area(<span style="color:#e6db74">&#34;请输入要合成的文本：&#34;</span>, height<span style="color:#f92672">=</span><span style="color:#ae81ff">120</span>, max_chars<span style="color:#f92672">=</span><span style="color:#ae81ff">200</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 发音人选择</span>
</span></span><span style="display:flex;"><span>speaker <span style="color:#f92672">=</span> st<span style="color:#f92672">.</span>selectbox(<span style="color:#e6db74">&#34;请选择发音人：&#34;</span>, [<span style="color:#e6db74">&#34;Chelsie&#34;</span>, <span style="color:#e6db74">&#34;Ethan&#34;</span>], index<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 生成按钮</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> st<span style="color:#f92672">.</span>button(<span style="color:#e6db74">&#34;生成语音&#34;</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 生成语音并播放</span>
</span></span><span style="display:flex;"><span>    audio_path <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(OUTPUT_DIR, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;tts_</span><span style="color:#e6db74">{</span>uuid<span style="color:#f92672">.</span>uuid4()<span style="color:#f92672">.</span>hex<span style="color:#e6db74">}</span><span style="color:#e6db74">.wav&#34;</span>)
</span></span><span style="display:flex;"><span>    text_to_speech(text_input, output_audio_path<span style="color:#f92672">=</span>audio_path, speaker<span style="color:#f92672">=</span>speaker)
</span></span><span style="display:flex;"><span>    st<span style="color:#f92672">.</span>audio(audio_path, format<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;audio/wav&#34;</span>)
</span></span></code></pre></div><h2 id="restful-api实现">RESTful API实现</h2>
<p>使用FastAPI构建了RESTful API接口：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@app.post</span>(<span style="color:#e6db74">&#34;/tts&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">tts</span>(request: TTSRequest):
</span></span><span style="display:flex;"><span>    audio_filename <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;tts_</span><span style="color:#e6db74">{</span>uuid<span style="color:#f92672">.</span>uuid4()<span style="color:#f92672">.</span>hex<span style="color:#e6db74">}</span><span style="color:#e6db74">.wav&#34;</span>
</span></span><span style="display:flex;"><span>    audio_path <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(OUTPUT_DIR, audio_filename)
</span></span><span style="display:flex;"><span>    text_to_speech(request<span style="color:#f92672">.</span>text, audio_path, request<span style="color:#f92672">.</span>speaker)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#34;audio_url&#34;</span>: <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;/output/</span><span style="color:#e6db74">{</span>audio_filename<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@app.post</span>(<span style="color:#e6db74">&#34;/asr&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">asr</span>(file: UploadFile <span style="color:#f92672">=</span> File(<span style="color:#f92672">...</span>)):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 处理上传的音频文件</span>
</span></span><span style="display:flex;"><span>    audio_path <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(OUTPUT_DIR, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;asr_</span><span style="color:#e6db74">{</span>uuid<span style="color:#f92672">.</span>uuid4()<span style="color:#f92672">.</span>hex<span style="color:#e6db74">}</span><span style="color:#e6db74">.wav&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> open(audio_path, <span style="color:#e6db74">&#34;wb&#34;</span>) <span style="color:#66d9ef">as</span> buffer:
</span></span><span style="display:flex;"><span>        shutil<span style="color:#f92672">.</span>copyfileobj(file<span style="color:#f92672">.</span>file, buffer)
</span></span><span style="display:flex;"><span>    text <span style="color:#f92672">=</span> audio_to_text(audio_path)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#34;text&#34;</span>: text}
</span></span></code></pre></div><h2 id="使用说明">使用说明</h2>
<ol>
<li>启动Web界面：</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>streamlit run app_text2audio.py
</span></span></code></pre></div><ol start="2">
<li>启动API服务：</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>uvicorn fastapi_app:app --host 0.0.0.0 --port <span style="color:#ae81ff">8000</span>
</span></span></code></pre></div><h2 id="注意事项">注意事项</h2>
<ol>
<li>模型文件较大，建议提前下载并配置好模型路径</li>
<li>使用conda安装依赖可以避免大多数环境问题</li>
<li>音频文件会保存在output目录下</li>
<li>API接口支持文件上传和文本转写</li>
</ol>
<h2 id="后续优化方向">后续优化方向</h2>
<ol>
<li>支持更多发音人选项</li>
<li>优化模型加载速度</li>
<li>添加批量处理功能</li>
<li>支持更多音频格式</li>
<li>添加历史记录功能</li>
</ol>
<h2 id="参考资源">参考资源</h2>
<ul>
<li><a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B">Qwen2.5-Omni-7B官方文档</a></li>
<li><a href="https://docs.streamlit.io/">Streamlit文档</a></li>
<li><a href="https://fastapi.tiangolo.com/">FastAPI文档</a></li>
</ul>
]]></content:encoded></item><item><title>Qwen2.5-Omni-7B 文本转语音部署指南</title><link>https://jackypanster.github.io/ai-stream/posts/how-to-setup-qwen-omni/</link><pubDate>Mon, 19 May 2025 10:54:50 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/how-to-setup-qwen-omni/</guid><description>&lt;p>&lt;img alt="Qwen2.5-Omni-7B TTS" loading="lazy" src="https://via.placeholder.com/800x400.png/007bff/ffffff?text=Qwen2.5-Omni-7B+TTS">&lt;/p>
&lt;h2 id="概述">概述&lt;/h2>
&lt;p>本脚本基于 Qwen2.5-Omni-7B 多模态模型实现文本转语音（TTS）功能，支持生成自然流畅的中文 / 英文语音，并提供两种语音类型（女性 “Chelsie”、男性 “Ethan”）。脚本可将输入文本转换为音频文件（.wav格式），适用于语音助手、内容创作、无障碍服务等场景。&lt;/p>
&lt;h2 id="主要特性">主要特性&lt;/h2>
&lt;ul>
&lt;li>🎙️ 支持自然流畅的 &lt;strong>中文/英文&lt;/strong> 语音合成&lt;/li>
&lt;li>👥 提供两种语音类型选择：
&lt;ul>
&lt;li>女性声线：&amp;ldquo;Chelsie&amp;rdquo;&lt;/li>
&lt;li>男性声线：&amp;ldquo;Ethan&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>💾 输出格式：标准 &lt;code>.wav&lt;/code> 音频文件&lt;/li>
&lt;li>🚀 高性能推理，适用于生产环境&lt;/li>
&lt;/ul>
&lt;h2 id="应用场景">应用场景&lt;/h2>
&lt;ul>
&lt;li>智能语音助手开发&lt;/li>
&lt;li>内容创作与播客制作&lt;/li>
&lt;li>无障碍服务&lt;/li>
&lt;li>教育类应用&lt;/li>
&lt;li>多媒体内容生成&lt;/li>
&lt;/ul>
&lt;h2 id="开始使用">开始使用&lt;/h2>
&lt;blockquote>
&lt;p>💡 在开始之前，请确保您的系统满足以下要求：&lt;/p>
&lt;ul>
&lt;li>Python 3.8+&lt;/li>
&lt;li>CUDA 11.7+ (如需GPU加速)&lt;/li>
&lt;li>至少16GB可用内存&lt;/li>
&lt;/ul>&lt;/blockquote>
&lt;p>安装依赖库&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>uv init
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uv add git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uv add accelerate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uv add qwen-omni-utils&lt;span style="color:#f92672">[&lt;/span>decord&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uv add soundfile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uv add torchvision
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uv sync
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>完整脚本代码（main_text2audio.py）&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> os
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> soundfile &lt;span style="color:#66d9ef">as&lt;/span> sf
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> torch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> transformers &lt;span style="color:#f92672">import&lt;/span> Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> qwen_omni_utils &lt;span style="color:#f92672">import&lt;/span> process_mm_info
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> transformers &lt;span style="color:#f92672">import&lt;/span> AutoConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">text_to_speech&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text_input: str,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> output_audio_path: str &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;output/test_audio.wav&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> speaker: str &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;Chelsie&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model_path: str &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;/home/llm/model/qwen/Omni/&amp;#34;&lt;/span> &lt;span style="color:#75715e"># 改为本地路径或远程路径&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> 文本转语音核心函数
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> :param text_input: 输入文本（支持中文/英文）
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> :param output_audio_path: 音频输出路径（含文件名）
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> :param speaker: 语音类型（&amp;#34;Chelsie&amp;#34;女性/&amp;#34;Ethan&amp;#34;男性）
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> :param model_path: 模型路径（本地/远程）
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 1. 加载模型配置（修复ROPE参数兼容性）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config &lt;span style="color:#f92672">=&lt;/span> AutoConfig&lt;span style="color:#f92672">.&lt;/span>from_pretrained(model_path, local_files_only&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> hasattr(config, &lt;span style="color:#e6db74">&amp;#34;rope_scaling&amp;#34;&lt;/span>) &lt;span style="color:#f92672">and&lt;/span> &lt;span style="color:#e6db74">&amp;#34;mrope_section&amp;#34;&lt;/span> &lt;span style="color:#f92672">in&lt;/span> config&lt;span style="color:#f92672">.&lt;/span>rope_scaling:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config&lt;span style="color:#f92672">.&lt;/span>rope_scaling&lt;span style="color:#f92672">.&lt;/span>pop(&lt;span style="color:#e6db74">&amp;#34;mrope_section&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 2. 加载模型（支持GPU自动分配）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model &lt;span style="color:#f92672">=&lt;/span> Qwen2_5OmniForConditionalGeneration&lt;span style="color:#f92672">.&lt;/span>from_pretrained(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model_path,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config&lt;span style="color:#f92672">=&lt;/span>config,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> torch_dtype&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;auto&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> device_map&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;auto&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> local_files_only&lt;span style="color:#f92672">=&lt;/span>(model_path &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;Qwen/Qwen2.5-Omni-7B&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> processor &lt;span style="color:#f92672">=&lt;/span> Qwen2_5OmniProcessor&lt;span style="color:#f92672">.&lt;/span>from_pretrained(model_path)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 3. 系统提示（必须包含语音生成能力声明）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> system_prompt &lt;span style="color:#f92672">=&lt;/span> [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;role&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;system&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;content&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {&lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&amp;#34;&lt;/span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 4. 构建对话（纯文本输入）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> conversation &lt;span style="color:#f92672">=&lt;/span> system_prompt &lt;span style="color:#f92672">+&lt;/span> [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {&lt;span style="color:#e6db74">&amp;#34;role&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;user&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;content&amp;#34;&lt;/span>: [{&lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>: text_input}]}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 5. 处理输入数据&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text &lt;span style="color:#f92672">=&lt;/span> processor&lt;span style="color:#f92672">.&lt;/span>apply_chat_template(conversation, add_generation_prompt&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>, tokenize&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> audios, images, videos &lt;span style="color:#f92672">=&lt;/span> process_mm_info(conversation, use_audio_in_video&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 6. 生成语音&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> inputs &lt;span style="color:#f92672">=&lt;/span> processor(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text&lt;span style="color:#f92672">=&lt;/span>text, audio&lt;span style="color:#f92672">=&lt;/span>audios, images&lt;span style="color:#f92672">=&lt;/span>images, videos&lt;span style="color:#f92672">=&lt;/span>videos,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> return_tensors&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;pt&amp;#34;&lt;/span>, padding&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>, use_audio_in_video&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )&lt;span style="color:#f92672">.&lt;/span>to(model&lt;span style="color:#f92672">.&lt;/span>device, model&lt;span style="color:#f92672">.&lt;/span>dtype)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">with&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>no_grad():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text_ids, audio &lt;span style="color:#f92672">=&lt;/span> model&lt;span style="color:#f92672">.&lt;/span>generate(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">**&lt;/span>inputs,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> speaker&lt;span style="color:#f92672">=&lt;/span>speaker,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> do_sample&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>, &lt;span style="color:#75715e"># 启用采样模式以使用temperature/top_p&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> temperature&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.8&lt;/span>, &lt;span style="color:#75715e"># 控制随机性（0.5-1.0较自然）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> top_p&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.95&lt;/span>, &lt;span style="color:#75715e"># 核采样参数&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> max_new_tokens&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1024&lt;/span>, &lt;span style="color:#75715e"># 控制语音时长（约15秒）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> use_audio_in_video&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 7. 保存结果&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> os&lt;span style="color:#f92672">.&lt;/span>makedirs(os&lt;span style="color:#f92672">.&lt;/span>path&lt;span style="color:#f92672">.&lt;/span>dirname(output_audio_path), exist_ok&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sf&lt;span style="color:#f92672">.&lt;/span>write(output_audio_path, audio&lt;span style="color:#f92672">.&lt;/span>reshape(&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>)&lt;span style="color:#f92672">.&lt;/span>cpu()&lt;span style="color:#f92672">.&lt;/span>numpy(), samplerate&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">24000&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(&lt;span style="color:#e6db74">f&lt;/span>&lt;span style="color:#e6db74">&amp;#34;✅ 生成完成：&lt;/span>&lt;span style="color:#e6db74">{&lt;/span>output_audio_path&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(&lt;span style="color:#e6db74">f&lt;/span>&lt;span style="color:#e6db74">&amp;#34;📄 生成文本：&lt;/span>&lt;span style="color:#e6db74">{&lt;/span>processor&lt;span style="color:#f92672">.&lt;/span>batch_decode(text_ids, skip_special_tokens&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>)[&lt;span style="color:#ae81ff">0&lt;/span>]&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">if&lt;/span> __name__ &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#e6db74">&amp;#34;__main__&amp;#34;&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 示例输入（可替换为任意文本）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> input_text &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;你好，这是Qwen2.5-Omni的文本转语音示例。祝你使用愉快！&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 调用函数（指定输出路径和语音类型）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text_to_speech(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> input_text,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> output_audio_path&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;output/hello_qwen.wav&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> speaker&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Chelsie&amp;#34;&lt;/span> &lt;span style="color:#75715e"># 可选&amp;#34;Ethan&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>运行脚本&lt;/p></description><content:encoded><![CDATA[<p><img alt="Qwen2.5-Omni-7B TTS" loading="lazy" src="https://via.placeholder.com/800x400.png/007bff/ffffff?text=Qwen2.5-Omni-7B+TTS"></p>
<h2 id="概述">概述</h2>
<p>本脚本基于 Qwen2.5-Omni-7B 多模态模型实现文本转语音（TTS）功能，支持生成自然流畅的中文 / 英文语音，并提供两种语音类型（女性 “Chelsie”、男性 “Ethan”）。脚本可将输入文本转换为音频文件（.wav格式），适用于语音助手、内容创作、无障碍服务等场景。</p>
<h2 id="主要特性">主要特性</h2>
<ul>
<li>🎙️ 支持自然流畅的 <strong>中文/英文</strong> 语音合成</li>
<li>👥 提供两种语音类型选择：
<ul>
<li>女性声线：&ldquo;Chelsie&rdquo;</li>
<li>男性声线：&ldquo;Ethan&rdquo;</li>
</ul>
</li>
<li>💾 输出格式：标准 <code>.wav</code> 音频文件</li>
<li>🚀 高性能推理，适用于生产环境</li>
</ul>
<h2 id="应用场景">应用场景</h2>
<ul>
<li>智能语音助手开发</li>
<li>内容创作与播客制作</li>
<li>无障碍服务</li>
<li>教育类应用</li>
<li>多媒体内容生成</li>
</ul>
<h2 id="开始使用">开始使用</h2>
<blockquote>
<p>💡 在开始之前，请确保您的系统满足以下要求：</p>
<ul>
<li>Python 3.8+</li>
<li>CUDA 11.7+ (如需GPU加速)</li>
<li>至少16GB可用内存</li>
</ul></blockquote>
<p>安装依赖库</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>uv init
</span></span><span style="display:flex;"><span>uv add git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview
</span></span><span style="display:flex;"><span>uv add accelerate
</span></span><span style="display:flex;"><span>uv add qwen-omni-utils<span style="color:#f92672">[</span>decord<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>uv add soundfile
</span></span><span style="display:flex;"><span>uv add torchvision
</span></span><span style="display:flex;"><span>uv sync
</span></span></code></pre></div><p>完整脚本代码（main_text2audio.py）</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> soundfile <span style="color:#66d9ef">as</span> sf
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> qwen_omni_utils <span style="color:#f92672">import</span> process_mm_info
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoConfig
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">text_to_speech</span>(
</span></span><span style="display:flex;"><span>    text_input: str,
</span></span><span style="display:flex;"><span>    output_audio_path: str <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;output/test_audio.wav&#34;</span>,
</span></span><span style="display:flex;"><span>    speaker: str <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Chelsie&#34;</span>,
</span></span><span style="display:flex;"><span>    model_path: str <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/home/llm/model/qwen/Omni/&#34;</span>  <span style="color:#75715e"># 改为本地路径或远程路径</span>
</span></span><span style="display:flex;"><span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    文本转语音核心函数
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param text_input: 输入文本（支持中文/英文）
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param output_audio_path: 音频输出路径（含文件名）
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param speaker: 语音类型（&#34;Chelsie&#34;女性/&#34;Ethan&#34;男性）
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param model_path: 模型路径（本地/远程）
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 1. 加载模型配置（修复ROPE参数兼容性）</span>
</span></span><span style="display:flex;"><span>    config <span style="color:#f92672">=</span> AutoConfig<span style="color:#f92672">.</span>from_pretrained(model_path, local_files_only<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> hasattr(config, <span style="color:#e6db74">&#34;rope_scaling&#34;</span>) <span style="color:#f92672">and</span> <span style="color:#e6db74">&#34;mrope_section&#34;</span> <span style="color:#f92672">in</span> config<span style="color:#f92672">.</span>rope_scaling:
</span></span><span style="display:flex;"><span>        config<span style="color:#f92672">.</span>rope_scaling<span style="color:#f92672">.</span>pop(<span style="color:#e6db74">&#34;mrope_section&#34;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 2. 加载模型（支持GPU自动分配）</span>
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> Qwen2_5OmniForConditionalGeneration<span style="color:#f92672">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>        model_path,
</span></span><span style="display:flex;"><span>        config<span style="color:#f92672">=</span>config,
</span></span><span style="display:flex;"><span>        torch_dtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;auto&#34;</span>,
</span></span><span style="display:flex;"><span>        device_map<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;auto&#34;</span>,
</span></span><span style="display:flex;"><span>        local_files_only<span style="color:#f92672">=</span>(model_path <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#34;Qwen/Qwen2.5-Omni-7B&#34;</span>)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    processor <span style="color:#f92672">=</span> Qwen2_5OmniProcessor<span style="color:#f92672">.</span>from_pretrained(model_path)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 3. 系统提示（必须包含语音生成能力声明）</span>
</span></span><span style="display:flex;"><span>    system_prompt <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;system&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;content&#34;</span>: [
</span></span><span style="display:flex;"><span>                {<span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;text&#34;</span>, <span style="color:#e6db74">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&#34;</span>}
</span></span><span style="display:flex;"><span>            ]
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 4. 构建对话（纯文本输入）</span>
</span></span><span style="display:flex;"><span>    conversation <span style="color:#f92672">=</span> system_prompt <span style="color:#f92672">+</span> [
</span></span><span style="display:flex;"><span>        {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: [{<span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;text&#34;</span>, <span style="color:#e6db74">&#34;text&#34;</span>: text_input}]}
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 5. 处理输入数据</span>
</span></span><span style="display:flex;"><span>    text <span style="color:#f92672">=</span> processor<span style="color:#f92672">.</span>apply_chat_template(conversation, add_generation_prompt<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, tokenize<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>    audios, images, videos <span style="color:#f92672">=</span> process_mm_info(conversation, use_audio_in_video<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 6. 生成语音</span>
</span></span><span style="display:flex;"><span>    inputs <span style="color:#f92672">=</span> processor(
</span></span><span style="display:flex;"><span>        text<span style="color:#f92672">=</span>text, audio<span style="color:#f92672">=</span>audios, images<span style="color:#f92672">=</span>images, videos<span style="color:#f92672">=</span>videos,
</span></span><span style="display:flex;"><span>        return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pt&#34;</span>, padding<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, use_audio_in_video<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>    )<span style="color:#f92672">.</span>to(model<span style="color:#f92672">.</span>device, model<span style="color:#f92672">.</span>dtype)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        text_ids, audio <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>generate(
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">**</span>inputs,
</span></span><span style="display:flex;"><span>            speaker<span style="color:#f92672">=</span>speaker,
</span></span><span style="display:flex;"><span>            do_sample<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,  <span style="color:#75715e"># 启用采样模式以使用temperature/top_p</span>
</span></span><span style="display:flex;"><span>            temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">0.8</span>,  <span style="color:#75715e"># 控制随机性（0.5-1.0较自然）</span>
</span></span><span style="display:flex;"><span>            top_p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.95</span>,       <span style="color:#75715e"># 核采样参数</span>
</span></span><span style="display:flex;"><span>            max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>,  <span style="color:#75715e"># 控制语音时长（约15秒）</span>
</span></span><span style="display:flex;"><span>            use_audio_in_video<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 7. 保存结果</span>
</span></span><span style="display:flex;"><span>    os<span style="color:#f92672">.</span>makedirs(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>dirname(output_audio_path), exist_ok<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    sf<span style="color:#f92672">.</span>write(output_audio_path, audio<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>cpu()<span style="color:#f92672">.</span>numpy(), samplerate<span style="color:#f92672">=</span><span style="color:#ae81ff">24000</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;✅ 生成完成：</span><span style="color:#e6db74">{</span>output_audio_path<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;📄 生成文本：</span><span style="color:#e6db74">{</span>processor<span style="color:#f92672">.</span>batch_decode(text_ids, skip_special_tokens<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)[<span style="color:#ae81ff">0</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 示例输入（可替换为任意文本）</span>
</span></span><span style="display:flex;"><span>    input_text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;你好，这是Qwen2.5-Omni的文本转语音示例。祝你使用愉快！&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 调用函数（指定输出路径和语音类型）</span>
</span></span><span style="display:flex;"><span>    text_to_speech(
</span></span><span style="display:flex;"><span>        input_text,
</span></span><span style="display:flex;"><span>        output_audio_path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;output/hello_qwen.wav&#34;</span>,
</span></span><span style="display:flex;"><span>        speaker<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Chelsie&#34;</span>  <span style="color:#75715e"># 可选&#34;Ethan&#34;</span>
</span></span><span style="display:flex;"><span>    )
</span></span></code></pre></div><p>运行脚本</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>uv run main.py
</span></span></code></pre></div>]]></content:encoded></item></channel></rss>