<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Posts on Code Whispers</title><link>https://jackypanster.github.io/ai-stream/posts/</link><description>Recent content in Posts on Code Whispers</description><generator>Hugo -- 0.148.1</generator><language>zh-cn</language><lastBuildDate>Wed, 16 Jul 2025 18:14:40 +0800</lastBuildDate><atom:link href="https://jackypanster.github.io/ai-stream/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Ubuntu 24.04 终极性能调优指南：从系统臃肿到内核定制</title><link>https://jackypanster.github.io/ai-stream/posts/ubuntu-24-04-performance-tuning-guide/</link><pubDate>Wed, 16 Jul 2025 18:14:40 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/ubuntu-24-04-performance-tuning-guide/</guid><description>一份面向开发者的深度指南，记录了如何通过移除 Snap、更换 XanMod 内核、禁用 CPU 安全缓解等一系列硬核操作，将一台标准 Ubuntu 24.04 工作站打造成极致性能的开发机器。</description></item><item><title>Qwen3-32B-AWQ vLLM 多卡 2080 Ti 极限部署实战</title><link>https://jackypanster.github.io/ai-stream/posts/qwen3-32b-2080ti-vllm-deploy/</link><pubDate>Wed, 16 Jul 2025 13:19:48 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/qwen3-32b-2080ti-vllm-deploy/</guid><description>&lt;blockquote>
&lt;p>使用旧显卡也能跑 32B 大模型？本文手把手演示如何在 &lt;strong>4×RTX 2080 Ti (共 88 GB 显存)&lt;/strong> 服务器上，通过 vLLM 0.8.5 + AWQ 量化，跑起 &lt;strong>Qwen3-32B&lt;/strong> 并支持 &lt;strong>32 K tokens&lt;/strong> 超长上下文与高吞吐推理。全文记录了踩坑过程与参数权衡，希望给同样预算有限、硬件受限的工程师带来借鉴。&lt;/p>&lt;/blockquote>
&lt;p>{{.TableOfContents}}&lt;/p>
&lt;h2 id="1-项目背景">1 项目背景&lt;/h2>
&lt;ul>
&lt;li>主角：&lt;code>Qwen3-32B-AWQ&lt;/code> 量化模型 （≈ 18 GB）&lt;/li>
&lt;li>目标：在消费级 &lt;strong>Turing&lt;/strong> 架构显卡（2080 Ti）上最大化利用显存与吞吐。&lt;/li>
&lt;li>框架：&lt;code>vLLM 0.8.5&lt;/code> (openai-compatible server)&lt;/li>
&lt;li>取舍：牺牲部分延迟 / 稳定性 → 换取 &lt;strong>吞吐 + 上下文长度&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="2-硬件与系统环境">2 硬件与系统环境&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>组件&lt;/th>
&lt;th>规格&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>GPU&lt;/td>
&lt;td>4 × RTX 2080 Ti, 22 GB &lt;em>each&lt;/em>, Compute Capability 7.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CPU&lt;/td>
&lt;td>≥ 56 cores (vLLM 线程可吃满)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RAM&lt;/td>
&lt;td>512 GB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Storage&lt;/td>
&lt;td>NVMe SSD 2 TB (模型 + KV 缓冲)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OS&lt;/td>
&lt;td>Ubuntu 24.04&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Driver&lt;/td>
&lt;td>NVIDIA 570.153.02&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CUDA&lt;/td>
&lt;td>12.8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="21-nvidia-smi-基线信息">2.1 NVIDIA-SMI 基线信息&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>nvidia-smi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Wed Jul &lt;span style="color:#ae81ff">16&lt;/span> 13:27:17 &lt;span style="color:#ae81ff">2025&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----------------------------------------------------------------------------------------+
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| NVIDIA-SMI 570.153.02 Driver Version: 570.153.02 CUDA Version: 12.8 |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----------------------------------------------------------------------------------------+
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>可以看到驱动与 CUDA 版本与上表一致，确认环境无偏差。&lt;/p></description></item><item><title>从单卡瓶颈到四卡齐飞：一次完整的Ollama多GPU服务器性能优化实战</title><link>https://jackypanster.github.io/ai-stream/posts/the-ultimate-guide-to-multi-gpu-ollama-deployment/</link><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/the-ultimate-guide-to-multi-gpu-ollama-deployment/</guid><description>记录如何将一台拥有4块RTX 2080 Ti的服务器，从最初的Ollama单点服务，逐步优化，最终搭建成一个高性能、高并发的负载均衡集群的全过程。</description></item><item><title>为什么选择Gemini 2.5 Flash驱动K8s MCP Agent：深度技术选型分析</title><link>https://jackypanster.github.io/ai-stream/posts/gemini-2.5-flash-mcp-tech-selection/</link><pubDate>Wed, 18 Jun 2025 00:00:00 +0000</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/gemini-2.5-flash-mcp-tech-selection/</guid><description>深入分析Google Gemini 2.5 Flash在MCP工具调用方面的技术优势，对比主流LLM在工具调用兼容性方面的差异，分享K8s运维场景下的最佳实践。</description></item><item><title>优化CI/CD管道：实现Docker镜像白名单继承机制</title><link>https://jackypanster.github.io/ai-stream/posts/check-docker-image-size/</link><pubDate>Tue, 17 Jun 2025 15:26:29 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/check-docker-image-size/</guid><description>本文详细介绍如何在Jenkins共享库中优化Docker镜像大小检测，实现基础镜像白名单继承机制，解决构建镜像与白名单不匹配问题。</description></item><item><title>Qwen3-32B-AWQ 高效部署：基于 vLLM 的深度实践与优化</title><link>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-32b-awq-vllm-guide/</link><pubDate>Tue, 10 Jun 2025 20:45:00 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-32b-awq-vllm-guide/</guid><description>本文详细介绍了如何使用 vLLM 高效部署 Qwen3-32B-AWQ 量化模型，实现 32K 上下文窗口、OpenAI 兼容 API，并禁用思考模式。通过对 Docker 及 vLLM 参数的精细调优，最大化模型在多 GPU 环境下的推理性能。</description></item><item><title>AppArmor配置残留问题排查与彻底解决：从报错到系统净化的完整实践</title><link>https://jackypanster.github.io/ai-stream/posts/apparmor-troubleshooting/</link><pubDate>Mon, 09 Jun 2025 11:19:33 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/apparmor-troubleshooting/</guid><description>深入解析Ubuntu系统中AppArmor服务的SSSD配置残留问题，从原理分析到彻底解决的完整技术指南，适合系统运维工程师参考。</description></item><item><title>DeepSeek-R1-0528-Qwen3-8B部署优化实践</title><link>https://jackypanster.github.io/ai-stream/posts/deploy-deepseek-r1-qwen3-8b-optimization/</link><pubDate>Sat, 07 Jun 2025 17:50:00 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/deploy-deepseek-r1-qwen3-8b-optimization/</guid><description>&lt;h1 id="deepseek-r1-0528-qwen3-8b部署优化实践性能与稳定性的平衡艺术">DeepSeek-R1-0528-Qwen3-8B部署优化实践：性能与稳定性的平衡艺术&lt;/h1>
&lt;p>在AI大模型部署领域，本文详细记录对DeepSeek-R1-0528-Qwen3-8B模型使用vLLM进行部署优化的全过程，重点关注上下文窗口长度与硬件资源利用的平衡调优。&lt;/p>
&lt;h2 id="环境与基础设施">环境与基础设施&lt;/h2>
&lt;p>我们的部署环境具备以下配置：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>GPU&lt;/strong>: 4 x NVIDIA RTX 2080 Ti（每张22GB显存，总计88GB显存）
&lt;ul>
&lt;li>架构: Turing&lt;/li>
&lt;li>计算能力: 7.5&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>CPU&lt;/strong>: 56核&lt;/li>
&lt;li>&lt;strong>内存&lt;/strong>: 512GB RAM&lt;/li>
&lt;li>&lt;strong>存储&lt;/strong>: 2TB SSD&lt;/li>
&lt;li>&lt;strong>操作系统&lt;/strong>: Ubuntu 24.04&lt;/li>
&lt;li>&lt;strong>容器镜像&lt;/strong>: &lt;code>vllm/vllm-openai:v0.8.5&lt;/code>&lt;/li>
&lt;li>&lt;strong>NVIDIA驱动&lt;/strong>: 570.153.02（CUDA 12.8）&lt;/li>
&lt;/ul>
&lt;h2 id="优化前的部署脚本分析">优化前的部署脚本分析&lt;/h2>
&lt;p>我们最初的部署脚本如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -d &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --gpus all &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --name coder &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --shm-size 16g &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ulimit memlock&lt;span style="color:#f92672">=&lt;/span>-1 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --restart always &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ipc&lt;span style="color:#f92672">=&lt;/span>host &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -p 8000:8000 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -e CUDA_MODULE_LOADING&lt;span style="color:#f92672">=&lt;/span>LAZY &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> vllm/vllm-openai:v0.8.5 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --model /models &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --served-model-name coder &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --tensor-parallel-size &lt;span style="color:#ae81ff">4&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --gpu-memory-utilization 0.93 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --dtype float16 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --max-model-len &lt;span style="color:#ae81ff">65536&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --trust-remote-code &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --load-format safetensors &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --disable-custom-all-reduce
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>通过分析，我们发现几个可以优化的关键点：&lt;/p></description></item><item><title>Qwen3-30B 技术优化实践（二）：思考模式控制与15-20%性能提升</title><link>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-part2/</link><pubDate>Wed, 04 Jun 2025 14:30:00 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-part2/</guid><description>&lt;h1 id="qwen3-30b-技术优化实践二思考模式控制与性能提升">Qwen3-30B 技术优化实践（二）：思考模式控制与性能提升&lt;/h1>
&lt;blockquote>
&lt;p>本文是&lt;a href="blog-post.md">《从32K到131K：Qwen3-30B大模型上下文扩展实践》&lt;/a>的续篇，聚焦于模型性能调优特别是思考模式（reasoning mode）控制的技术细节与实践经验。&lt;/p>&lt;/blockquote>
&lt;p>在前文中，我们详细介绍了如何使用YaRN技术将Qwen3-30B的上下文长度从32K扩展到131K。今天，我们将深入探讨另一个关键优化维度：&lt;strong>思考模式控制&lt;/strong>及其对性能的影响。通过一系列实验和调优，我们发现禁用思考模式可以显著提升模型响应速度和内存效率，特别适合编程和直接输出类任务场景。&lt;/p>
&lt;h2 id="-思考模式reasoning-mode解析">🔍 思考模式（Reasoning Mode）解析&lt;/h2>
&lt;h3 id="什么是思考模式">什么是思考模式？&lt;/h3>
&lt;p>思考模式（Reasoning Mode，也称为Thinking Mode）是Qwen3系列模型的一个特性，让模型能够生成中间思考步骤，这些步骤被包含在&lt;code>&amp;lt;think&amp;gt;...&amp;lt;/think&amp;gt;&lt;/code>标签内。理论上，这种&amp;quot;思考过程&amp;quot;有助于模型进行更复杂的推理，但同时也引入了额外的计算和内存开销。&lt;/p>
&lt;p>在默认配置下，Qwen3模型会启用思考模式，产生类似以下的输出：&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;think&amp;gt;
首先，我需要分析用户的问题：如何实现一个简单的文件读写功能。
我应该使用Python的内置文件操作功能。
基本步骤应该是：
1. 打开文件（可以使用with语句自动管理资源）
2. 读取或写入内容
3. 确保文件正确关闭
&amp;lt;/think&amp;gt;
以下是一个简单的Python文件读写示例：
```python
# 写入文件
with open(&amp;#39;example.txt&amp;#39;, &amp;#39;w&amp;#39;) as file:
file.write(&amp;#39;Hello, World!&amp;#39;)
# 读取文件
with open(&amp;#39;example.txt&amp;#39;, &amp;#39;r&amp;#39;) as file:
content = file.read()
print(content)
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>
### 思考模式实现机制
vLLM部署Qwen3模型时，思考模式通过两种方式实现控制：
1. **服务器级控制**：通过部署参数`--enable-reasoning`和`--reasoning-parser deepseek_r1`启用
2. **API级控制**：通过API调用中的`chat_template_kwargs`参数或`enable_thinking`参数动态控制
我们的发现是，**仅删除服务器级别的参数并不足够完全禁用思考模式**，模型在某些情况下仍会产生思考过程。更彻底的解决方案是使用自定义聊天模板。
## 💡 禁用思考模式的技术实现
### 自定义聊天模板方案
经过研究Qwen官方文档和实验，我们发现使用自定义聊天模板是完全禁用思考模式的最可靠方法。我们创建了一个名为`qwen3_nonthinking.jinja`的模板文件：
```jinja
{% if messages %}
{% set loop_messages = messages %}
{% else %}
{% set loop_messages = [{&amp;#39;role&amp;#39;: &amp;#39;system&amp;#39;, &amp;#39;content&amp;#39;: &amp;#39;&amp;#39;}] %}
{% endif %}
{% for message in loop_messages %}
{% if message[&amp;#39;role&amp;#39;] == &amp;#39;user&amp;#39; %}
&amp;lt;|im_start|&amp;gt;user
{{ message[&amp;#39;content&amp;#39;] }}&amp;lt;|im_end|&amp;gt;
{% elif message[&amp;#39;role&amp;#39;] == &amp;#39;assistant&amp;#39; %}
&amp;lt;|im_start|&amp;gt;assistant
{{ message[&amp;#39;content&amp;#39;] }}&amp;lt;|im_end|&amp;gt;
{% elif message[&amp;#39;role&amp;#39;] == &amp;#39;system&amp;#39; %}
&amp;lt;|im_start|&amp;gt;system
{{ message[&amp;#39;content&amp;#39;] }}&amp;lt;|im_end|&amp;gt;
{% endif %}
{% endfor %}
&amp;lt;|im_start|&amp;gt;assistant
{% if add_generation_prompt is defined and add_generation_prompt %}{{ generation_prompt }}{% endif %}
&lt;/code>&lt;/pre>&lt;p>这个模板的关键点是&lt;strong>移除了所有与思考模式相关的标签和处理逻辑&lt;/strong>，确保模型无法生成&lt;code>&amp;lt;think&amp;gt;...&amp;lt;/think&amp;gt;&lt;/code>块，即使API请求中尝试启用思考模式。&lt;/p></description></item><item><title>高性能部署Qwen3-30B：vLLM优化实践指南</title><link>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3/</link><pubDate>Tue, 03 Jun 2025 16:00:00 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3/</guid><description>&lt;h1 id="高性能部署qwen3-30bvllm优化实践指南">高性能部署Qwen3-30B：vLLM优化实践指南&lt;/h1>
&lt;h2 id="-概述">📋 概述&lt;/h2>
&lt;p>本文详细介绍如何使用vLLM高效部署Qwen3-30B-A3B模型，实现32K上下文窗口和OpenAI兼容API，适用于生产环境。通过精细调整部署参数，我们能够在有限的GPU资源下最大化模型性能。&lt;/p>
&lt;h2 id="-系统要求">🖥️ 系统要求&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>硬件配置&lt;/strong>
&lt;ul>
&lt;li>4块NVIDIA GPU (每块22GB显存，总计88GB)&lt;/li>
&lt;li>512GB系统内存&lt;/li>
&lt;li>2TB SSD存储&lt;/li>
&lt;li>56核CPU&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>软件环境&lt;/strong>
&lt;ul>
&lt;li>Ubuntu 24.04&lt;/li>
&lt;li>NVIDIA驱动 550.144.03&lt;/li>
&lt;li>CUDA 12.4&lt;/li>
&lt;li>Docker + NVIDIA Container Toolkit&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="-模型与架构">🧠 模型与架构&lt;/h2>
&lt;p>Qwen3-30B-A3B是阿里云发布的通用大语言模型，具有以下特点：&lt;/p>
&lt;ul>
&lt;li>30B参数量&lt;/li>
&lt;li>原生支持32K上下文长度&lt;/li>
&lt;li>支持思考模式(Chain-of-Thought)&lt;/li>
&lt;li>优异的多语言与代码能力&lt;/li>
&lt;/ul>
&lt;p>我们使用vLLM作为推理引擎，主要基于以下考量：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>高效内存管理&lt;/strong>：通过PagedAttention技术优化KV缓存&lt;/li>
&lt;li>&lt;strong>张量并行&lt;/strong>：自动跨多GPU分布模型权重&lt;/li>
&lt;li>&lt;strong>OpenAI兼容API&lt;/strong>：直接替代OpenAI API，无需修改现有应用&lt;/li>
&lt;li>&lt;strong>动态批处理&lt;/strong>：自动批处理多请求，提高吞吐量&lt;/li>
&lt;/ol>
&lt;h2 id="-部署脚本">🐳 部署脚本&lt;/h2>
&lt;p>以下是我们用于部署的Docker命令，经过精心调优以平衡性能与资源利用：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run -d &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --runtime&lt;span style="color:#f92672">=&lt;/span>nvidia &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --gpus&lt;span style="color:#f92672">=&lt;/span>all &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --name coder &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -v /home/llm/model/qwen/qwen3-30b-a3b:/qwen/qwen3-30b-a3b &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -p 8000:8000 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --cpuset-cpus 0-55 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ulimit memlock&lt;span style="color:#f92672">=&lt;/span>-1 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ulimit stack&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">67108864&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --restart always &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ipc&lt;span style="color:#f92672">=&lt;/span>host &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> vllm/vllm-openai:v0.8.5 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --model /qwen/qwen3-30b-a3b &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --served-model-name coder &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --tensor-parallel-size &lt;span style="color:#ae81ff">4&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --dtype half &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --max-model-len &lt;span style="color:#ae81ff">32768&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --max-num-batched-tokens &lt;span style="color:#ae81ff">4096&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --gpu-memory-utilization 0.93 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --block-size &lt;span style="color:#ae81ff">32&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --enable-chunked-prefill &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --swap-space &lt;span style="color:#ae81ff">16&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --tokenizer-pool-size &lt;span style="color:#ae81ff">56&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --disable-custom-all-reduce
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="-参数详解与优化策略">🔧 参数详解与优化策略&lt;/h2>
&lt;h3 id="docker容器配置">Docker容器配置&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>参数&lt;/th>
&lt;th>值&lt;/th>
&lt;th>作用&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>--runtime=nvidia&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;td>启用NVIDIA容器运行时&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>--gpus=all&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;td>将所有GPU暴露给容器&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>--cpuset-cpus&lt;/code>&lt;/td>
&lt;td>&lt;code>0-55&lt;/code>&lt;/td>
&lt;td>限制容器使用0-55号CPU核心&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>--ulimit memlock=-1&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;td>移除内存锁定限制，提高性能&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>--ipc=host&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;td>使用主机IPC命名空间，对共享内存很重要&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="vllm引擎配置">vLLM引擎配置&lt;/h3>
&lt;h4 id="1-张量并行策略">1. 张量并行策略&lt;/h4>
&lt;pre tabindex="0">&lt;code>--tensor-parallel-size 4
&lt;/code>&lt;/pre>&lt;p>我们使用4路张量并行，将模型分布在4块GPU上。这是基于实验得出的最佳配置 - 在我们的硬件上，每块22GB显存的GPU无法单独加载完整的30B模型。&lt;/p></description></item><item><title>AI辅助编程实战：从需求到实现的完整对话记录</title><link>https://jackypanster.github.io/ai-stream/posts/coding-with-ai/</link><pubDate>Tue, 03 Jun 2025 13:28:50 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/coding-with-ai/</guid><description>&lt;h1 id="ai辅助编程实战从需求到实现的完整对话记录">AI辅助编程实战：从需求到实现的完整对话记录&lt;/h1>
&lt;h2 id="引言">引言&lt;/h2>
&lt;p>本文真实还原了我与AI助手协作实现Docker镜像大小监控功能的全过程。通过这次实践，可以看到AI不仅能辅助开发者梳理需求、设计方案，还能在代码实现、文档编写等环节提供高效帮助。整个流程自然流畅，极大提升了开发体验。&lt;/p>
&lt;h2 id="1-需求分析与方案设计">1. 需求分析与方案设计&lt;/h2>
&lt;h3 id="初始需求描述">初始需求描述&lt;/h3>
&lt;pre tabindex="0">&lt;code>用户：
我们需要在Jenkins共享库中增加Docker镜像大小监控功能，当镜像超过4GB时发出警告。请帮忙分析一下实现思路。
&lt;/code>&lt;/pre>&lt;h3 id="ai的初步分析与建议">AI的初步分析与建议&lt;/h3>
&lt;pre tabindex="0">&lt;code>AI：
让我先看看项目的结构……
这是一个Jenkins共享库项目，核心代码都在`vars/`目录。针对镜像大小监控，我建议直接在`buildDocker`方法里加逻辑：
1. 镜像构建完成后用`du`命令获取大小
2. 与4GB阈值比较
3. 超过阈值就输出警告
4. 相关指标写日志，方便后续分析
这个方案您觉得合适吗？有无补充？
&lt;/code>&lt;/pre>&lt;h3 id="需求确认与细化">需求确认与细化&lt;/h3>
&lt;pre tabindex="0">&lt;code>用户：
这个思路可以，不过还需要：
1. 阈值要可配置，默认4GB
2. 日志里要有镜像名、大小、时间戳等详细信息
3. 输出优化建议
另外，异常处理也要考虑得更优雅些。
&lt;/code>&lt;/pre>&lt;h2 id="2-代码实现">2. 代码实现&lt;/h2>
&lt;h3 id="初始代码实现">初始代码实现&lt;/h3>
&lt;pre tabindex="0">&lt;code>AI：
明白，我来实现。会在`buildDocker`方法里：
1. 用`getEnvOrDefault`支持阈值配置
2. 详细记录指标
3. 加强错误处理
4. 输出优化建议
实现逻辑您看还有什么要调整的吗？
&lt;/code>&lt;/pre>&lt;h3 id="代码审查与优化">代码审查与优化&lt;/h3>
&lt;pre tabindex="0">&lt;code>用户：
整体不错，不过警告要改成抛异常直接中断构建。另外，能不能顺便加上基础镜像大小检测？
&lt;/code>&lt;/pre>&lt;h3 id="异常处理优化">异常处理优化&lt;/h3>
&lt;pre tabindex="0">&lt;code>AI：
明白您的需求，代码我会这样优化：
1. 超标直接抛异常中断流水线
2. 增加基础镜像大小检测
3. 明确区分业务异常（比如镜像过大）和技术异常（比如命令失败）
基础镜像检测这块，我建议：
- 解析Dockerfile的FROM
- 拉取并检测基础镜像大小
- 超限就给出alpine等轻量镜像建议
这样处理您觉得OK吗？
&lt;/code>&lt;/pre>&lt;h2 id="3-测试与验证">3. 测试与验证&lt;/h2>
&lt;h3 id="测试方案讨论">测试方案讨论&lt;/h3>
&lt;pre tabindex="0">&lt;code>用户：
怎么验证这个功能？
AI：
建议这样测试：
1. 镜像大小正常
2. 恰好等于阈值
3. 超过阈值
4. Dockerfile解析异常
也可以临时把阈值调小，快速验证告警逻辑。
&lt;/code>&lt;/pre>&lt;h2 id="4-文档与沟通">4. 文档与沟通&lt;/h2>
&lt;h3 id="变更日志生成">变更日志生成&lt;/h3>
&lt;pre tabindex="0">&lt;code>AI：
我已为您生成详细CHANGELOG，包括：
- 功能说明
- 技术实现细节
- 配置参数说明
- 后续优化计划
如需调整，随时告诉我。
&lt;/code>&lt;/pre>&lt;h3 id="团队通知">团队通知&lt;/h3>
&lt;pre tabindex="0">&lt;code>用户：
要发个通知给团队，说明我们要限制镜像大小。
AI：
建议分阶段推进：
1. 先监控和通知
2. 再进入警告期
3. 最后强制限制
这样大家有缓冲期，可以逐步优化Dockerfile。需要帮忙起草通知吗？
&lt;/code>&lt;/pre>&lt;h2 id="5-经验总结">5. 经验总结&lt;/h2>
&lt;h3 id="技术收获">技术收获&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>AI辅助效率&lt;/strong>&lt;/p></description></item><item><title>基于FunAudioLLM/SenseVoiceSmall搭建高效语音转录服务的实践之路</title><link>https://jackypanster.github.io/ai-stream/posts/howto-use-sensevoicesmall/</link><pubDate>Wed, 21 May 2025 15:43:08 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/howto-use-sensevoicesmall/</guid><description>&lt;h1 id="基于funaudiollmsensevoicesmall搭建高效语音转录服务的实践之路">基于FunAudioLLM/SenseVoiceSmall搭建高效语音转录服务的实践之路&lt;/h1>
&lt;h2 id="项目概述">项目概述&lt;/h2>
&lt;p>实现一个语音转录文本（ASR）的服务，目标是能够高效地将用户上传的音频文件转换为文字。出于中文语音的考虑，选择了来自 &lt;code>FunAudioLLM&lt;/code> 的 &lt;code>SenseVoiceSmall&lt;/code> 模型，它以其多语种支持、高效率以及集成的语音理解能力（如情感识别、事件检测）吸引了我。本文将详细记录从环境配置、核心功能实现到踩坑解决的全过程，并分享一些关于模型选型的思考。&lt;/p>
&lt;p>完整代码已开源在 GitHub 仓库：&lt;a href="https://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall">https://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall&lt;/a>&lt;/p>
&lt;p>项目需求文档（&lt;code>prd.md&lt;/code>）关键信息如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>模型&lt;/strong>: FunAudioLLM/SenseVoice (具体为 &lt;code>SenseVoiceSmall&lt;/code>)&lt;/li>
&lt;li>&lt;strong>本地模型路径&lt;/strong>: &lt;code>/home/llm/model/iic/SenseVoiceSmall&lt;/code> (从 ModelScope 下载)&lt;/li>
&lt;li>&lt;strong>API框架&lt;/strong>: FastAPI&lt;/li>
&lt;li>&lt;strong>Python环境管理&lt;/strong>: &lt;code>uv&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="环境配置">环境配置&lt;/h2>
&lt;p>为了保持开发环境的纯净和高效，采用了 &lt;code>uv&lt;/code> 来管理 Python 依赖。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>创建虚拟环境&lt;/strong> (如果尚未创建):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>uv venv .venv
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>source .venv/bin/activate
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>&lt;strong>安装核心依赖&lt;/strong>:
初始的 &lt;code>requirements.txt&lt;/code> 包含了 &lt;code>fastapi&lt;/code>, &lt;code>uvicorn&lt;/code>, &lt;code>python-multipart&lt;/code> 等基础库。后续根据模型加载和处理的需求，逐步添加了 &lt;code>torch&lt;/code>, &lt;code>torchaudio&lt;/code>, &lt;code>numpy&lt;/code>, &lt;code>transformers&lt;/code>, &lt;code>sentencepiece&lt;/code>, 以及最终解决模型加载问题的核心库 &lt;code>funasr&lt;/code>。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>uv pip install -r requirements.txt
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;h2 id="核心功能实现概览">核心功能实现概览&lt;/h2>
&lt;h3 id="项目结构">项目结构&lt;/h3>
&lt;p>项目的主要结构包括：&lt;/p>
&lt;ul>
&lt;li>&lt;code>app/main.py&lt;/code>: FastAPI 应用入口，定义 API 路由和应用生命周期事件（如模型加载）。&lt;/li>
&lt;li>&lt;code>app/models/sensevoice_loader.py&lt;/code>: 负责加载 &lt;code>SenseVoiceSmall&lt;/code> 模型，采用单例模式。&lt;/li>
&lt;li>&lt;code>app/services/asr_service.py&lt;/code>: 封装语音处理和模型推理的核心逻辑。&lt;/li>
&lt;li>&lt;code>app/schemas.py&lt;/code>: 定义 API 的请求和响应数据模型 (Pydantic models)。&lt;/li>
&lt;/ul>
&lt;h3 id="api-端点">API 端点&lt;/h3>
&lt;p>关键的 API 端点设计为：&lt;/p></description></item><item><title>如何使用Qwen2.5-Omni实现文本转语音(TTS)和语音转文本(ASR)</title><link>https://jackypanster.github.io/ai-stream/posts/how-to-use-qwen-omni-tts-asr/</link><pubDate>Tue, 20 May 2025 20:24:06 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/how-to-use-qwen-omni-tts-asr/</guid><description>&lt;h1 id="如何使用qwen25-omni实现文本转语音tts和语音转文本asr">如何使用Qwen2.5-Omni实现文本转语音(TTS)和语音转文本(ASR)&lt;/h1>
&lt;h2 id="项目概述">项目概述&lt;/h2>
&lt;p>本项目基于Qwen2.5-Omni-7B模型，实现了两个核心功能：&lt;/p>
&lt;ol>
&lt;li>文本转语音（TTS）：将输入文本转换为自然流畅的语音&lt;/li>
&lt;li>语音转文本（ASR）：将语音文件转换为文本，支持标准ASR和纯ASR两种模式&lt;/li>
&lt;/ol>
&lt;p>项目地址：&lt;a href="https://github.com/jackypanster/qwen-omni">https://github.com/jackypanster/qwen-omni&lt;/a>&lt;/p>
&lt;h2 id="环境配置">环境配置&lt;/h2>
&lt;p>推荐使用conda管理Python环境，确保依赖安装的稳定性：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 创建并激活环境&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>conda create -n qwen-tts python&lt;span style="color:#f92672">=&lt;/span>3.10
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>conda activate qwen-tts
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 安装PyTorch（GPU版本）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>conda install pytorch&lt;span style="color:#f92672">=&lt;/span>2.5.1 pytorch-cuda&lt;span style="color:#f92672">=&lt;/span>12.1 -c pytorch -c nvidia
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>conda install torchvision torchaudio -c pytorch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 安装其他依赖&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>conda install streamlit python-soundfile -c conda-forge
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pip install git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pip install qwen-omni-utils
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="核心功能实现">核心功能实现&lt;/h2>
&lt;h3 id="1-文本转语音tts">1. 文本转语音（TTS）&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">text_to_speech&lt;/span>(text_input, output_audio_path&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;output/output.wav&amp;#34;&lt;/span>, speaker&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Chelsie&amp;#34;&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 加载模型和处理器&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model &lt;span style="color:#f92672">=&lt;/span> Qwen2_5OmniForConditionalGeneration&lt;span style="color:#f92672">.&lt;/span>from_pretrained(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model_path,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config&lt;span style="color:#f92672">=&lt;/span>config,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> torch_dtype&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;auto&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> device_map&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;auto&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> processor &lt;span style="color:#f92672">=&lt;/span> Qwen2_5OmniProcessor&lt;span style="color:#f92672">.&lt;/span>from_pretrained(model_path)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 构造对话&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> conversation &lt;span style="color:#f92672">=&lt;/span> [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {&lt;span style="color:#e6db74">&amp;#34;role&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;system&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;content&amp;#34;&lt;/span>: [{&lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;You are Qwen...&amp;#34;&lt;/span>}]},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {&lt;span style="color:#e6db74">&amp;#34;role&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;user&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;content&amp;#34;&lt;/span>: [{&lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>: text_input}]}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 生成语音&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">with&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>no_grad():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text_ids, audio &lt;span style="color:#f92672">=&lt;/span> model&lt;span style="color:#f92672">.&lt;/span>generate(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">**&lt;/span>inputs,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> speaker&lt;span style="color:#f92672">=&lt;/span>speaker,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> do_sample&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> temperature&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.8&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> top_p&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.95&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> max_new_tokens&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1024&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="2-语音转文本asr">2. 语音转文本（ASR）&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">audio_to_text&lt;/span>(audio_path: str) &lt;span style="color:#f92672">-&amp;gt;&lt;/span> str:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 标准ASR模式&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> conversation &lt;span style="color:#f92672">=&lt;/span> [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {&lt;span style="color:#e6db74">&amp;#34;role&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;system&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;content&amp;#34;&lt;/span>: [{&lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;你是Qwen...&amp;#34;&lt;/span>}]},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {&lt;span style="color:#e6db74">&amp;#34;role&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;user&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;content&amp;#34;&lt;/span>: [{&lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;audio&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;audio&amp;#34;&lt;/span>: audio_path}]}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 生成文本&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">with&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>no_grad():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text_ids &lt;span style="color:#f92672">=&lt;/span> model&lt;span style="color:#f92672">.&lt;/span>generate(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">**&lt;/span>inputs,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> do_sample&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> max_new_tokens&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1024&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> return_audio&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="web界面实现">Web界面实现&lt;/h2>
&lt;p>使用Streamlit构建了简洁的Web界面：&lt;/p></description></item><item><title>Qwen2.5-Omni-7B 文本转语音部署指南</title><link>https://jackypanster.github.io/ai-stream/posts/how-to-setup-qwen-omni/</link><pubDate>Mon, 19 May 2025 10:54:50 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/how-to-setup-qwen-omni/</guid><description>&lt;p>&lt;img alt="Qwen2.5-Omni-7B TTS" loading="lazy" src="https://via.placeholder.com/800x400.png/007bff/ffffff?text=Qwen2.5-Omni-7B+TTS">&lt;/p>
&lt;h2 id="概述">概述&lt;/h2>
&lt;p>本脚本基于 Qwen2.5-Omni-7B 多模态模型实现文本转语音（TTS）功能，支持生成自然流畅的中文 / 英文语音，并提供两种语音类型（女性 “Chelsie”、男性 “Ethan”）。脚本可将输入文本转换为音频文件（.wav格式），适用于语音助手、内容创作、无障碍服务等场景。&lt;/p>
&lt;h2 id="主要特性">主要特性&lt;/h2>
&lt;ul>
&lt;li>🎙️ 支持自然流畅的 &lt;strong>中文/英文&lt;/strong> 语音合成&lt;/li>
&lt;li>👥 提供两种语音类型选择：
&lt;ul>
&lt;li>女性声线：&amp;ldquo;Chelsie&amp;rdquo;&lt;/li>
&lt;li>男性声线：&amp;ldquo;Ethan&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>💾 输出格式：标准 &lt;code>.wav&lt;/code> 音频文件&lt;/li>
&lt;li>🚀 高性能推理，适用于生产环境&lt;/li>
&lt;/ul>
&lt;h2 id="应用场景">应用场景&lt;/h2>
&lt;ul>
&lt;li>智能语音助手开发&lt;/li>
&lt;li>内容创作与播客制作&lt;/li>
&lt;li>无障碍服务&lt;/li>
&lt;li>教育类应用&lt;/li>
&lt;li>多媒体内容生成&lt;/li>
&lt;/ul>
&lt;h2 id="开始使用">开始使用&lt;/h2>
&lt;blockquote>
&lt;p>💡 在开始之前，请确保您的系统满足以下要求：&lt;/p>
&lt;ul>
&lt;li>Python 3.8+&lt;/li>
&lt;li>CUDA 11.7+ (如需GPU加速)&lt;/li>
&lt;li>至少16GB可用内存&lt;/li>
&lt;/ul>&lt;/blockquote>
&lt;p>安装依赖库&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>uv init
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uv add git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uv add accelerate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uv add qwen-omni-utils&lt;span style="color:#f92672">[&lt;/span>decord&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uv add soundfile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uv add torchvision
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uv sync
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>完整脚本代码（main_text2audio.py）&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> os
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> soundfile &lt;span style="color:#66d9ef">as&lt;/span> sf
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> torch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> transformers &lt;span style="color:#f92672">import&lt;/span> Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> qwen_omni_utils &lt;span style="color:#f92672">import&lt;/span> process_mm_info
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> transformers &lt;span style="color:#f92672">import&lt;/span> AutoConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">text_to_speech&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text_input: str,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> output_audio_path: str &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;output/test_audio.wav&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> speaker: str &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;Chelsie&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model_path: str &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;/home/llm/model/qwen/Omni/&amp;#34;&lt;/span> &lt;span style="color:#75715e"># 改为本地路径或远程路径&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> 文本转语音核心函数
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> :param text_input: 输入文本（支持中文/英文）
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> :param output_audio_path: 音频输出路径（含文件名）
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> :param speaker: 语音类型（&amp;#34;Chelsie&amp;#34;女性/&amp;#34;Ethan&amp;#34;男性）
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> :param model_path: 模型路径（本地/远程）
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 1. 加载模型配置（修复ROPE参数兼容性）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config &lt;span style="color:#f92672">=&lt;/span> AutoConfig&lt;span style="color:#f92672">.&lt;/span>from_pretrained(model_path, local_files_only&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> hasattr(config, &lt;span style="color:#e6db74">&amp;#34;rope_scaling&amp;#34;&lt;/span>) &lt;span style="color:#f92672">and&lt;/span> &lt;span style="color:#e6db74">&amp;#34;mrope_section&amp;#34;&lt;/span> &lt;span style="color:#f92672">in&lt;/span> config&lt;span style="color:#f92672">.&lt;/span>rope_scaling:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config&lt;span style="color:#f92672">.&lt;/span>rope_scaling&lt;span style="color:#f92672">.&lt;/span>pop(&lt;span style="color:#e6db74">&amp;#34;mrope_section&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 2. 加载模型（支持GPU自动分配）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model &lt;span style="color:#f92672">=&lt;/span> Qwen2_5OmniForConditionalGeneration&lt;span style="color:#f92672">.&lt;/span>from_pretrained(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model_path,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config&lt;span style="color:#f92672">=&lt;/span>config,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> torch_dtype&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;auto&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> device_map&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;auto&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> local_files_only&lt;span style="color:#f92672">=&lt;/span>(model_path &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;Qwen/Qwen2.5-Omni-7B&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> processor &lt;span style="color:#f92672">=&lt;/span> Qwen2_5OmniProcessor&lt;span style="color:#f92672">.&lt;/span>from_pretrained(model_path)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 3. 系统提示（必须包含语音生成能力声明）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> system_prompt &lt;span style="color:#f92672">=&lt;/span> [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;role&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;system&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;content&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {&lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&amp;#34;&lt;/span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 4. 构建对话（纯文本输入）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> conversation &lt;span style="color:#f92672">=&lt;/span> system_prompt &lt;span style="color:#f92672">+&lt;/span> [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {&lt;span style="color:#e6db74">&amp;#34;role&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;user&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;content&amp;#34;&lt;/span>: [{&lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>: text_input}]}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 5. 处理输入数据&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text &lt;span style="color:#f92672">=&lt;/span> processor&lt;span style="color:#f92672">.&lt;/span>apply_chat_template(conversation, add_generation_prompt&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>, tokenize&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> audios, images, videos &lt;span style="color:#f92672">=&lt;/span> process_mm_info(conversation, use_audio_in_video&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 6. 生成语音&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> inputs &lt;span style="color:#f92672">=&lt;/span> processor(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text&lt;span style="color:#f92672">=&lt;/span>text, audio&lt;span style="color:#f92672">=&lt;/span>audios, images&lt;span style="color:#f92672">=&lt;/span>images, videos&lt;span style="color:#f92672">=&lt;/span>videos,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> return_tensors&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;pt&amp;#34;&lt;/span>, padding&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>, use_audio_in_video&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )&lt;span style="color:#f92672">.&lt;/span>to(model&lt;span style="color:#f92672">.&lt;/span>device, model&lt;span style="color:#f92672">.&lt;/span>dtype)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">with&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>no_grad():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text_ids, audio &lt;span style="color:#f92672">=&lt;/span> model&lt;span style="color:#f92672">.&lt;/span>generate(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">**&lt;/span>inputs,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> speaker&lt;span style="color:#f92672">=&lt;/span>speaker,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> do_sample&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>, &lt;span style="color:#75715e"># 启用采样模式以使用temperature/top_p&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> temperature&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.8&lt;/span>, &lt;span style="color:#75715e"># 控制随机性（0.5-1.0较自然）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> top_p&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.95&lt;/span>, &lt;span style="color:#75715e"># 核采样参数&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> max_new_tokens&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1024&lt;/span>, &lt;span style="color:#75715e"># 控制语音时长（约15秒）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> use_audio_in_video&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 7. 保存结果&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> os&lt;span style="color:#f92672">.&lt;/span>makedirs(os&lt;span style="color:#f92672">.&lt;/span>path&lt;span style="color:#f92672">.&lt;/span>dirname(output_audio_path), exist_ok&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sf&lt;span style="color:#f92672">.&lt;/span>write(output_audio_path, audio&lt;span style="color:#f92672">.&lt;/span>reshape(&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>)&lt;span style="color:#f92672">.&lt;/span>cpu()&lt;span style="color:#f92672">.&lt;/span>numpy(), samplerate&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">24000&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(&lt;span style="color:#e6db74">f&lt;/span>&lt;span style="color:#e6db74">&amp;#34;✅ 生成完成：&lt;/span>&lt;span style="color:#e6db74">{&lt;/span>output_audio_path&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(&lt;span style="color:#e6db74">f&lt;/span>&lt;span style="color:#e6db74">&amp;#34;📄 生成文本：&lt;/span>&lt;span style="color:#e6db74">{&lt;/span>processor&lt;span style="color:#f92672">.&lt;/span>batch_decode(text_ids, skip_special_tokens&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>)[&lt;span style="color:#ae81ff">0&lt;/span>]&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">if&lt;/span> __name__ &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#e6db74">&amp;#34;__main__&amp;#34;&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 示例输入（可替换为任意文本）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> input_text &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;你好，这是Qwen2.5-Omni的文本转语音示例。祝你使用愉快！&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 调用函数（指定输出路径和语音类型）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text_to_speech(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> input_text,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> output_audio_path&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;output/hello_qwen.wav&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> speaker&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Chelsie&amp;#34;&lt;/span> &lt;span style="color:#75715e"># 可选&amp;#34;Ethan&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>运行脚本&lt;/p></description></item></channel></rss>