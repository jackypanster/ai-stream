<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>SenseVoice on AI 避难所</title><link>https://jackypanster.github.io/ai-stream/tags/sensevoice/</link><description>Recent content in SenseVoice on AI 避难所</description><generator>Hugo -- 0.147.4</generator><language>en-us</language><lastBuildDate>Wed, 21 May 2025 15:43:08 +0800</lastBuildDate><atom:link href="https://jackypanster.github.io/ai-stream/tags/sensevoice/index.xml" rel="self" type="application/rss+xml"/><item><title>基于 FunAudioLLM/SenseVoiceSmall 搭建高效语音转录服务的实践之路</title><link>https://jackypanster.github.io/ai-stream/posts/howto-use-sensevoicesmall/</link><pubDate>Wed, 21 May 2025 15:43:08 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/howto-use-sensevoicesmall/</guid><description>详细记录了使用 SenseVoiceSmall 模型和 FastAPI 框架搭建语音转文本服务的过程，包括环境配置、核心代码、遇到的主要问题及解决方案，并对比了其他主流 ASR 模型。</description><content:encoded><![CDATA[<h2 id="项目概述">项目概述</h2>
<p>实现一个语音转录文本（ASR）的服务，目标是能够高效地将用户上传的音频文件转换为文字。出于中文语音的考虑，选择了来自 <code>FunAudioLLM</code> 的 <code>SenseVoiceSmall</code> 模型，它以其多语种支持、高效率以及集成的语音理解能力（如情感识别、事件检测）吸引了我。本文将详细记录从环境配置、核心功能实现到踩坑解决的全过程，并分享一些关于模型选型的思考。</p>
<p>完整代码已开源在 GitHub 仓库：<a href="https://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall">https://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall</a></p>
<p>项目需求文档（<code>prd.md</code>）关键信息如下：</p>
<ul>
<li><strong>模型</strong>: FunAudioLLM/SenseVoice (具体为 <code>SenseVoiceSmall</code>)</li>
<li><strong>本地模型路径</strong>: <code>/home/llm/model/iic/SenseVoiceSmall</code> (从 ModelScope 下载)</li>
<li><strong>API框架</strong>: FastAPI</li>
<li><strong>Python环境管理</strong>: <code>uv</code></li>
</ul>
<h2 id="环境配置">环境配置</h2>
<p>为了保持开发环境的纯净和高效，采用了 <code>uv</code> 来管理 Python 依赖。</p>
<ol>
<li>
<p><strong>创建虚拟环境</strong> (如果尚未创建):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>uv venv .venv
</span></span><span style="display:flex;"><span>source .venv/bin/activate
</span></span></code></pre></div></li>
<li>
<p><strong>安装核心依赖</strong>:
初始的 <code>requirements.txt</code> 包含了 <code>fastapi</code>, <code>uvicorn</code>, <code>python-multipart</code> 等基础库。后续根据模型加载和处理的需求，逐步添加了 <code>torch</code>, <code>torchaudio</code>, <code>numpy</code>, <code>transformers</code>, <code>sentencepiece</code>, 以及最终解决模型加载问题的核心库 <code>funasr</code>。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>uv pip install -r requirements.txt
</span></span></code></pre></div></li>
</ol>
<h2 id="核心功能实现概览">核心功能实现概览</h2>
<h3 id="项目结构">项目结构</h3>
<p>项目的主要结构包括：</p>
<ul>
<li><code>app/main.py</code>: FastAPI 应用入口，定义 API 路由和应用生命周期事件（如模型加载）。</li>
<li><code>app/models/sensevoice_loader.py</code>: 负责加载 <code>SenseVoiceSmall</code> 模型，采用单例模式。</li>
<li><code>app/services/asr_service.py</code>: 封装语音处理和模型推理的核心逻辑。</li>
<li><code>app/schemas.py</code>: 定义 API 的请求和响应数据模型 (Pydantic models)。</li>
</ul>
<h3 id="api-端点">API 端点</h3>
<p>关键的 API 端点设计为：</p>
<h4 id="post-asr_pure">POST /asr_pure</h4>
<ul>
<li><strong>Content-Type</strong>: <code>multipart/form-data</code></li>
<li><strong>Body</strong>: <code>file</code> (音频文件)</li>
</ul>
<p>返回转录后的文本及处理时间。</p>
<h2 id="踩坑与解决之路模型加载的曲折历程">踩坑与解决之路：模型加载的曲折历程</h2>
<p>在项目推进过程中，模型加载部分是遇到问题最多的地方，也是收获最多的地方。</p>
<h3 id="坑1hugging-face-autoclass-的-unrecognized-model">坑1：Hugging Face <code>AutoClass</code> 的 &ldquo;Unrecognized model&rdquo;</h3>
<p>最初，尝试使用 Hugging Face <code>transformers</code> 库通用的 <code>AutoProcessor.from_pretrained()</code> 和 <code>AutoModelForSpeechSeq2Seq.from_pretrained()</code> 来加载本地的 <code>SenseVoiceSmall</code> 模型文件。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># app/models/sensevoice_loader.py (早期尝试)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ...</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># self.processor = AutoProcessor.from_pretrained(MODEL_PATH)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># self.model = AutoModelForSpeechSeq2Seq.from_pretrained(MODEL_PATH)</span>
</span></span></code></pre></div><p>然而，服务启动时立即报错：</p>
<pre tabindex="0"><code>ValueError: Unrecognized model in /home/llm/model/iic/SenseVoiceSmall. Should have a model_type key in its config.json...
</code></pre><p>这个错误表明 <code>transformers</code> 的自动发现机制无法识别模型类型，通常是因为模型目录下的 <code>config.json</code> 文件缺少 <code>model_type</code> 字段，或者该模型需要特定的加载类。</p>
<h3 id="坑2转向-funasr-与-trust_remote_code-的初步探索">坑2：转向 <code>funasr</code> 与 <code>trust_remote_code</code> 的初步探索</h3>
<p>查阅 <code>FunAudioLLM/SenseVoice</code> 的官方文档后发现，推荐使用 <code>funasr</code> 库的 <code>AutoModel</code> 来加载 <code>SenseVoice</code> 系列模型。于是调整了代码：</p>
<ol>
<li><strong>添加 <code>funasr</code> 到 <code>requirements.txt</code></strong>。</li>
<li><strong>修改 <code>SenseVoiceLoader</code></strong>:
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># app/models/sensevoice_loader.py (引入 funasr)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> funasr <span style="color:#f92672">import</span> AutoModel
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ...</span>
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> AutoModel(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span>FUNASR_MODEL_NAME_OR_PATH, <span style="color:#75715e"># 即本地路径</span>
</span></span><span style="display:flex;"><span>    trust_remote_code<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    device<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>device
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div>同时，<code>asr_service.py</code> 中的推理逻辑也相应调整为调用 <code>funasr</code> 模型对象的 <code>.generate()</code> 方法。</li>
</ol>
<p>本以为这样能解决问题，但启动时又遇到了新的日志：</p>
<pre tabindex="0"><code>Loading remote code failed: model, No module named &#39;model&#39;
</code></pre><p>尽管这条日志出现，但后续的 API 调用测试居然成功了！这让我非常困惑。</p>
<h3 id="坑3remote_code-参数与-modelpy-文件的幻影">坑3：<code>remote_code</code> 参数与 <code>model.py</code> 文件的“幻影”</h3>
<p>深入研究 <code>funasr</code> 和 <code>SenseVoice</code> 的文档，注意到对于包含自定义代码（如 <code>model.py</code>）的模型，除了 <code>trust_remote_code=True</code>，有时还需要明确指定 <code>remote_code</code> 参数。</p>
<p>我检查了 Hugging Face 仓库 <code>FunAudioLLM/SenseVoiceSmall</code> (<a href="https://huggingface.co/FunAudioLLM/SenseVoiceSmall/tree/main">https://huggingface.co/FunAudioLLM/SenseVoiceSmall/tree/main</a>)，发现其文件列表中确实包含一个 <code>model.py</code>。因此，我尝试在 <code>AutoModel</code> 调用中加入 <code>remote_code=&quot;model.py&quot;</code>。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># app/models/sensevoice_loader.py (尝试指定 remote_code)</span>
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> AutoModel(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span>FUNASR_MODEL_NAME_OR_PATH,
</span></span><span style="display:flex;"><span>    trust_remote_code<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    remote_code<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;model.py&#34;</span>, <span style="color:#75715e"># &lt;--- 新增</span>
</span></span><span style="display:flex;"><span>    device<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>device
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>结果，<code>No module named 'model'</code> 的错误依旧。</p>
<h3 id="解决方案澄清-modelscope-与-hugging-face-的模型文件差异">解决方案：澄清 ModelScope 与 Hugging Face 的模型文件差异</h3>
<p>本地模型 <code>/home/llm/model/iic/SenseVoiceSmall</code> 是从 <strong>ModelScope</strong> (<a href="https://www.modelscope.cn/models/iic/SenseVoiceSmall/files">https://www.modelscope.cn/models/iic/SenseVoiceSmall/files</a>) 下载的，而非直接 clone Hugging Face 的仓库。通过 <code>ls -al /home/llm/model/iic/SenseVoiceSmall/</code> 查看本地文件，<strong>发现确实没有 <code>model.py</code> 文件！</strong></p>
<p>这解释了为什么指定 <code>remote_code=&quot;model.py&quot;</code> 依然报错。ModelScope 提供的模型包可能与 Hugging Face 仓库中的文件结构不完全一致，特别是对于这种依赖 <code>funasr</code> 特定加载方式的模型。</p>
<p><strong>最终的正确配置</strong>：移除 <code>remote_code</code> 参数，但保留 <code>trust_remote_code=True</code>。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># app/models/sensevoice_loader.py (最终正确配置)</span>
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> AutoModel(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span>FUNASR_MODEL_NAME_OR_PATH,
</span></span><span style="display:flex;"><span>    trust_remote_code<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, <span style="color:#75715e"># 保留，funasr 可能仍需此权限处理 ModelScope 模型</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># remote_code=&#34;model.py&#34;, # 移除，因为本地 ModelScope 版本无此文件</span>
</span></span><span style="display:flex;"><span>    device<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>device
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>这样修改后，服务启动时仍然会打印 <code>Loading remote code failed: model, No module named 'model'</code>，但 API 调用完全正常！</p>
<p><strong>原因分析</strong>：<code>funasr</code> 在 <code>trust_remote_code=True</code> 时，会优先尝试加载自定义代码。如果本地模型路径（如从 ModelScope 下载的）没有 <code>model.py</code>，这个尝试会失败并打印日志。但随后，<code>funasr</code> 能够识别出这是一个有效的 ModelScope 模型路径，并转用其内部的标准加载流程成功加载模型。因此，该日志在这种情况下是良性的。</p>
<h2 id="模型对比与选型思考">模型对比与选型思考</h2>
<p>在解决问题的过程中，也探讨了 <code>FunAudioLLM/SenseVoiceSmall</code> 与其他主流 ASR 模型的对比：</p>
<ul>
<li>
<p><strong>OpenAI Whisper 系列</strong> (如 <code>whisper-large-v3</code>):</p>
<ul>
<li><strong>优势</strong>: 极高的准确率，强大的多语言能力，庞大的社区。</li>
<li><strong>劣势</strong>: 推理速度相对较慢（尤其大模型），不直接提供情感/事件检测。</li>
</ul>
</li>
<li>
<p><strong>Wav2Vec2 系列</strong>:</p>
<ul>
<li><strong>优势</strong>: 自监督学习典范，大量特定语言微调模型。</li>
<li><strong>劣势</strong>: 基础模型功能相对单一。</li>
</ul>
</li>
</ul>
<h3 id="sensevoicesmall"><strong><code>SenseVoiceSmall</code> 的核心优势</strong></h3>
<ol>
<li>
<p><strong>高效推理</strong>：其模型卡声称采用非自回归端到端框架，比 Whisper-Large 快15倍。这对于需要低延迟的应用至关重要。</p>
</li>
<li>
<p><strong>多任务集成</strong>：内置 ASR、LID（语种识别）、SER（情感识别）、AED（事件检测）。如果应用场景需要这些附加信息，<code>SenseVoiceSmall</code> 提供了一站式解决方案。</p>
</li>
<li>
<p><strong>特定语言优化</strong>：在中文、粤语等语言上表现突出。</p>
</li>
</ol>
<h3 id="结论"><strong>结论</strong></h3>
<p>没有绝对的“最好”，只有“最适合”。</p>
<ul>
<li>若追求极致准确性和最广语言覆盖，且对延迟不敏感，Whisper 仍是首选。</li>
<li>若对<strong>推理效率、集成的多任务语音理解（特别是情感/事件）或中文等特定场景有高要求</strong>，<code>SenseVoiceSmall</code> 是一个极具竞争力的选择。</li>
</ul>
<p>目前选择的 <code>SenseVoiceSmall</code>，尤其是在确认了其 ModelScope 版本能够顺畅运行后，对于我的项目目标来说是一个合适的起点。</p>
<h2 id="当前状态与展望">当前状态与展望</h2>
<p>目前，基于 <code>FunAudioLLM/SenseVoiceSmall</code> 和 FastAPI 的语音转录服务已成功搭建并能正确处理请求。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>$ curl -X POST <span style="color:#e6db74">&#34;http://&lt;your_server_ip&gt;:8888/asr_pure&#34;</span> -F <span style="color:#e6db74">&#34;file=@test_audio.wav&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">{</span><span style="color:#e6db74">&#34;text&#34;</span>:<span style="color:#e6db74">&#34;太好了，那接下来咱们可以试试其他功能了。比如说你想测试一下语音合成的效果怎么样，或者是看看有没有什么新的语音处理功能出来啦。😔&#34;</span>,<span style="color:#e6db74">&#34;status&#34;</span>:<span style="color:#e6db74">&#34;success&#34;</span>,<span style="color:#e6db74">&#34;processing_time_ms&#34;</span>:503.39...<span style="color:#f92672">}</span>
</span></span></code></pre></div><h3 id="后续可优化的方向"><strong>后续可优化的方向</strong></h3>
<ul>
<li><strong>性能优化</strong>：进一步测试并发处理能力，考虑多 worker 配置。</li>
<li><strong>错误处理与日志</strong>：完善更细致的错误捕获和日志记录。</li>
<li><strong>功能扩展</strong>：如果需要，可以利用 <code>SenseVoiceSmall</code> 的情感识别和事件检测能力。</li>
<li><strong>VAD 集成</strong>：对于长音频，考虑在 <code>funasr.AutoModel</code> 加载时集成 VAD (Voice Activity Detection) 功能，以实现自动分段处理，提升长音频处理的稳定性和效率。</li>
<li><strong>异步处理与队列</strong>：对于高并发场景，引入消息队列和异步任务处理。</li>
</ul>
]]></content:encoded></item></channel></rss>