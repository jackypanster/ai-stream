<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Gpu-Deployment on AI é¿éš¾æ‰€</title><link>https://jackypanster.github.io/ai-stream/tags/gpu-deployment/</link><description>Recent content in Gpu-Deployment on AI é¿éš¾æ‰€</description><generator>Hugo -- 0.148.1</generator><language>en-us</language><lastBuildDate>Wed, 16 Jul 2025 13:19:48 +0800</lastBuildDate><atom:link href="https://jackypanster.github.io/ai-stream/tags/gpu-deployment/index.xml" rel="self" type="application/rss+xml"/><item><title>Qwen3-32B-AWQ vLLM å¤šå¡ 2080 Ti æé™éƒ¨ç½²å®æˆ˜</title><link>https://jackypanster.github.io/ai-stream/posts/qwen3-32b-2080ti-vllm-deploy/</link><pubDate>Wed, 16 Jul 2025 13:19:48 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/qwen3-32b-2080ti-vllm-deploy/</guid><description>&lt;blockquote>
&lt;p>ä½¿ç”¨æ—§æ˜¾å¡ä¹Ÿèƒ½è·‘ 32B å¤§æ¨¡å‹ï¼Ÿæœ¬æ–‡æ‰‹æŠŠæ‰‹æ¼”ç¤ºå¦‚ä½•åœ¨ &lt;strong>4Ã—RTX 2080 Ti (å…± 88 GB æ˜¾å­˜)&lt;/strong> æœåŠ¡å™¨ä¸Šï¼Œé€šè¿‡ vLLM 0.8.5 + AWQ é‡åŒ–ï¼Œè·‘èµ· &lt;strong>Qwen3-32B&lt;/strong> å¹¶æ”¯æŒ &lt;strong>32 K tokens&lt;/strong> è¶…é•¿ä¸Šä¸‹æ–‡ä¸é«˜ååæ¨ç†ã€‚å…¨æ–‡è®°å½•äº†è¸©å‘è¿‡ç¨‹ä¸å‚æ•°æƒè¡¡ï¼Œå¸Œæœ›ç»™åŒæ ·é¢„ç®—æœ‰é™ã€ç¡¬ä»¶å—é™çš„å·¥ç¨‹å¸ˆå¸¦æ¥å€Ÿé‰´ã€‚&lt;/p>&lt;/blockquote>
&lt;p>{{.TableOfContents}}&lt;/p>
&lt;h2 id="1-é¡¹ç›®èƒŒæ™¯">1 é¡¹ç›®èƒŒæ™¯&lt;/h2>
&lt;ul>
&lt;li>ä¸»è§’ï¼š&lt;code>Qwen3-32B-AWQ&lt;/code> é‡åŒ–æ¨¡å‹ ï¼ˆâ‰ˆ 18 GBï¼‰&lt;/li>
&lt;li>ç›®æ ‡ï¼šåœ¨æ¶ˆè´¹çº§ &lt;strong>Turing&lt;/strong> æ¶æ„æ˜¾å¡ï¼ˆ2080 Tiï¼‰ä¸Šæœ€å¤§åŒ–åˆ©ç”¨æ˜¾å­˜ä¸ååã€‚&lt;/li>
&lt;li>æ¡†æ¶ï¼š&lt;code>vLLM 0.8.5&lt;/code> (openai-compatible server)&lt;/li>
&lt;li>å–èˆï¼šç‰ºç‰²éƒ¨åˆ†å»¶è¿Ÿ / ç¨³å®šæ€§ â†’ æ¢å– &lt;strong>åå + ä¸Šä¸‹æ–‡é•¿åº¦&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="2-ç¡¬ä»¶ä¸ç³»ç»Ÿç¯å¢ƒ">2 ç¡¬ä»¶ä¸ç³»ç»Ÿç¯å¢ƒ&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>ç»„ä»¶&lt;/th>
&lt;th>è§„æ ¼&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>GPU&lt;/td>
&lt;td>4 Ã— RTX 2080 Ti, 22 GB &lt;em>each&lt;/em>, Compute Capability 7.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CPU&lt;/td>
&lt;td>â‰¥ 56 cores (vLLM çº¿ç¨‹å¯åƒæ»¡)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RAM&lt;/td>
&lt;td>512 GB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Storage&lt;/td>
&lt;td>NVMe SSD 2 TB (æ¨¡å‹ + KV ç¼“å†²)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OS&lt;/td>
&lt;td>Ubuntu 24.04&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Driver&lt;/td>
&lt;td>NVIDIA 570.153.02&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CUDA&lt;/td>
&lt;td>12.8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="21-nvidia-smi-åŸºçº¿ä¿¡æ¯">2.1 NVIDIA-SMI åŸºçº¿ä¿¡æ¯&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>nvidia-smi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Wed Jul &lt;span style="color:#ae81ff">16&lt;/span> 13:27:17 &lt;span style="color:#ae81ff">2025&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----------------------------------------------------------------------------------------+
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| NVIDIA-SMI 570.153.02 Driver Version: 570.153.02 CUDA Version: 12.8 |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----------------------------------------------------------------------------------------+
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>å¯ä»¥çœ‹åˆ°é©±åŠ¨ä¸ CUDA ç‰ˆæœ¬ä¸ä¸Šè¡¨ä¸€è‡´ï¼Œç¡®è®¤ç¯å¢ƒæ— åå·®ã€‚&lt;/p></description><content:encoded><![CDATA[<blockquote>
<p>ä½¿ç”¨æ—§æ˜¾å¡ä¹Ÿèƒ½è·‘ 32B å¤§æ¨¡å‹ï¼Ÿæœ¬æ–‡æ‰‹æŠŠæ‰‹æ¼”ç¤ºå¦‚ä½•åœ¨ <strong>4Ã—RTX 2080 Ti (å…± 88 GB æ˜¾å­˜)</strong> æœåŠ¡å™¨ä¸Šï¼Œé€šè¿‡ vLLM 0.8.5 + AWQ é‡åŒ–ï¼Œè·‘èµ· <strong>Qwen3-32B</strong> å¹¶æ”¯æŒ <strong>32 K tokens</strong> è¶…é•¿ä¸Šä¸‹æ–‡ä¸é«˜ååæ¨ç†ã€‚å…¨æ–‡è®°å½•äº†è¸©å‘è¿‡ç¨‹ä¸å‚æ•°æƒè¡¡ï¼Œå¸Œæœ›ç»™åŒæ ·é¢„ç®—æœ‰é™ã€ç¡¬ä»¶å—é™çš„å·¥ç¨‹å¸ˆå¸¦æ¥å€Ÿé‰´ã€‚</p></blockquote>
<p>{{.TableOfContents}}</p>
<h2 id="1-é¡¹ç›®èƒŒæ™¯">1 é¡¹ç›®èƒŒæ™¯</h2>
<ul>
<li>ä¸»è§’ï¼š<code>Qwen3-32B-AWQ</code> é‡åŒ–æ¨¡å‹  ï¼ˆâ‰ˆ 18 GBï¼‰</li>
<li>ç›®æ ‡ï¼šåœ¨æ¶ˆè´¹çº§ <strong>Turing</strong> æ¶æ„æ˜¾å¡ï¼ˆ2080 Tiï¼‰ä¸Šæœ€å¤§åŒ–åˆ©ç”¨æ˜¾å­˜ä¸ååã€‚</li>
<li>æ¡†æ¶ï¼š<code>vLLM 0.8.5</code> (openai-compatible server)</li>
<li>å–èˆï¼šç‰ºç‰²éƒ¨åˆ†å»¶è¿Ÿ / ç¨³å®šæ€§ â†’ æ¢å– <strong>åå + ä¸Šä¸‹æ–‡é•¿åº¦</strong></li>
</ul>
<h2 id="2-ç¡¬ä»¶ä¸ç³»ç»Ÿç¯å¢ƒ">2 ç¡¬ä»¶ä¸ç³»ç»Ÿç¯å¢ƒ</h2>
<table>
  <thead>
      <tr>
          <th>ç»„ä»¶</th>
          <th>è§„æ ¼</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>GPU</td>
          <td>4 Ã— RTX 2080 Ti, 22 GB <em>each</em>, Compute Capability 7.5</td>
      </tr>
      <tr>
          <td>CPU</td>
          <td>â‰¥ 56 cores (vLLM çº¿ç¨‹å¯åƒæ»¡)</td>
      </tr>
      <tr>
          <td>RAM</td>
          <td>512 GB</td>
      </tr>
      <tr>
          <td>Storage</td>
          <td>NVMe SSD 2 TB (æ¨¡å‹ + KV ç¼“å†²)</td>
      </tr>
      <tr>
          <td>OS</td>
          <td>Ubuntu 24.04</td>
      </tr>
      <tr>
          <td>Driver</td>
          <td>NVIDIA 570.153.02</td>
      </tr>
      <tr>
          <td>CUDA</td>
          <td>12.8</td>
      </tr>
  </tbody>
</table>
<h3 id="21-nvidia-smi-åŸºçº¿ä¿¡æ¯">2.1 NVIDIA-SMI åŸºçº¿ä¿¡æ¯</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>nvidia-smi
</span></span><span style="display:flex;"><span>Wed Jul <span style="color:#ae81ff">16</span> 13:27:17 <span style="color:#ae81ff">2025</span>
</span></span><span style="display:flex;"><span>+-----------------------------------------------------------------------------------------+
</span></span><span style="display:flex;"><span>| NVIDIA-SMI 570.153.02             Driver Version: 570.153.02     CUDA Version: 12.8     |
</span></span><span style="display:flex;"><span>+-----------------------------------------------------------------------------------------+
</span></span></code></pre></div><blockquote>
<p>å¯ä»¥çœ‹åˆ°é©±åŠ¨ä¸ CUDA ç‰ˆæœ¬ä¸ä¸Šè¡¨ä¸€è‡´ï¼Œç¡®è®¤ç¯å¢ƒæ— åå·®ã€‚</p></blockquote>
<blockquote>
<p><strong>ä¸ºä»€ä¹ˆ 2080 Tiï¼Ÿ</strong> äºŒæ‰‹å¸‚åœºä»·æ ¼å‹å¥½ï¼Œä½† Flash-Attention-2 ä¸æ”¯æŒï¼Œéœ€è¦è‡ªå·±ç¼–è¯‘ flash-attn-1 æˆ–ä½¿ç”¨ XFormersã€‚</p></blockquote>
<h2 id="3-å¿«é€Ÿéƒ¨ç½²æ­¥éª¤æ¦‚è§ˆ">3 å¿«é€Ÿéƒ¨ç½²æ­¥éª¤æ¦‚è§ˆ</h2>
<ol>
<li>ä¸‹è½½å¹¶è§£å‹ <code>Qwen3-32B-AWQ</code> æƒé‡è‡³ <code>/home/llm/model/qwen/Qwen3-32B-AWQ</code>ã€‚</li>
<li>ï¼ˆå¯é€‰ï¼‰ç¼–è¯‘ <code>flash-attn-1</code> ä»¥æ›¿ä»£åŸç”Ÿ attentionã€‚</li>
<li>æ‹‰å–å®˜æ–¹ vLLM é•œåƒ <code>vllm/vllm-openai:v0.8.5</code>ã€‚</li>
<li>æŒ‰ä¸‹æ–‡ <strong>run.sh</strong> å‚æ•°å¯åŠ¨å®¹å™¨ã€‚</li>
</ol>
<p>ä¸‹é¢æ‹†è§£æ¯ä¸€æ­¥çš„æŠ€æœ¯ç»†èŠ‚ã€‚</p>
<h3 id="31-æ¨¡å‹å‡†å¤‡">3.1 æ¨¡å‹å‡†å¤‡</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>mkdir -p /home/llm/model/qwen
</span></span><span style="display:flex;"><span><span style="color:#75715e"># çœç•¥ huggingface-cli ç™»å½•æ­¥éª¤</span>
</span></span><span style="display:flex;"><span>huggingface-cli download Qwen/Qwen3-32B-AWQ --local-dir /home/llm/model/qwen/Qwen3-32B-AWQ --local-dir-use-symlinks False
</span></span></code></pre></div><h3 id="32-ç¼–è¯‘-flash-attention-12080-ti-ä¸“ç”¨">3.2 ç¼–è¯‘ Flash-Attention-1ï¼ˆ2080 Ti ä¸“ç”¨ï¼‰</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># CUDA 12.x + Python 3.12 ç¤ºä¾‹</span>
</span></span><span style="display:flex;"><span>python3 -m pip install --upgrade pip
</span></span><span style="display:flex;"><span>python3 -m pip install ninja packaging cmake
</span></span><span style="display:flex;"><span><span style="color:#75715e"># å¼ºåˆ¶æºç ç¼–è¯‘ï¼Œç¡®ä¿ç”Ÿæˆ sm75 kernel</span>
</span></span><span style="display:flex;"><span>FLASH_ATTENTION_FORCE_BUILD<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  python3 -m pip install flash-attn --no-build-isolation --no-binary :all:
</span></span></code></pre></div><blockquote>
<p><strong>å®¹å™¨ç”¨æˆ·è¯·æ³¨æ„</strong>ï¼šå¦‚æœä½¿ç”¨ä¸‹æ–‡çš„å®˜æ–¹ vLLM Docker é•œåƒï¼Œéœ€åœ¨ <em>å®¹å™¨å†…éƒ¨</em> æˆ–è‡ªå»º Dockerfile å®ŒæˆåŒæ ·çš„ flash-attn-1 ç¼–è¯‘ï¼ˆæˆ–å°†å·²ç¼–è¯‘å¥½çš„ wheel å¤åˆ¶è¿›é•œåƒï¼‰ã€‚å®¿ä¸»æœºå®‰è£…çš„ Python åŒ…ä¸ä¼šè¢«å®¹å™¨ç¯å¢ƒè¯»å–ã€‚</p></blockquote>
<h4 id="321-æ— éœ€é‡å»ºå¤§é•œåƒçš„æŠ˜ä¸­åšæ³•">3.2.1 æ— éœ€é‡å»ºå¤§é•œåƒçš„æŠ˜ä¸­åšæ³•</h4>
<table>
  <thead>
      <tr>
          <th>åšæ³•</th>
          <th>è¯´æ˜</th>
          <th>é¢å¤–ä½“ç§¯</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>å¯åŠ¨æ—¶ä¸´æ—¶ <code>--pip-install</code></strong></td>
          <td>vLLM â‰¥0.9 æ”¯æŒ <code>--pip</code> å‚æ•°ï¼Œå®¹å™¨å¯åŠ¨æ—¶å³åœ¨çº¿ç¼–è¯‘ <code>flash-attn</code></td>
          <td>0ï¼ˆç¼–è¯‘äº§ç‰©ç¼“å­˜äº volumeï¼‰</td>
      </tr>
      <tr>
          <td><strong>å®¿ä¸»æœºå…ˆç¼–è¯‘ wheel</strong></td>
          <td><code>pip wheel flash-attn -w /tmp/wheels</code>ï¼Œè¿è¡Œæ—¶æŒ‚è½½ <code>/tmp/wheels</code> å¹¶ <code>pip install</code></td>
          <td>~30-40 MB</td>
      </tr>
      <tr>
          <td><strong>æ”¹ç”¨ XFormers</strong></td>
          <td>åŠ  <code>--xformers</code>ï¼Œæ€§èƒ½ç•¥ä½äº flash-attn-1ï¼Œä½†å…ç¼–è¯‘</td>
          <td>0</td>
      </tr>
      <tr>
          <td><strong>ä¿æŒé»˜è®¤ attention</strong></td>
          <td>å¯¹ååè¦æ±‚ä¸€èˆ¬çš„åœºæ™¯å¯æ¥å—</td>
          <td>0</td>
      </tr>
  </tbody>
</table>
<blockquote>
<p>æ¨èé¡ºåºï¼šä¸´æ—¶ <code>--pip</code> &gt; wheel æŒ‚è½½ &gt; XFormers &gt; é»˜è®¤ Attentionã€‚æŒ‰ä¸šåŠ¡å¯¹æ€§èƒ½ &amp; ç®€æ˜“åº¦çš„æƒè¡¡è‡ªè¡Œé€‰æ‹©ã€‚</p></blockquote>
<p>éªŒè¯ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python3 - <span style="color:#e6db74">&lt;&lt;&#39;PY&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">import flash_attn, torch, platform
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">print(&#39;flash-attn&#39;, flash_attn.__version__, &#39;torch&#39;, torch.__version__, &#39;python&#39;, platform.python_version())
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">PY</span>
</span></span></code></pre></div><h3 id="33-å¯åŠ¨è„šæœ¬-runsh">3.3 å¯åŠ¨è„šæœ¬ <code>run.sh</code></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e">#!/usr/bin/env bash
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>docker run -d <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --runtime<span style="color:#f92672">=</span>nvidia --gpus<span style="color:#f92672">=</span>all --name coder <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -v /home/llm/model/qwen/Qwen3-32B-AWQ:/model/Qwen3-32B-AWQ <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -p 8888:8000 --cpuset-cpus 0-55 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --ulimit memlock<span style="color:#f92672">=</span>-1 --ulimit stack<span style="color:#f92672">=</span><span style="color:#ae81ff">67108864</span> --restart always --ipc<span style="color:#f92672">=</span>host <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  vllm/vllm-openai:v0.8.5 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --model /model/Qwen3-32B-AWQ --served-model-name coder <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --tensor-parallel-size <span style="color:#ae81ff">4</span> --quantization awq --dtype auto <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --max-model-len <span style="color:#ae81ff">32768</span> --max-num-batched-tokens <span style="color:#ae81ff">32768</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --gpu-memory-utilization 0.96 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --block-size <span style="color:#ae81ff">16</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --enable-prefix-caching <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --swap-space <span style="color:#ae81ff">64</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --max-num-seqs <span style="color:#ae81ff">64</span>
</span></span></code></pre></div><blockquote>
<p><strong>å®¹å™¨ vs æœ¬æœº</strong>ï¼šç›´æ¥è£¸è·‘äº¦å¯ï¼Œæ ¸å¿ƒå‚æ•°å®Œå…¨ç›¸åŒã€‚å®¹å™¨ä¾¿äºå¤ç°ä¸å¿«é€Ÿé‡å¯ã€‚</p></blockquote>
<h2 id="4-å…³é”®è¿è¡Œå‚æ•°æ‹†è§£">4 å…³é”®è¿è¡Œå‚æ•°æ‹†è§£</h2>
<table>
  <thead>
      <tr>
          <th>å‚æ•°</th>
          <th>ä½œç”¨ / è°ƒä¼˜æ€è·¯</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><code>--tensor-parallel-size 4</code></td>
          <td>4 å¡åˆ‡åˆ†æ¨¡å‹å‚æ•°ï¼Œ2080 Ti å•å¡æ˜¾å­˜æœ‰é™å¿…é¡»æ‹†åˆ†ã€‚</td>
      </tr>
      <tr>
          <td><code>--quantization awq</code></td>
          <td>å¯ç”¨ <strong>AWQ</strong> æƒé‡é‡åŒ–ï¼Œæ˜¾å­˜â‰ˆå†é™ 40%ã€‚æŸäº›é•¿æ–‡æœ¬åœºæ™¯ä¸‹ FP16 ä»æ›´å¿«ï¼Œéœ€å®æµ‹ã€‚</td>
      </tr>
      <tr>
          <td><code>--max-model-len 32768</code></td>
          <td>æ”¯æŒ 32 K tokensï¼›å¤§å¹…å¢åŠ  KV Cacheï¼Œéœ€è¦é…åˆ <code>--swap-space</code>ã€‚</td>
      </tr>
      <tr>
          <td><code>--max-num-batched-tokens 32768</code></td>
          <td>å•æ‰¹æ¬¡ tokens ä¸Šé™ã€‚åå / æ˜¾å­˜ trade-offã€‚</td>
      </tr>
      <tr>
          <td><code>--gpu-memory-utilization 0.96</code></td>
          <td>è¿‘ä¹åƒæ»¡æ˜¾å­˜ï¼Œè°¨æ…è°ƒï¼›ç•™ 0.04 ä½œä½™é‡ã€‚</td>
      </tr>
      <tr>
          <td><code>--block-size 16</code></td>
          <td>KV Cache åˆ†å—ã€‚å—è¶Šå°è¶Šçµæ´»ï¼Œç®¡ç†å¼€é”€ç¨å¢ã€‚</td>
      </tr>
      <tr>
          <td><code>--enable-prefix-caching</code></td>
          <td>é«˜å¤ç”¨ prompt å‘½ä¸­ç‡å¯&gt;90%ï¼Œæ˜¾è‘—æå‡é•¿å¯¹è¯ååã€‚</td>
      </tr>
      <tr>
          <td><code>--swap-space 64</code></td>
          <td>å…è®¸ 64 GB CPU RAM ä½œä¸º KV Cache æº¢å‡ºã€‚swap å¤§å»¶è¿Ÿé«˜ã€‚</td>
      </tr>
      <tr>
          <td><code>--max-num-seqs 64</code></td>
          <td>æ§åˆ¶å¹¶å‘åºåˆ—æ•°ã€‚è¶Šå¤§ååé«˜ï¼Œé•¿æ–‡æœ¬ OOM é£é™©ä¹Ÿé«˜ã€‚</td>
      </tr>
  </tbody>
</table>
<h2 id="5-api-è°ƒç”¨èŒƒä¾‹">5 API è°ƒç”¨èŒƒä¾‹</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl http://localhost:8888/v1/chat/completions <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -H <span style="color:#e6db74">&#39;Content-Type: application/json&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -d <span style="color:#e6db74">&#39;{
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;model&#34;: &#34;coder&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;messages&#34;: [
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;ä½ æ˜¯ä¸€ä¸ªèªæ˜çš„ AI åŠ©æ‰‹ã€‚&#34;},
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;è¯·å†™ä¸€ä¸ª Python å†’æ³¡æ’åºã€‚&#34;}
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    ],
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;max_tokens&#34;: 512,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;temperature&#34;: 0.2
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  }&#39;</span>
</span></span></code></pre></div><ul>
<li><code>max_tokens</code> å»ºè®® 512 ~ 2048ï¼›æé™ context æ—¶è¿‡å¤§æ˜“ <strong>OOM</strong>ã€‚</li>
<li><code>stream=true</code> å¯è·å¾—æµå¼è¾“å‡ºï¼›è€—æ—¶æ›´çŸ­ï¼Œå ç”¨æ›´ä½ã€‚</li>
</ul>
<h2 id="6-æ€§èƒ½å‹æ¦¨æŠ€å·§">6 æ€§èƒ½å‹æ¦¨æŠ€å·§</h2>
<ol>
<li><strong>AWQ vs FP16</strong>
<ul>
<li>æŸäº›æ¨ç†é˜¶æ®µ AWQ kernel å°šæœªä¼˜åŒ–ï¼ŒğŸš€ ç»“æœ FP16 æ›´å¿«ã€‚å®æµ‹äºŒé€‰ä¸€ã€‚</li>
</ul>
</li>
<li><strong>Flash-Attn-1 / XFormers</strong>
<ul>
<li>2080 Ti æ—  <strong>Flash-Attn-2</strong>ï¼›ç¼–è¯‘ v1 æˆ–ä½¿ç”¨ XFormers çš†å¯ã€‚</li>
</ul>
</li>
<li><strong>KV Cache &amp; Swap</strong>
<ul>
<li>ç›‘æ§ <code>gpu_kv_cache</code> ä¸ <code>swap_used</code> ä¸¤é¡¹ï¼›é•¿æ–‡æœ¬æ˜“ç‚¸ã€‚</li>
</ul>
</li>
<li><strong>å¤šå®ä¾‹åˆ†å¡</strong>
<ul>
<li>æŠŠ 4 å¡æ‹†æˆ 2 Ã— 2 å¡å®ä¾‹ï¼Œå¯æé«˜ GPU åˆ©ç”¨ç‡ (ä¸åŒä¸šåŠ¡è´Ÿè½½)ã€‚</li>
</ul>
</li>
<li><strong>è‡ªåŠ¨é™çº§</strong>
<ul>
<li>åœ¨ API å±‚æ£€æµ‹ OOM â†’ è‡ªåŠ¨ç¼©çŸ­ä¸Šä¸‹æ–‡ or è°ƒå°å¹¶å‘ï¼Œä¿è¯å¯ç”¨æ€§ã€‚</li>
</ul>
</li>
</ol>
<h2 id="7-å¸¸è§é—®é¢˜é€ŸæŸ¥">7 å¸¸è§é—®é¢˜é€ŸæŸ¥</h2>
<table>
  <thead>
      <tr>
          <th>ç—‡çŠ¶</th>
          <th>è§£å†³æ–¹æ¡ˆ</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>è¿”å›ä¸å®Œæ•´/æˆªæ–­</strong></td>
          <td>å¢å¤§ <code>max_tokens</code>ï¼›ç¼©çŸ­è¾“å…¥ï¼›æ£€æŸ¥æ—¥å¿—ä¸­ <code>context_window</code>ã€‚</td>
      </tr>
      <tr>
          <td><strong>CUDA OOM / å®¹å™¨å´©æºƒ</strong></td>
          <td>é™ä½ <code>max-model-len</code>ã€<code>max-num-batched-tokens</code>ï¼›å¢å¤§ <code>swap-space</code>ã€‚</td>
      </tr>
      <tr>
          <td><strong>æ¨ç†é€Ÿåº¦æ…¢</strong></td>
          <td>ç¡®è®¤ flash-attn-1 å·²å¯ç”¨ï¼›å¹¶å‘ä¸è¦è¿‡é«˜ï¼›å°è¯• FP16ã€‚</td>
      </tr>
      <tr>
          <td><strong>NCCL æ­»é” / hang</strong></td>
          <td>åŠ  <code>--disable-custom-all-reduce</code> æˆ–å‡çº§ NCCLã€‚</td>
      </tr>
  </tbody>
</table>
<h2 id="8-å®æˆ˜å‹æµ‹ç»“æœ-10-å¹¶å‘--32-k-prompt">8 å®æˆ˜å‹æµ‹ç»“æœ (10 å¹¶å‘ Â· 32 K prompt)</h2>
<table>
  <thead>
      <tr>
          <th>æŒ‡æ ‡</th>
          <th>æ•°å€¼</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Avg prompt throughput</td>
          <td><strong>63 K tokens/s</strong></td>
      </tr>
      <tr>
          <td>Avg generation throughput</td>
          <td><strong>57 tokens/s</strong></td>
      </tr>
      <tr>
          <td>å¹³å‡å“åº”æ—¶é—´</td>
          <td><strong>5.63 s</strong></td>
      </tr>
      <tr>
          <td>GPU KV Cache å ç”¨</td>
          <td>15 %</td>
      </tr>
      <tr>
          <td>Prefix cache å‘½ä¸­ç‡</td>
          <td>94 %</td>
      </tr>
      <tr>
          <td>é”™è¯¯ / OOM</td>
          <td>0</td>
      </tr>
  </tbody>
</table>
<blockquote>
<p>é«˜ååå½’åŠŸäºï¼š1) prefix caching 2) AWQ é‡åŒ– 3) è¿‘ä¹æ»¡æ˜¾å­˜åˆ©ç”¨ã€‚</p></blockquote>
<h3 id="ç»“æœè§£è¯»">ç»“æœè§£è¯»</h3>
<ul>
<li><strong>åå</strong>ï¼šè¾“å…¥é˜¶æ®µ 63K tokens/sï¼Œç”Ÿæˆé˜¶æ®µ 57 tokens/sï¼Œå¯¹ 32B æ¨¡å‹éå¸¸å¯è§‚ã€‚</li>
<li><strong>èµ„æº</strong>ï¼šGPU KV Cache ä»… 15 %ï¼›ç³»ç»Ÿè¿˜å¯ä¸Šè°ƒå¹¶å‘ / ä¸Šä¸‹æ–‡ã€‚</li>
<li><strong>ç¨³å®š</strong>ï¼šé•¿æ—¶é—´å‹æµ‹æ—  OOM / pendingï¼›å®¹å™¨ restart=always å¯å…œåº•ã€‚</li>
</ul>
<h2 id="9-æ€»ç»“--å»ºè®®">9 æ€»ç»“ &amp; å»ºè®®</h2>
<p>ä½¿ç”¨æ—§ä¸–ä»£æ˜¾å¡å¹¶ä¸æ„å‘³ç€æ”¾å¼ƒå¤§æ¨¡å‹ã€‚é€šè¿‡ <strong>vLLM + AWQ + Prefix Cache</strong> ç­‰ç»„åˆæ‹³ï¼Œ4Ã—2080 Ti ä¾æ—§èƒ½å¤Ÿæ”¯æ’‘ <strong>Qwen3-32B</strong> çš„ 32 K è¶…é•¿ä¸Šä¸‹æ–‡æ¨ç†ã€‚</p>
<ul>
<li><strong>ç§‘ç ” / æµ‹è¯•</strong> åœºæ™¯ï¼šå¼ºçƒˆæ¨èè¯¥æ–¹æ¡ˆï¼Œå¯ç”¨æœ€ä½æˆæœ¬æ¢ç´¢å¤§æ¨¡å‹æ¨ç†æé™ã€‚</li>
<li><strong>ç”Ÿäº§</strong> åœºæ™¯ï¼šéœ€è°¨æ…è¯„ä¼°å´©æºƒæ¦‚ç‡ä¸å»¶è¿Ÿï¼Œåšå¥½ç›‘æ§ä¸è‡ªåŠ¨é™çº§ã€‚</li>
</ul>
<p>âš™ï¸ <strong>åç»­æ–¹å‘</strong></p>
<ol>
<li>è¿ç§»åˆ° <strong>RTX 5000 Ada</strong> ç­‰æ–°å¡ï¼Œå¯è§£é” Flash-Attn-2 ä¸æ›´é«˜å¸¦å®½ã€‚</li>
<li>å…³æ³¨ vLLM åç»­å¯¹ AWQ Kernel çš„ä¼˜åŒ–ï¼›å‡çº§ &gt;=0.9 å¯èƒ½å…å»è‡ªå·±ç¼–è¯‘ã€‚</li>
<li>å°è¯• <strong>TensorRT-LLM</strong> è‡ªåŠ¨å¹¶è¡Œæ‹†åˆ†ï¼Œè·å¾—é¢å¤– 10~20% æ€§èƒ½ã€‚</li>
</ol>
]]></content:encoded></item></channel></rss>