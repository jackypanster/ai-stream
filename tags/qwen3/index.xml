<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Qwen3 on Code Whispers</title><link>https://jackypanster.github.io/ai-stream/tags/qwen3/</link><description>Recent content in Qwen3 on Code Whispers</description><generator>Hugo -- 0.148.1</generator><language>zh-cn</language><lastBuildDate>Wed, 16 Jul 2025 13:19:48 +0800</lastBuildDate><atom:link href="https://jackypanster.github.io/ai-stream/tags/qwen3/index.xml" rel="self" type="application/rss+xml"/><item><title>Qwen3-32B-AWQ vLLM 多卡 2080 Ti 极限部署实战</title><link>https://jackypanster.github.io/ai-stream/posts/qwen3-32b-2080ti-vllm-deploy/</link><pubDate>Wed, 16 Jul 2025 13:19:48 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/qwen3-32b-2080ti-vllm-deploy/</guid><description>&lt;blockquote>
&lt;p>使用旧显卡也能跑 32B 大模型？本文手把手演示如何在 &lt;strong>4×RTX 2080 Ti (共 88 GB 显存)&lt;/strong> 服务器上，通过 vLLM 0.8.5 + AWQ 量化，跑起 &lt;strong>Qwen3-32B&lt;/strong> 并支持 &lt;strong>32 K tokens&lt;/strong> 超长上下文与高吞吐推理。全文记录了踩坑过程与参数权衡，希望给同样预算有限、硬件受限的工程师带来借鉴。&lt;/p>&lt;/blockquote>
&lt;p>{{.TableOfContents}}&lt;/p>
&lt;h2 id="1-项目背景">1 项目背景&lt;/h2>
&lt;ul>
&lt;li>主角：&lt;code>Qwen3-32B-AWQ&lt;/code> 量化模型 （≈ 18 GB）&lt;/li>
&lt;li>目标：在消费级 &lt;strong>Turing&lt;/strong> 架构显卡（2080 Ti）上最大化利用显存与吞吐。&lt;/li>
&lt;li>框架：&lt;code>vLLM 0.8.5&lt;/code> (openai-compatible server)&lt;/li>
&lt;li>取舍：牺牲部分延迟 / 稳定性 → 换取 &lt;strong>吞吐 + 上下文长度&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="2-硬件与系统环境">2 硬件与系统环境&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>组件&lt;/th>
&lt;th>规格&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>GPU&lt;/td>
&lt;td>4 × RTX 2080 Ti, 22 GB &lt;em>each&lt;/em>, Compute Capability 7.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CPU&lt;/td>
&lt;td>≥ 56 cores (vLLM 线程可吃满)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RAM&lt;/td>
&lt;td>512 GB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Storage&lt;/td>
&lt;td>NVMe SSD 2 TB (模型 + KV 缓冲)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OS&lt;/td>
&lt;td>Ubuntu 24.04&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Driver&lt;/td>
&lt;td>NVIDIA 570.153.02&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CUDA&lt;/td>
&lt;td>12.8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="21-nvidia-smi-基线信息">2.1 NVIDIA-SMI 基线信息&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>nvidia-smi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Wed Jul &lt;span style="color:#ae81ff">16&lt;/span> 13:27:17 &lt;span style="color:#ae81ff">2025&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----------------------------------------------------------------------------------------+
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| NVIDIA-SMI 570.153.02 Driver Version: 570.153.02 CUDA Version: 12.8 |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----------------------------------------------------------------------------------------+
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>可以看到驱动与 CUDA 版本与上表一致，确认环境无偏差。&lt;/p></description></item><item><title>Qwen3-32B-AWQ 高效部署：基于 vLLM 的深度实践与优化</title><link>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-32b-awq-vllm-guide/</link><pubDate>Tue, 10 Jun 2025 20:45:00 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-32b-awq-vllm-guide/</guid><description>本文详细介绍了如何使用 vLLM 高效部署 Qwen3-32B-AWQ 量化模型，实现 32K 上下文窗口、OpenAI 兼容 API，并禁用思考模式。通过对 Docker 及 vLLM 参数的精细调优，最大化模型在多 GPU 环境下的推理性能。</description></item><item><title>DeepSeek-R1-0528-Qwen3-8B部署优化实践</title><link>https://jackypanster.github.io/ai-stream/posts/deploy-deepseek-r1-qwen3-8b-optimization/</link><pubDate>Sat, 07 Jun 2025 17:50:00 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/deploy-deepseek-r1-qwen3-8b-optimization/</guid><description>&lt;h1 id="deepseek-r1-0528-qwen3-8b部署优化实践性能与稳定性的平衡艺术">DeepSeek-R1-0528-Qwen3-8B部署优化实践：性能与稳定性的平衡艺术&lt;/h1>
&lt;p>在AI大模型部署领域，本文详细记录对DeepSeek-R1-0528-Qwen3-8B模型使用vLLM进行部署优化的全过程，重点关注上下文窗口长度与硬件资源利用的平衡调优。&lt;/p>
&lt;h2 id="环境与基础设施">环境与基础设施&lt;/h2>
&lt;p>我们的部署环境具备以下配置：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>GPU&lt;/strong>: 4 x NVIDIA RTX 2080 Ti（每张22GB显存，总计88GB显存）
&lt;ul>
&lt;li>架构: Turing&lt;/li>
&lt;li>计算能力: 7.5&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>CPU&lt;/strong>: 56核&lt;/li>
&lt;li>&lt;strong>内存&lt;/strong>: 512GB RAM&lt;/li>
&lt;li>&lt;strong>存储&lt;/strong>: 2TB SSD&lt;/li>
&lt;li>&lt;strong>操作系统&lt;/strong>: Ubuntu 24.04&lt;/li>
&lt;li>&lt;strong>容器镜像&lt;/strong>: &lt;code>vllm/vllm-openai:v0.8.5&lt;/code>&lt;/li>
&lt;li>&lt;strong>NVIDIA驱动&lt;/strong>: 570.153.02（CUDA 12.8）&lt;/li>
&lt;/ul>
&lt;h2 id="优化前的部署脚本分析">优化前的部署脚本分析&lt;/h2>
&lt;p>我们最初的部署脚本如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -d &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --gpus all &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --name coder &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --shm-size 16g &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ulimit memlock&lt;span style="color:#f92672">=&lt;/span>-1 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --restart always &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ipc&lt;span style="color:#f92672">=&lt;/span>host &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -p 8000:8000 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -e CUDA_MODULE_LOADING&lt;span style="color:#f92672">=&lt;/span>LAZY &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> vllm/vllm-openai:v0.8.5 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --model /models &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --served-model-name coder &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --tensor-parallel-size &lt;span style="color:#ae81ff">4&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --gpu-memory-utilization 0.93 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --dtype float16 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --max-model-len &lt;span style="color:#ae81ff">65536&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --trust-remote-code &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --load-format safetensors &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --disable-custom-all-reduce
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>通过分析，我们发现几个可以优化的关键点：&lt;/p></description></item></channel></rss>