<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Docker on AI é¿éš¾æ‰€</title><link>https://jackypanster.github.io/ai-stream/tags/docker/</link><description>Recent content in Docker on AI é¿éš¾æ‰€</description><generator>Hugo -- 0.147.7</generator><language>en-us</language><lastBuildDate>Sat, 07 Jun 2025 17:50:00 +0800</lastBuildDate><atom:link href="https://jackypanster.github.io/ai-stream/tags/docker/index.xml" rel="self" type="application/rss+xml"/><item><title>DeepSeek-R1-0528-Qwen3-8Béƒ¨ç½²ä¼˜åŒ–å®è·µ</title><link>https://jackypanster.github.io/ai-stream/posts/deploy-deepseek-r1-qwen3-8b-optimization/</link><pubDate>Sat, 07 Jun 2025 17:50:00 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/deploy-deepseek-r1-qwen3-8b-optimization/</guid><description>&lt;h1 id="deepseek-r1-0528-qwen3-8béƒ¨ç½²ä¼˜åŒ–å®è·µæ€§èƒ½ä¸ç¨³å®šæ€§çš„å¹³è¡¡è‰ºæœ¯">DeepSeek-R1-0528-Qwen3-8Béƒ¨ç½²ä¼˜åŒ–å®è·µï¼šæ€§èƒ½ä¸ç¨³å®šæ€§çš„å¹³è¡¡è‰ºæœ¯&lt;/h1>
&lt;p>åœ¨AIå¤§æ¨¡å‹éƒ¨ç½²é¢†åŸŸï¼Œæœ¬æ–‡è¯¦ç»†è®°å½•å¯¹DeepSeek-R1-0528-Qwen3-8Bæ¨¡å‹ä½¿ç”¨vLLMè¿›è¡Œéƒ¨ç½²ä¼˜åŒ–çš„å…¨è¿‡ç¨‹ï¼Œé‡ç‚¹å…³æ³¨ä¸Šä¸‹æ–‡çª—å£é•¿åº¦ä¸ç¡¬ä»¶èµ„æºåˆ©ç”¨çš„å¹³è¡¡è°ƒä¼˜ã€‚&lt;/p>
&lt;h2 id="ç¯å¢ƒä¸åŸºç¡€è®¾æ–½">ç¯å¢ƒä¸åŸºç¡€è®¾æ–½&lt;/h2>
&lt;p>æˆ‘ä»¬çš„éƒ¨ç½²ç¯å¢ƒå…·å¤‡ä»¥ä¸‹é…ç½®ï¼š&lt;/p>
&lt;ul>
&lt;li>&lt;strong>GPU&lt;/strong>: 4 x NVIDIA RTX 2080 Tiï¼ˆæ¯å¼ 22GBæ˜¾å­˜ï¼Œæ€»è®¡88GBæ˜¾å­˜ï¼‰
&lt;ul>
&lt;li>æ¶æ„: Turing&lt;/li>
&lt;li>è®¡ç®—èƒ½åŠ›: 7.5&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>CPU&lt;/strong>: 56æ ¸&lt;/li>
&lt;li>&lt;strong>å†…å­˜&lt;/strong>: 512GB RAM&lt;/li>
&lt;li>&lt;strong>å­˜å‚¨&lt;/strong>: 2TB SSD&lt;/li>
&lt;li>&lt;strong>æ“ä½œç³»ç»Ÿ&lt;/strong>: Ubuntu 24.04&lt;/li>
&lt;li>&lt;strong>å®¹å™¨é•œåƒ&lt;/strong>: &lt;code>vllm/vllm-openai:v0.8.5&lt;/code>&lt;/li>
&lt;li>&lt;strong>NVIDIAé©±åŠ¨&lt;/strong>: 570.153.02ï¼ˆCUDA 12.8ï¼‰&lt;/li>
&lt;/ul>
&lt;h2 id="ä¼˜åŒ–å‰çš„éƒ¨ç½²è„šæœ¬åˆ†æ">ä¼˜åŒ–å‰çš„éƒ¨ç½²è„šæœ¬åˆ†æ&lt;/h2>
&lt;p>æˆ‘ä»¬æœ€åˆçš„éƒ¨ç½²è„šæœ¬å¦‚ä¸‹ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -d &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --gpus all &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --name coder &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --shm-size 16g &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ulimit memlock&lt;span style="color:#f92672">=&lt;/span>-1 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --restart always &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ipc&lt;span style="color:#f92672">=&lt;/span>host &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -p 8000:8000 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -e CUDA_MODULE_LOADING&lt;span style="color:#f92672">=&lt;/span>LAZY &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> vllm/vllm-openai:v0.8.5 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --model /models &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --served-model-name coder &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --tensor-parallel-size &lt;span style="color:#ae81ff">4&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --gpu-memory-utilization 0.93 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --dtype float16 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --max-model-len &lt;span style="color:#ae81ff">65536&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --trust-remote-code &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --load-format safetensors &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --disable-custom-all-reduce
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>é€šè¿‡åˆ†æï¼Œæˆ‘ä»¬å‘ç°å‡ ä¸ªå¯ä»¥ä¼˜åŒ–çš„å…³é”®ç‚¹ï¼š&lt;/p></description><content:encoded><![CDATA[<h1 id="deepseek-r1-0528-qwen3-8béƒ¨ç½²ä¼˜åŒ–å®è·µæ€§èƒ½ä¸ç¨³å®šæ€§çš„å¹³è¡¡è‰ºæœ¯">DeepSeek-R1-0528-Qwen3-8Béƒ¨ç½²ä¼˜åŒ–å®è·µï¼šæ€§èƒ½ä¸ç¨³å®šæ€§çš„å¹³è¡¡è‰ºæœ¯</h1>
<p>åœ¨AIå¤§æ¨¡å‹éƒ¨ç½²é¢†åŸŸï¼Œæœ¬æ–‡è¯¦ç»†è®°å½•å¯¹DeepSeek-R1-0528-Qwen3-8Bæ¨¡å‹ä½¿ç”¨vLLMè¿›è¡Œéƒ¨ç½²ä¼˜åŒ–çš„å…¨è¿‡ç¨‹ï¼Œé‡ç‚¹å…³æ³¨ä¸Šä¸‹æ–‡çª—å£é•¿åº¦ä¸ç¡¬ä»¶èµ„æºåˆ©ç”¨çš„å¹³è¡¡è°ƒä¼˜ã€‚</p>
<h2 id="ç¯å¢ƒä¸åŸºç¡€è®¾æ–½">ç¯å¢ƒä¸åŸºç¡€è®¾æ–½</h2>
<p>æˆ‘ä»¬çš„éƒ¨ç½²ç¯å¢ƒå…·å¤‡ä»¥ä¸‹é…ç½®ï¼š</p>
<ul>
<li><strong>GPU</strong>: 4 x NVIDIA RTX 2080 Tiï¼ˆæ¯å¼ 22GBæ˜¾å­˜ï¼Œæ€»è®¡88GBæ˜¾å­˜ï¼‰
<ul>
<li>æ¶æ„: Turing</li>
<li>è®¡ç®—èƒ½åŠ›: 7.5</li>
</ul>
</li>
<li><strong>CPU</strong>: 56æ ¸</li>
<li><strong>å†…å­˜</strong>: 512GB RAM</li>
<li><strong>å­˜å‚¨</strong>: 2TB SSD</li>
<li><strong>æ“ä½œç³»ç»Ÿ</strong>: Ubuntu 24.04</li>
<li><strong>å®¹å™¨é•œåƒ</strong>: <code>vllm/vllm-openai:v0.8.5</code></li>
<li><strong>NVIDIAé©±åŠ¨</strong>: 570.153.02ï¼ˆCUDA 12.8ï¼‰</li>
</ul>
<h2 id="ä¼˜åŒ–å‰çš„éƒ¨ç½²è„šæœ¬åˆ†æ">ä¼˜åŒ–å‰çš„éƒ¨ç½²è„šæœ¬åˆ†æ</h2>
<p>æˆ‘ä»¬æœ€åˆçš„éƒ¨ç½²è„šæœ¬å¦‚ä¸‹ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -d <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --gpus all <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --name coder <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --shm-size 16g <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --ulimit memlock<span style="color:#f92672">=</span>-1 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --restart always <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --ipc<span style="color:#f92672">=</span>host <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -p 8000:8000 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -e CUDA_MODULE_LOADING<span style="color:#f92672">=</span>LAZY <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  vllm/vllm-openai:v0.8.5 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --model /models <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --served-model-name coder <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --tensor-parallel-size <span style="color:#ae81ff">4</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --gpu-memory-utilization 0.93 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --dtype float16 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --max-model-len <span style="color:#ae81ff">65536</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --trust-remote-code <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --load-format safetensors <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --disable-custom-all-reduce
</span></span></code></pre></div><p>é€šè¿‡åˆ†æï¼Œæˆ‘ä»¬å‘ç°å‡ ä¸ªå¯ä»¥ä¼˜åŒ–çš„å…³é”®ç‚¹ï¼š</p>
<ol>
<li><strong>å…±äº«å†…å­˜</strong>ï¼š16GBå¯èƒ½ä¸è¶³ä»¥æ”¯æŒé«˜å¹¶å‘è¯·æ±‚</li>
<li><strong>äº¤æ¢ç©ºé—´</strong>ï¼šæœªé…ç½®SSDäº¤æ¢ç©ºé—´æ”¯æŒ</li>
<li><strong>æ‰¹å¤„ç†èƒ½åŠ›</strong>ï¼šæœªè®¾ç½®<code>--max-num-batched-tokens</code>å‚æ•°</li>
<li><strong>CUDAå›¾å½¢ä¼˜åŒ–</strong>ï¼šæœªä½¿ç”¨<code>--enforce-eager</code>æé«˜ç¨³å®šæ€§</li>
</ol>
<h2 id="æ·±å…¥ä¼˜åŒ–ç­–ç•¥">æ·±å…¥ä¼˜åŒ–ç­–ç•¥</h2>
<h3 id="1-å†…å­˜ä¸è®¡ç®—èµ„æºåˆ†é…">1. å†…å­˜ä¸è®¡ç®—èµ„æºåˆ†é…</h3>
<p>å¯¹äºRTX 2080 Tiè¿™ç±»Turingæ¶æ„GPUï¼Œæˆ‘ä»¬éœ€è¦ç‰¹åˆ«æ³¨æ„æ˜¾å­˜åˆ†é…ä¸å¹¶è¡Œç­–ç•¥ï¼š</p>
<ul>
<li><strong>å…±äº«å†…å­˜æ‰©å±•</strong>ï¼šå°†<code>--shm-size</code>ä»16gå¢åŠ åˆ°64gï¼Œå……åˆ†åˆ©ç”¨512GBç³»ç»Ÿå†…å­˜</li>
<li><strong>æ˜¾å­˜åˆ©ç”¨ç‡</strong>ï¼šç»´æŒ<code>--gpu-memory-utilization 0.93</code>çš„æ¿€è¿›ä½†å¯æ§è®¾ç½®</li>
<li><strong>å¼ é‡å¹¶è¡ŒåŒ–</strong>ï¼šä¿æŒ<code>--tensor-parallel-size 4</code>å……åˆ†åˆ©ç”¨æ‰€æœ‰GPU</li>
<li><strong>æ‰¹å¤„ç†æ”¯æŒ</strong>ï¼šæ·»åŠ <code>--max-num-batched-tokens 8192</code>æé«˜ååé‡</li>
</ul>
<h3 id="2-ç¨³å®šæ€§ä¸æ•ˆç‡å¹³è¡¡">2. ç¨³å®šæ€§ä¸æ•ˆç‡å¹³è¡¡</h3>
<ul>
<li><strong>CUDAæ‰§è¡Œæ¨¡å¼</strong>ï¼šæ·»åŠ <code>--enforce-eager</code>å‚æ•°ï¼Œé¿å…CUDAå›¾æ•è·å¯èƒ½å¯¼è‡´çš„OOMé—®é¢˜</li>
<li><strong>äº¤æ¢ç©ºé—´æ”¯æŒ</strong>ï¼šæ·»åŠ <code>--swap-space 32</code>å‚æ•°ï¼Œä¸ºå¤„ç†é•¿ä¸Šä¸‹æ–‡æä¾›é¢å¤–å†…å­˜ä¿éšœ</li>
<li><strong>all-reduceä¼˜åŒ–</strong>ï¼šç§»é™¤<code>--disable-custom-all-reduce</code>å‚æ•°ï¼ˆæ³¨ï¼šæ—¥å¿—æ˜¾ç¤ºç³»ç»Ÿè‡ªåŠ¨ç¦ç”¨ï¼‰</li>
</ul>
<h3 id="3-ä¸Šä¸‹æ–‡é•¿åº¦è®¾è®¡">3. ä¸Šä¸‹æ–‡é•¿åº¦è®¾è®¡</h3>
<p>è™½ç„¶æˆ‘ä»¬æœ€ç»ˆä¿ç•™äº†<code>--max-model-len 65536</code>è®¾ç½®ï¼Œä½†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åº”å½“æ ¹æ®å…·ä½“ä½¿ç”¨åœºæ™¯å’Œç¨³å®šæ€§éœ€æ±‚è€ƒè™‘é™è‡³32768ã€‚å¯¹äºå¤§å¤šæ•°åº”ç”¨åœºæ™¯ï¼Œè¿™ä¸ªé•¿åº¦å·²ç»è¶³å¤Ÿï¼Œå¹¶ä¸”èƒ½æä¾›æ›´å¥½çš„æ€§èƒ½å’Œç¨³å®šæ€§å¹³è¡¡ã€‚</p>
<h2 id="ä¼˜åŒ–åçš„éƒ¨ç½²è„šæœ¬">ä¼˜åŒ–åçš„éƒ¨ç½²è„šæœ¬</h2>
<p>ç»è¿‡ä¸€ç³»åˆ—ä¼˜åŒ–ï¼Œæˆ‘ä»¬çš„æœ€ç»ˆéƒ¨ç½²è„šæœ¬å¦‚ä¸‹ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -d <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --gpus all <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --name coder <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --shm-size 64g <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --ulimit memlock<span style="color:#f92672">=</span>-1 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --restart always <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --ipc<span style="color:#f92672">=</span>host <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -p 8000:8000 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -e CUDA_MODULE_LOADING<span style="color:#f92672">=</span>LAZY <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  vllm/vllm-openai:v0.8.5 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --model /models <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --served-model-name coder <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --tensor-parallel-size <span style="color:#ae81ff">4</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --gpu-memory-utilization 0.93 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --dtype float16 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --max-model-len <span style="color:#ae81ff">65536</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --trust-remote-code <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --load-format safetensors <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --swap-space <span style="color:#ae81ff">32</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --enforce-eager <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --max-num-batched-tokens <span style="color:#ae81ff">8192</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --chat-template /models/qwen3_programming.jinja
</span></span></code></pre></div><h2 id="æ€§èƒ½ä¸èµ„æºåˆ†æ">æ€§èƒ½ä¸èµ„æºåˆ†æ</h2>
<p>éƒ¨ç½²åï¼Œé€šè¿‡æ—¥å¿—åˆ†ææˆ‘ä»¬å¾—åˆ°ä»¥ä¸‹æ€§èƒ½æŒ‡æ ‡ï¼š</p>
<pre tabindex="0"><code>Memory profiling takes 5.76 seconds
the current vLLM instance can use total_gpu_memory (21.48GiB) x gpu_memory_utilization (0.93) = 19.98GiB
model weights take 3.85GiB; non_torch_memory takes 0.20GiB; PyTorch activation peak memory takes 1.45GiB; the rest of the memory reserved for KV Cache is 14.49GiB.
</code></pre><p>å…³é”®æ€§èƒ½å‘ç°ï¼š</p>
<ul>
<li><strong>KVç¼“å­˜ç©ºé—´</strong>ï¼š14.49GiBï¼Œè¶³å¤Ÿæ”¯æŒ65536 tokençš„ä¸Šä¸‹æ–‡å¤„ç†</li>
<li><strong>æœ€å¤§å¹¶å‘èƒ½åŠ›</strong>ï¼šå¯åŒæ—¶å¤„ç†çº¦6.44ä¸ªæœ€å¤§é•¿åº¦ï¼ˆ65536 tokensï¼‰çš„è¯·æ±‚</li>
<li><strong>åˆå§‹åŒ–æ—¶é—´</strong>ï¼š31.86ç§’ï¼Œç›¸æ¯”æœªä¼˜åŒ–é…ç½®æœ‰æ‰€æ”¹å–„</li>
</ul>
<h2 id="å®ç”¨éƒ¨ç½²å»ºè®®">å®ç”¨éƒ¨ç½²å»ºè®®</h2>
<p>æ ¹æ®æˆ‘ä»¬çš„å®è·µç»éªŒï¼Œæä¾›ä»¥ä¸‹éƒ¨ç½²å»ºè®®ï¼š</p>
<ol>
<li>
<p><strong>ä¸Šä¸‹æ–‡é•¿åº¦é€‰æ‹©</strong></p>
<ul>
<li>å¯¹äºè¿½æ±‚ç¨³å®šæ€§çš„ç”Ÿäº§ç¯å¢ƒï¼šä½¿ç”¨<code>--max-model-len 32768</code></li>
<li>å¯¹äºéœ€è¦æé™æ€§èƒ½çš„åœºæ™¯ï¼šå¯å°è¯•<code>--max-model-len 65536</code>ä½†éœ€å¯†åˆ‡ç›‘æ§ç¨³å®šæ€§</li>
</ul>
</li>
<li>
<p><strong>æ˜¾å­˜åˆ©ç”¨ç‡è°ƒä¼˜</strong></p>
<ul>
<li>ç¨³å®šæ€§ä¼˜å…ˆï¼š<code>--gpu-memory-utilization 0.9</code></li>
<li>æ€§èƒ½ä¼˜å…ˆï¼š<code>--gpu-memory-utilization 0.93</code>æˆ–æ›´é«˜ï¼ˆéœ€è°¨æ…ï¼‰</li>
</ul>
</li>
<li>
<p><strong>æ‰¹å¤„ç†å‚æ•°ä¼˜åŒ–</strong></p>
<ul>
<li>å¯¹äºå¤šç”¨æˆ·åœºæ™¯ï¼šå¢åŠ <code>--max-num-batched-tokens</code>è‡³8192æˆ–æ›´é«˜</li>
<li>å¯¹äºå•ä¸€å¤æ‚ä»»åŠ¡ï¼šå¯é€‚å½“é™ä½æ­¤å‚æ•°ï¼Œä¸“æ³¨å•ä»»åŠ¡æ€§èƒ½</li>
</ul>
</li>
<li>
<p><strong>ç¡¬ä»¶èµ„æºåˆ†é…</strong></p>
<ul>
<li>å…±äº«å†…å­˜ä¸ç³»ç»Ÿå†…å­˜æ¯”ä¾‹ï¼šå»ºè®®1:8å·¦å³ï¼ˆå¦‚512GBç³»ç»Ÿå†…å­˜é…ç½®64GBå…±äº«å†…å­˜ï¼‰</li>
<li>äº¤æ¢ç©ºé—´è®¾ç½®ï¼šæ ¹æ®SSDé€Ÿåº¦å’Œå®¹é‡ï¼Œå¯è®¾ç½®ä¸ºæ˜¾å­˜æ€»é‡çš„1/3è‡³1/2</li>
</ul>
</li>
</ol>
<h2 id="æ’éšœä¸éªŒè¯">æ’éšœä¸éªŒè¯</h2>
<p>æ¯æ¬¡ä¿®æ”¹é…ç½®åï¼Œé€šè¿‡ä»¥ä¸‹å‘½ä»¤éªŒè¯éƒ¨ç½²çŠ¶æ€ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl http://localhost:8000/v1/models
</span></span></code></pre></div><p>éªŒè¯ç»“æœæ˜¾ç¤ºæ¨¡å‹å·²æˆåŠŸéƒ¨ç½²ï¼Œå¹¶è¿”å›äº†ä»¥ä¸‹å®é™…è¾“å‡ºï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;object&#34;</span>: <span style="color:#e6db74">&#34;list&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;data&#34;</span>: [
</span></span><span style="display:flex;"><span>    {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;id&#34;</span>: <span style="color:#e6db74">&#34;coder&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;object&#34;</span>: <span style="color:#e6db74">&#34;model&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;created&#34;</span>: <span style="color:#ae81ff">1749289780</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;owned_by&#34;</span>: <span style="color:#e6db74">&#34;vllm&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;root&#34;</span>: <span style="color:#e6db74">&#34;/models&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;parent&#34;</span>: <span style="color:#66d9ef">null</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;max_model_len&#34;</span>: <span style="color:#ae81ff">65536</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;permission&#34;</span>: [
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;id&#34;</span>: <span style="color:#e6db74">&#34;modelperm-ee339bc1702c402f8ae06ea2f1b05c7c&#34;</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;object&#34;</span>: <span style="color:#e6db74">&#34;model_permission&#34;</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;created&#34;</span>: <span style="color:#ae81ff">1749289780</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;allow_create_engine&#34;</span>: <span style="color:#66d9ef">false</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;allow_sampling&#34;</span>: <span style="color:#66d9ef">true</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;allow_logprobs&#34;</span>: <span style="color:#66d9ef">true</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;allow_search_indices&#34;</span>: <span style="color:#66d9ef">false</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;allow_view&#34;</span>: <span style="color:#66d9ef">true</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;allow_fine_tuning&#34;</span>: <span style="color:#66d9ef">false</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;organization&#34;</span>: <span style="color:#e6db74">&#34;*&#34;</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;group&#34;</span>: <span style="color:#66d9ef">null</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;is_blocking&#34;</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>      ]
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  ]
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>ä»è¿”å›çš„JSONå“åº”ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®è®¤æ¨¡å‹éƒ¨ç½²æˆåŠŸå¹¶è§£è¯»ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š</p>
<ul>
<li><strong>id</strong>: &ldquo;coder&rdquo; - ç¡®è®¤æˆ‘ä»¬çš„æ¨¡å‹æœåŠ¡åç§°å·²æ­£ç¡®è®¾ç½®</li>
<li><strong>max_model_len</strong>: 65536 - éªŒè¯äº†æˆ‘ä»¬è®¾ç½®çš„ä¸Šä¸‹æ–‡çª—å£é•¿åº¦ä¸º65536 tokens</li>
<li><strong>owned_by</strong>: &ldquo;vllm&rdquo; - è¡¨æ˜æ¨¡å‹ç”±vLLMæœåŠ¡ç®¡ç†</li>
<li><strong>permission</strong>å¯¹è±¡ä¸­ï¼š
<ul>
<li><strong>allow_sampling</strong>: true - æ”¯æŒé‡‡æ ·ç”Ÿæˆï¼ˆtemperatureã€top_pç­‰å‚æ•°ï¼‰</li>
<li><strong>allow_logprobs</strong>: true - æ”¯æŒè¾“å‡ºtokenæ¦‚ç‡</li>
<li><strong>organization</strong>: &ldquo;*&rdquo; - å…è®¸æ‰€æœ‰ç»„ç»‡è®¿é—®æ¨¡å‹</li>
</ul>
</li>
</ul>
<p>è¿™äº›å‚æ•°ç¡®è®¤äº†æˆ‘ä»¬çš„éƒ¨ç½²é…ç½®å·²ç»æ­£ç¡®åº”ç”¨ï¼Œä¸”æ¨¡å‹æœåŠ¡å·²å‡†å¤‡å¥½æ¥æ”¶æ¨ç†è¯·æ±‚ã€‚</p>
<h2 id="ä¸“ç”¨ç¼–ç¨‹æç¤ºè¯æ¨¡æ¿">ä¸“ç”¨ç¼–ç¨‹æç¤ºè¯æ¨¡æ¿</h2>
<p>ç”±äºDeepSeek-R1-0528-Qwen3-8Bæ¨¡å‹ç‰¹åˆ«é€‚åˆç¼–ç¨‹ä»»åŠ¡ï¼Œæˆ‘ä»¬åœ¨éƒ¨ç½²ä¸­åŠ å…¥äº†ä¸“é—¨çš„æç¤ºè¯æ¨¡æ¿æ¥ä¼˜åŒ–å…¶ç¼–ç¨‹èƒ½åŠ›ã€‚æˆ‘ä»¬å·²ç»é€šè¿‡<code>--chat-template</code>å‚æ•°æŒ‡å®šäº†æ¨¡æ¿è·¯å¾„ï¼Œæ¨¡æ¿å†…å®¹å¦‚ä¸‹ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-jinja" data-lang="jinja"><span style="display:flex;"><span><span style="color:#75715e">{# Enhanced template for Qwen3 optimized for programming tasks #}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">{%</span> <span style="color:#66d9ef">if</span> messages<span style="color:#f92672">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">][</span><span style="color:#e6db74">&#39;role&#39;</span><span style="color:#f92672">]</span> <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;system&#39;</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">set</span> loop_messages <span style="color:#f92672">=</span> messages<span style="color:#f92672">[</span><span style="color:#ae81ff">1</span><span style="color:#f92672">:]</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">set</span> system_message <span style="color:#f92672">=</span> messages<span style="color:#f92672">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">][</span><span style="color:#e6db74">&#39;content&#39;</span><span style="color:#f92672">]</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">{%</span> <span style="color:#66d9ef">else</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">set</span> loop_messages <span style="color:#f92672">=</span> messages <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">set</span> system_message <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;You are a programming assistant specialized in writing clean, efficient, and well-documented code. Provide direct code solutions without unnecessary explanations unless requested. Focus on best practices, optimal algorithms, and proper error handling. When multiple approaches exist, choose the most efficient one by default. Always include necessary imports and dependencies.&#34;</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">{%</span> <span style="color:#66d9ef">endif</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">{# Always include system message for programming optimization #}</span>
</span></span><span style="display:flex;"><span>&lt;|im_start|&gt;system
</span></span><span style="display:flex;"><span><span style="color:#75715e">{{</span> system_message <span style="color:#75715e">}}</span>&lt;|im_end|&gt;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">{%</span> <span style="color:#66d9ef">for</span> message <span style="color:#66d9ef">in</span> loop_messages <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">if</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;role&#39;</span><span style="color:#f92672">]</span> <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;user&#39;</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>&lt;|im_start|&gt;user
</span></span><span style="display:flex;"><span><span style="color:#75715e">{{</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;content&#39;</span><span style="color:#f92672">]</span> <span style="color:#75715e">}}</span>&lt;|im_end|&gt;
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">elif</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;role&#39;</span><span style="color:#f92672">]</span> <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;assistant&#39;</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>&lt;|im_start|&gt;assistant
</span></span><span style="display:flex;"><span><span style="color:#75715e">{{</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;content&#39;</span><span style="color:#f92672">]</span> <span style="color:#75715e">}}</span>&lt;|im_end|&gt;
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">elif</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;role&#39;</span><span style="color:#f92672">]</span> <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;tool&#39;</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>&lt;|im_start|&gt;tool
</span></span><span style="display:flex;"><span><span style="color:#75715e">{{</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;content&#39;</span><span style="color:#f92672">]</span> <span style="color:#75715e">}}</span>&lt;|im_end|&gt;
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">else</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>&lt;|im_start|&gt;<span style="color:#75715e">{{</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;role&#39;</span><span style="color:#f92672">]</span> <span style="color:#75715e">}}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">{{</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;content&#39;</span><span style="color:#f92672">]</span> <span style="color:#75715e">}}</span>&lt;|im_end|&gt;
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">endif</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">{%</span> <span style="color:#66d9ef">endfor</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">{%</span> <span style="color:#66d9ef">if</span> add_generation_prompt <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>&lt;|im_start|&gt;assistant
</span></span><span style="display:flex;"><span><span style="color:#75715e">{%</span> <span style="color:#66d9ef">endif</span> <span style="color:#75715e">%}</span>
</span></span></code></pre></div><p>æ­¤æ¨¡æ¿å…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š</p>
<ol>
<li><strong>ä¸“ä¸šç¼–ç¨‹æŒ‡ä»¤</strong>ï¼šé»˜è®¤ç³»ç»Ÿæç¤ºè¯ä¸“é—¨é’ˆå¯¹ç¼–ç¨‹ä»»åŠ¡ä¼˜åŒ–ï¼Œå¼ºè°ƒä»£ç è´¨é‡ã€æ•ˆç‡å’Œæ–‡æ¡£</li>
<li><strong>ç›´æ¥è¾“å‡º</strong>ï¼šå€¾å‘äºç›´æ¥æä¾›ä»£ç è§£å†³æ–¹æ¡ˆï¼Œå‡å°‘ä¸å¿…è¦çš„è§£é‡Šï¼ˆé™¤éç‰¹åˆ«è¦æ±‚ï¼‰</li>
<li><strong>æ ‡å‡†åŒ–æ ¼å¼</strong>ï¼šä½¿ç”¨<code>&lt;|im_start|&gt;</code>å’Œ<code>&lt;|im_end|&gt;</code>æ ‡è®°æ¸…æ™°ç•Œå®šä¸åŒè§’è‰²çš„æ¶ˆæ¯</li>
<li><strong>çµæ´»æ€§</strong>ï¼šå…è®¸è¦†ç›–é»˜è®¤ç³»ç»Ÿæç¤ºè¯ï¼Œä»¥é€‚åº”ç‰¹å®šç¼–ç¨‹åœºæ™¯</li>
</ol>
<p>åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œå¯ä»¥å°†è¯¥æ¨¡æ¿ä¸vLLMçš„APIè°ƒç”¨ç»“åˆï¼Œä¾‹å¦‚ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> requests
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;http://localhost:8000/v1/chat/completions&#34;</span>
</span></span><span style="display:flex;"><span>headers <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;Content-Type&#34;</span>: <span style="color:#e6db74">&#34;application/json&#34;</span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>payload <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;model&#34;</span>: <span style="color:#e6db74">&#34;coder&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;messages&#34;</span>: [
</span></span><span style="display:flex;"><span>        {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹ï¼Œè¦æ±‚ä½¿ç”¨åŠ¨æ€è§„åˆ’ä¼˜åŒ–æ€§èƒ½&#34;</span>}
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;temperature&#34;</span>: <span style="color:#ae81ff">0.2</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;response_format&#34;</span>: {<span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;text&#34;</span>}
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> requests<span style="color:#f92672">.</span>post(url, headers<span style="color:#f92672">=</span>headers, json<span style="color:#f92672">=</span>payload)
</span></span><span style="display:flex;"><span>print(response<span style="color:#f92672">.</span>json())
</span></span></code></pre></div><p>é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥å……åˆ†å‘æŒ¥æ¨¡å‹åœ¨ç¼–ç¨‹é¢†åŸŸçš„ä¸“é•¿ï¼Œè·å¾—æ›´é«˜è´¨é‡ã€æ›´ç¬¦åˆå·¥ç¨‹å®è·µçš„ä»£ç è¾“å‡ºã€‚</p>
<h2 id="ç»“è®ºä¸æœªæ¥æ–¹å‘">ç»“è®ºä¸æœªæ¥æ–¹å‘</h2>
<p>é€šè¿‡ç²¾å¿ƒè°ƒæ•´vLLMå‚æ•°ï¼Œæˆ‘ä»¬æˆåŠŸå®ç°äº†DeepSeek-R1-0528-Qwen3-8Bæ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²ï¼Œåœ¨æœ‰é™çš„RTX 2080 Tiæ˜¾å¡ä¸Šå®ç°äº†æœ€å¤§åŒ–çš„æ€§èƒ½å’Œä¸Šä¸‹æ–‡é•¿åº¦ã€‚</p>
<p>æœªæ¥çš„ä¼˜åŒ–æ–¹å‘å¯ä»¥æ¢ç´¢ï¼š</p>
<ol>
<li><strong>è¿›ä¸€æ­¥é‡åŒ–ç ”ç©¶</strong>ï¼šæ¢ç´¢int8é‡åŒ–å¯¹æ€§èƒ½å’Œè´¨é‡çš„å½±å“</li>
<li><strong>è°ƒåº¦ç­–ç•¥ä¼˜åŒ–</strong>ï¼šé€šè¿‡<code>--scheduler-delay-factor</code>å’Œ<code>--preemption-mode</code>å‚æ•°ä¼˜åŒ–å¤šç”¨æˆ·åœºæ™¯</li>
<li><strong>è‡ªåŠ¨æ‰©ç¼©å®¹æ–¹æ¡ˆ</strong>ï¼šæ ¹æ®è´Ÿè½½åŠ¨æ€è°ƒæ•´GPUåˆ†é…</li>
</ol>
<p>å¸Œæœ›è¿™ä»½éƒ¨ç½²ä¼˜åŒ–å®è·µèƒ½ä¸ºæ›´å¤šå·¥ç¨‹å¸ˆæä¾›å‚è€ƒï¼Œåœ¨å¤§æ¨¡å‹éƒ¨ç½²ä¸­æ‰¾åˆ°æ€§èƒ½ä¸ç¨³å®šæ€§çš„æœ€ä½³å¹³è¡¡ç‚¹ã€‚</p>
<h2 id="å‚è€ƒèµ„æ–™">å‚è€ƒèµ„æ–™</h2>
<ol>
<li><a href="https://docs.vllm.ai/">vLLMå®˜æ–¹æ–‡æ¡£</a></li>
<li><a href="https://github.com/QwenLM/Qwen">Qwen3ç³»åˆ—æ¨¡å‹è¯´æ˜</a></li>
<li><a href="https://github.com/deepseek-ai">DeepSeek R1æ¨¡å‹ç³»åˆ—ä»‹ç»</a></li>
</ol>
]]></content:encoded></item><item><title>é«˜æ€§èƒ½éƒ¨ç½²Qwen3-30Bï¼švLLMä¼˜åŒ–å®è·µæŒ‡å—</title><link>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3/</link><pubDate>Tue, 03 Jun 2025 16:00:00 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3/</guid><description>&lt;h1 id="é«˜æ€§èƒ½éƒ¨ç½²qwen3-30bvllmä¼˜åŒ–å®è·µæŒ‡å—">é«˜æ€§èƒ½éƒ¨ç½²Qwen3-30Bï¼švLLMä¼˜åŒ–å®è·µæŒ‡å—&lt;/h1>
&lt;h2 id="-æ¦‚è¿°">ğŸ“‹ æ¦‚è¿°&lt;/h2>
&lt;p>æœ¬æ–‡è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨vLLMé«˜æ•ˆéƒ¨ç½²Qwen3-30B-A3Bæ¨¡å‹ï¼Œå®ç°32Kä¸Šä¸‹æ–‡çª—å£å’ŒOpenAIå…¼å®¹APIï¼Œé€‚ç”¨äºç”Ÿäº§ç¯å¢ƒã€‚é€šè¿‡ç²¾ç»†è°ƒæ•´éƒ¨ç½²å‚æ•°ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨æœ‰é™çš„GPUèµ„æºä¸‹æœ€å¤§åŒ–æ¨¡å‹æ€§èƒ½ã€‚&lt;/p>
&lt;h2 id="-ç³»ç»Ÿè¦æ±‚">ğŸ–¥ï¸ ç³»ç»Ÿè¦æ±‚&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>ç¡¬ä»¶é…ç½®&lt;/strong>
&lt;ul>
&lt;li>4å—NVIDIA GPU (æ¯å—22GBæ˜¾å­˜ï¼Œæ€»è®¡88GB)&lt;/li>
&lt;li>512GBç³»ç»Ÿå†…å­˜&lt;/li>
&lt;li>2TB SSDå­˜å‚¨&lt;/li>
&lt;li>56æ ¸CPU&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>è½¯ä»¶ç¯å¢ƒ&lt;/strong>
&lt;ul>
&lt;li>Ubuntu 24.04&lt;/li>
&lt;li>NVIDIAé©±åŠ¨ 550.144.03&lt;/li>
&lt;li>CUDA 12.4&lt;/li>
&lt;li>Docker + NVIDIA Container Toolkit&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="-æ¨¡å‹ä¸æ¶æ„">ğŸ§  æ¨¡å‹ä¸æ¶æ„&lt;/h2>
&lt;p>Qwen3-30B-A3Bæ˜¯é˜¿é‡Œäº‘å‘å¸ƒçš„é€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š&lt;/p>
&lt;ul>
&lt;li>30Bå‚æ•°é‡&lt;/li>
&lt;li>åŸç”Ÿæ”¯æŒ32Kä¸Šä¸‹æ–‡é•¿åº¦&lt;/li>
&lt;li>æ”¯æŒæ€è€ƒæ¨¡å¼(Chain-of-Thought)&lt;/li>
&lt;li>ä¼˜å¼‚çš„å¤šè¯­è¨€ä¸ä»£ç èƒ½åŠ›&lt;/li>
&lt;/ul>
&lt;p>æˆ‘ä»¬ä½¿ç”¨vLLMä½œä¸ºæ¨ç†å¼•æ“ï¼Œä¸»è¦åŸºäºä»¥ä¸‹è€ƒé‡ï¼š&lt;/p>
&lt;ol>
&lt;li>&lt;strong>é«˜æ•ˆå†…å­˜ç®¡ç†&lt;/strong>ï¼šé€šè¿‡PagedAttentionæŠ€æœ¯ä¼˜åŒ–KVç¼“å­˜&lt;/li>
&lt;li>&lt;strong>å¼ é‡å¹¶è¡Œ&lt;/strong>ï¼šè‡ªåŠ¨è·¨å¤šGPUåˆ†å¸ƒæ¨¡å‹æƒé‡&lt;/li>
&lt;li>&lt;strong>OpenAIå…¼å®¹API&lt;/strong>ï¼šç›´æ¥æ›¿ä»£OpenAI APIï¼Œæ— éœ€ä¿®æ”¹ç°æœ‰åº”ç”¨&lt;/li>
&lt;li>&lt;strong>åŠ¨æ€æ‰¹å¤„ç†&lt;/strong>ï¼šè‡ªåŠ¨æ‰¹å¤„ç†å¤šè¯·æ±‚ï¼Œæé«˜ååé‡&lt;/li>
&lt;/ol>
&lt;h2 id="-éƒ¨ç½²è„šæœ¬">ğŸ³ éƒ¨ç½²è„šæœ¬&lt;/h2>
&lt;p>ä»¥ä¸‹æ˜¯æˆ‘ä»¬ç”¨äºéƒ¨ç½²çš„Dockerå‘½ä»¤ï¼Œç»è¿‡ç²¾å¿ƒè°ƒä¼˜ä»¥å¹³è¡¡æ€§èƒ½ä¸èµ„æºåˆ©ç”¨ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run -d &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --runtime&lt;span style="color:#f92672">=&lt;/span>nvidia &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --gpus&lt;span style="color:#f92672">=&lt;/span>all &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --name coder &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -v /home/llm/model/qwen/qwen3-30b-a3b:/qwen/qwen3-30b-a3b &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -p 8000:8000 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --cpuset-cpus 0-55 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ulimit memlock&lt;span style="color:#f92672">=&lt;/span>-1 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ulimit stack&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">67108864&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --restart always &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ipc&lt;span style="color:#f92672">=&lt;/span>host &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> vllm/vllm-openai:v0.8.5 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --model /qwen/qwen3-30b-a3b &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --served-model-name coder &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --tensor-parallel-size &lt;span style="color:#ae81ff">4&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --dtype half &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --max-model-len &lt;span style="color:#ae81ff">32768&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --max-num-batched-tokens &lt;span style="color:#ae81ff">4096&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --gpu-memory-utilization 0.93 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --block-size &lt;span style="color:#ae81ff">32&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --enable-chunked-prefill &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --swap-space &lt;span style="color:#ae81ff">16&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --tokenizer-pool-size &lt;span style="color:#ae81ff">56&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --disable-custom-all-reduce
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="-å‚æ•°è¯¦è§£ä¸ä¼˜åŒ–ç­–ç•¥">ğŸ”§ å‚æ•°è¯¦è§£ä¸ä¼˜åŒ–ç­–ç•¥&lt;/h2>
&lt;h3 id="dockerå®¹å™¨é…ç½®">Dockerå®¹å™¨é…ç½®&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>å‚æ•°&lt;/th>
&lt;th>å€¼&lt;/th>
&lt;th>ä½œç”¨&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>--runtime=nvidia&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;td>å¯ç”¨NVIDIAå®¹å™¨è¿è¡Œæ—¶&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>--gpus=all&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;td>å°†æ‰€æœ‰GPUæš´éœ²ç»™å®¹å™¨&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>--cpuset-cpus&lt;/code>&lt;/td>
&lt;td>&lt;code>0-55&lt;/code>&lt;/td>
&lt;td>é™åˆ¶å®¹å™¨ä½¿ç”¨0-55å·CPUæ ¸å¿ƒ&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>--ulimit memlock=-1&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;td>ç§»é™¤å†…å­˜é”å®šé™åˆ¶ï¼Œæé«˜æ€§èƒ½&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>--ipc=host&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;td>ä½¿ç”¨ä¸»æœºIPCå‘½åç©ºé—´ï¼Œå¯¹å…±äº«å†…å­˜å¾ˆé‡è¦&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="vllmå¼•æ“é…ç½®">vLLMå¼•æ“é…ç½®&lt;/h3>
&lt;h4 id="1-å¼ é‡å¹¶è¡Œç­–ç•¥">1. å¼ é‡å¹¶è¡Œç­–ç•¥&lt;/h4>
&lt;pre tabindex="0">&lt;code>--tensor-parallel-size 4
&lt;/code>&lt;/pre>&lt;p>æˆ‘ä»¬ä½¿ç”¨4è·¯å¼ é‡å¹¶è¡Œï¼Œå°†æ¨¡å‹åˆ†å¸ƒåœ¨4å—GPUä¸Šã€‚è¿™æ˜¯åŸºäºå®éªŒå¾—å‡ºçš„æœ€ä½³é…ç½® - åœ¨æˆ‘ä»¬çš„ç¡¬ä»¶ä¸Šï¼Œæ¯å—22GBæ˜¾å­˜çš„GPUæ— æ³•å•ç‹¬åŠ è½½å®Œæ•´çš„30Bæ¨¡å‹ã€‚&lt;/p></description><content:encoded><![CDATA[<h1 id="é«˜æ€§èƒ½éƒ¨ç½²qwen3-30bvllmä¼˜åŒ–å®è·µæŒ‡å—">é«˜æ€§èƒ½éƒ¨ç½²Qwen3-30Bï¼švLLMä¼˜åŒ–å®è·µæŒ‡å—</h1>
<h2 id="-æ¦‚è¿°">ğŸ“‹ æ¦‚è¿°</h2>
<p>æœ¬æ–‡è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨vLLMé«˜æ•ˆéƒ¨ç½²Qwen3-30B-A3Bæ¨¡å‹ï¼Œå®ç°32Kä¸Šä¸‹æ–‡çª—å£å’ŒOpenAIå…¼å®¹APIï¼Œé€‚ç”¨äºç”Ÿäº§ç¯å¢ƒã€‚é€šè¿‡ç²¾ç»†è°ƒæ•´éƒ¨ç½²å‚æ•°ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨æœ‰é™çš„GPUèµ„æºä¸‹æœ€å¤§åŒ–æ¨¡å‹æ€§èƒ½ã€‚</p>
<h2 id="-ç³»ç»Ÿè¦æ±‚">ğŸ–¥ï¸ ç³»ç»Ÿè¦æ±‚</h2>
<ul>
<li><strong>ç¡¬ä»¶é…ç½®</strong>
<ul>
<li>4å—NVIDIA GPU (æ¯å—22GBæ˜¾å­˜ï¼Œæ€»è®¡88GB)</li>
<li>512GBç³»ç»Ÿå†…å­˜</li>
<li>2TB SSDå­˜å‚¨</li>
<li>56æ ¸CPU</li>
</ul>
</li>
<li><strong>è½¯ä»¶ç¯å¢ƒ</strong>
<ul>
<li>Ubuntu 24.04</li>
<li>NVIDIAé©±åŠ¨ 550.144.03</li>
<li>CUDA 12.4</li>
<li>Docker + NVIDIA Container Toolkit</li>
</ul>
</li>
</ul>
<h2 id="-æ¨¡å‹ä¸æ¶æ„">ğŸ§  æ¨¡å‹ä¸æ¶æ„</h2>
<p>Qwen3-30B-A3Bæ˜¯é˜¿é‡Œäº‘å‘å¸ƒçš„é€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š</p>
<ul>
<li>30Bå‚æ•°é‡</li>
<li>åŸç”Ÿæ”¯æŒ32Kä¸Šä¸‹æ–‡é•¿åº¦</li>
<li>æ”¯æŒæ€è€ƒæ¨¡å¼(Chain-of-Thought)</li>
<li>ä¼˜å¼‚çš„å¤šè¯­è¨€ä¸ä»£ç èƒ½åŠ›</li>
</ul>
<p>æˆ‘ä»¬ä½¿ç”¨vLLMä½œä¸ºæ¨ç†å¼•æ“ï¼Œä¸»è¦åŸºäºä»¥ä¸‹è€ƒé‡ï¼š</p>
<ol>
<li><strong>é«˜æ•ˆå†…å­˜ç®¡ç†</strong>ï¼šé€šè¿‡PagedAttentionæŠ€æœ¯ä¼˜åŒ–KVç¼“å­˜</li>
<li><strong>å¼ é‡å¹¶è¡Œ</strong>ï¼šè‡ªåŠ¨è·¨å¤šGPUåˆ†å¸ƒæ¨¡å‹æƒé‡</li>
<li><strong>OpenAIå…¼å®¹API</strong>ï¼šç›´æ¥æ›¿ä»£OpenAI APIï¼Œæ— éœ€ä¿®æ”¹ç°æœ‰åº”ç”¨</li>
<li><strong>åŠ¨æ€æ‰¹å¤„ç†</strong>ï¼šè‡ªåŠ¨æ‰¹å¤„ç†å¤šè¯·æ±‚ï¼Œæé«˜ååé‡</li>
</ol>
<h2 id="-éƒ¨ç½²è„šæœ¬">ğŸ³ éƒ¨ç½²è„šæœ¬</h2>
<p>ä»¥ä¸‹æ˜¯æˆ‘ä»¬ç”¨äºéƒ¨ç½²çš„Dockerå‘½ä»¤ï¼Œç»è¿‡ç²¾å¿ƒè°ƒä¼˜ä»¥å¹³è¡¡æ€§èƒ½ä¸èµ„æºåˆ©ç”¨ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run -d <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --runtime<span style="color:#f92672">=</span>nvidia <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --gpus<span style="color:#f92672">=</span>all <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --name coder <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -v /home/llm/model/qwen/qwen3-30b-a3b:/qwen/qwen3-30b-a3b <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -p 8000:8000 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --cpuset-cpus 0-55 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --ulimit memlock<span style="color:#f92672">=</span>-1 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --ulimit stack<span style="color:#f92672">=</span><span style="color:#ae81ff">67108864</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --restart always <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --ipc<span style="color:#f92672">=</span>host <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  vllm/vllm-openai:v0.8.5 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --model /qwen/qwen3-30b-a3b <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --served-model-name coder <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --tensor-parallel-size <span style="color:#ae81ff">4</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --dtype half <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --max-model-len <span style="color:#ae81ff">32768</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --max-num-batched-tokens <span style="color:#ae81ff">4096</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --gpu-memory-utilization 0.93 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --block-size <span style="color:#ae81ff">32</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --enable-chunked-prefill <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --swap-space <span style="color:#ae81ff">16</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --tokenizer-pool-size <span style="color:#ae81ff">56</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --disable-custom-all-reduce
</span></span></code></pre></div><h2 id="-å‚æ•°è¯¦è§£ä¸ä¼˜åŒ–ç­–ç•¥">ğŸ”§ å‚æ•°è¯¦è§£ä¸ä¼˜åŒ–ç­–ç•¥</h2>
<h3 id="dockerå®¹å™¨é…ç½®">Dockerå®¹å™¨é…ç½®</h3>
<table>
  <thead>
      <tr>
          <th>å‚æ•°</th>
          <th>å€¼</th>
          <th>ä½œç”¨</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><code>--runtime=nvidia</code></td>
          <td></td>
          <td>å¯ç”¨NVIDIAå®¹å™¨è¿è¡Œæ—¶</td>
      </tr>
      <tr>
          <td><code>--gpus=all</code></td>
          <td></td>
          <td>å°†æ‰€æœ‰GPUæš´éœ²ç»™å®¹å™¨</td>
      </tr>
      <tr>
          <td><code>--cpuset-cpus</code></td>
          <td><code>0-55</code></td>
          <td>é™åˆ¶å®¹å™¨ä½¿ç”¨0-55å·CPUæ ¸å¿ƒ</td>
      </tr>
      <tr>
          <td><code>--ulimit memlock=-1</code></td>
          <td></td>
          <td>ç§»é™¤å†…å­˜é”å®šé™åˆ¶ï¼Œæé«˜æ€§èƒ½</td>
      </tr>
      <tr>
          <td><code>--ipc=host</code></td>
          <td></td>
          <td>ä½¿ç”¨ä¸»æœºIPCå‘½åç©ºé—´ï¼Œå¯¹å…±äº«å†…å­˜å¾ˆé‡è¦</td>
      </tr>
  </tbody>
</table>
<h3 id="vllmå¼•æ“é…ç½®">vLLMå¼•æ“é…ç½®</h3>
<h4 id="1-å¼ é‡å¹¶è¡Œç­–ç•¥">1. å¼ é‡å¹¶è¡Œç­–ç•¥</h4>
<pre tabindex="0"><code>--tensor-parallel-size 4
</code></pre><p>æˆ‘ä»¬ä½¿ç”¨4è·¯å¼ é‡å¹¶è¡Œï¼Œå°†æ¨¡å‹åˆ†å¸ƒåœ¨4å—GPUä¸Šã€‚è¿™æ˜¯åŸºäºå®éªŒå¾—å‡ºçš„æœ€ä½³é…ç½® - åœ¨æˆ‘ä»¬çš„ç¡¬ä»¶ä¸Šï¼Œæ¯å—22GBæ˜¾å­˜çš„GPUæ— æ³•å•ç‹¬åŠ è½½å®Œæ•´çš„30Bæ¨¡å‹ã€‚</p>
<h4 id="2-å†…å­˜ä¼˜åŒ–">2. å†…å­˜ä¼˜åŒ–</h4>
<pre tabindex="0"><code>--dtype half
--gpu-memory-utilization 0.93
--block-size 32
--swap-space 16
</code></pre><ul>
<li><code>half</code>ç²¾åº¦(FP16)ç›¸æ¯”<code>bfloat16</code>èƒ½è¿›ä¸€æ­¥èŠ‚çœå†…å­˜ï¼Œä¸”åœ¨æˆ‘ä»¬çš„åœºæ™¯ä¸­ç²¾åº¦æŸå¤±å¯æ¥å—</li>
<li>GPUå†…å­˜åˆ©ç”¨ç‡93%ç•™å‡ºä¸€å®šç¼“å†²ç©ºé—´é˜²æ­¢OOMé”™è¯¯</li>
<li>KVç¼“å­˜å—å¤§å°è®¾ä¸º32ï¼Œå¹³è¡¡å†…å­˜ä½¿ç”¨ä¸è®¡ç®—æ•ˆç‡</li>
<li>16GBçš„CPU-GPUäº¤æ¢ç©ºé—´æ”¯æŒå¤„ç†è¶…é•¿åºåˆ—</li>
</ul>
<h4 id="3-ä¸Šä¸‹æ–‡é•¿åº¦ä¸æ‰¹å¤„ç†">3. ä¸Šä¸‹æ–‡é•¿åº¦ä¸æ‰¹å¤„ç†</h4>
<pre tabindex="0"><code>--max-model-len 32768
--max-num-batched-tokens 4096
--enable-chunked-prefill
</code></pre><p>æˆ‘ä»¬å°†ä¸Šä¸‹æ–‡é•¿åº¦ä»é»˜è®¤çš„16Kå¢åŠ åˆ°32Kï¼Œä»¥æ”¯æŒæ›´é•¿è¾“å…¥å’Œè¾“å‡ºã€‚ä¸ºäº†å¹³è¡¡èµ„æºä½¿ç”¨ï¼Œç›¸åº”åœ°å°†æ‰¹å¤„ç†ä»¤ç‰Œæ•°ä»8192å‡å°‘åˆ°4096ï¼Œè¿™æ˜¯ä¸€ä¸ªç»è¿‡æµ‹è¯•çš„åˆç†æŠ˜ä¸­æ–¹æ¡ˆã€‚</p>
<p>å¯ç”¨åˆ†å—é¢„å¡«å……(<code>chunked-prefill</code>)å¯¹äºå¤„ç†é•¿ä¸Šä¸‹æ–‡å°¤ä¸ºé‡è¦ï¼Œå®ƒå°†é•¿åºåˆ—åˆ†è§£ä¸ºæ›´å°çš„å—è¿›è¡Œå¤„ç†ï¼Œå‡å°‘æ˜¾å­˜å³°å€¼ä½¿ç”¨ã€‚</p>
<h4 id="4-å…¶ä»–æ€§èƒ½è°ƒä¼˜">4. å…¶ä»–æ€§èƒ½è°ƒä¼˜</h4>
<pre tabindex="0"><code>--tokenizer-pool-size 56
--disable-custom-all-reduce
</code></pre><ul>
<li>ä»¤ç‰ŒåŒ–å·¥ä½œæ± å¤§å°ä¸CPUæ ¸å¿ƒæ•°åŒ¹é…ï¼Œä¼˜åŒ–å¹¶è¡Œå¤„ç†èƒ½åŠ›</li>
<li>ç¦ç”¨è‡ªå®šä¹‰all-reduceæ“ä½œï¼Œè§£å†³æŸäº›ç¡¬ä»¶é…ç½®ä¸Šçš„å…¼å®¹æ€§é—®é¢˜</li>
</ul>
<h2 id="-æ€§èƒ½åˆ†æ">ğŸ“Š æ€§èƒ½åˆ†æ</h2>
<p>éƒ¨ç½²åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡<code>docker logs -f coder</code>æŸ¥çœ‹æœåŠ¡çŠ¶æ€ï¼Œå…³é”®æ€§èƒ½æŒ‡æ ‡å¦‚ä¸‹ï¼š</p>
<pre tabindex="0"><code>INFO 06-03 02:01:19 [worker.py:287] the current vLLM instance can use total_gpu_memory (21.66GiB) x gpu_memory_utilization (0.93) = 20.15GiB
INFO 06-03 02:01:19 [worker.py:287] model weights take 14.25GiB; non_torch_memory takes 0.20GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 4.30GiB.
INFO 06-03 02:01:20 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 5.73x
</code></pre><p>è¿™è¡¨æ˜ï¼š</p>
<ul>
<li>æ¯ä¸ªGPUä½¿ç”¨çº¦20.15GBå†…å­˜</li>
<li>æ¨¡å‹æƒé‡å ç”¨14.25GB</li>
<li>å¯¹äº32Kä»¤ç‰Œè¯·æ±‚ï¼Œç³»ç»Ÿå¯ä»¥å¹¶å‘å¤„ç†5.73å€çš„è¯·æ±‚</li>
</ul>
<p>åœ¨æˆ‘ä»¬çš„ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¿™ä¸ªé…ç½®èƒ½å¤Ÿå¤„ç†æ¯åˆ†é’Ÿçº¦15-20ä¸ªå¹¶å‘å¯¹è¯ï¼Œæ»¡è¶³ä¸­å°å‹åº”ç”¨éœ€æ±‚ã€‚</p>
<h2 id="-apiä½¿ç”¨ç¤ºä¾‹">ğŸ“ APIä½¿ç”¨ç¤ºä¾‹</h2>
<p>æœåŠ¡å¯åŠ¨åï¼Œå¯ä»¥é€šè¿‡OpenAIå…¼å®¹çš„APIåœ¨æœ¬åœ°ç«¯å£8000è®¿é—®ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl http://localhost:8000/v1/chat/completions <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -H <span style="color:#e6db74">&#34;Content-Type: application/json&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -d <span style="color:#e6db74">&#39;{
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;model&#34;: &#34;coder&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;messages&#34;: [
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;è¯·è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†&#34;}
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    ],
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;temperature&#34;: 0.7,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;max_tokens&#34;: 2000
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  }&#39;</span>
</span></span></code></pre></div><p>ä½¿ç”¨Pythonå®¢æˆ·ç«¯ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> openai <span style="color:#f92672">import</span> OpenAI
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>client <span style="color:#f92672">=</span> OpenAI(
</span></span><span style="display:flex;"><span>    base_url<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;http://localhost:8000/v1&#34;</span>,
</span></span><span style="display:flex;"><span>    api_key<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;not-needed&#34;</span>  <span style="color:#75715e"># vLLMä¸è¦æ±‚APIå¯†é’¥</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> client<span style="color:#f92672">.</span>chat<span style="color:#f92672">.</span>completions<span style="color:#f92672">.</span>create(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;coder&#34;</span>,
</span></span><span style="display:flex;"><span>    messages<span style="color:#f92672">=</span>[
</span></span><span style="display:flex;"><span>        {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—&#34;</span>}
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>    temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">0.7</span>,
</span></span><span style="display:flex;"><span>    max_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(response<span style="color:#f92672">.</span>choices[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>message<span style="color:#f92672">.</span>content)
</span></span></code></pre></div><h2 id="-æ‰©å±•åˆ°æ›´é•¿ä¸Šä¸‹æ–‡">ğŸš€ æ‰©å±•åˆ°æ›´é•¿ä¸Šä¸‹æ–‡</h2>
<p>Qwen3-30BåŸç”Ÿæ”¯æŒ32Kä¸Šä¸‹æ–‡ï¼Œä½†å¦‚éœ€æ‰©å±•åˆ°æ›´é•¿ä¸Šä¸‹æ–‡(å¦‚131Kä»¤ç‰Œ)ï¼Œå¯ä»¥ä½¿ç”¨YaRNæŠ€æœ¯ï¼Œé€šè¿‡åœ¨vLLMå‚æ•°ä¸­æ·»åŠ ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>--rope-scaling <span style="color:#e6db74">&#39;{&#34;rope_type&#34;:&#34;yarn&#34;,&#34;factor&#34;:4.0,&#34;original_max_position_embeddings&#34;:32768}&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>--max-model-len <span style="color:#ae81ff">131072</span>
</span></span></code></pre></div><p>æ³¨æ„è¿™ä¼šå¢åŠ å†…å­˜ä½¿ç”¨ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´å…¶ä»–å‚æ•°ä»¥å¹³è¡¡èµ„æºã€‚</p>
<h2 id="-å¸¸è§é—®é¢˜æ’æŸ¥">ğŸ” å¸¸è§é—®é¢˜æ’æŸ¥</h2>
<ol>
<li><strong>OOMé”™è¯¯</strong>ï¼šå‡å°<code>gpu-memory-utilization</code>æˆ–<code>max-num-batched-tokens</code></li>
<li><strong>æ¨ç†é€Ÿåº¦æ…¢</strong>ï¼šæ£€æŸ¥GPUåˆ©ç”¨ç‡ï¼Œè€ƒè™‘å¢åŠ batchå¤§å°æˆ–å‡å°<code>max-model-len</code></li>
<li><strong>CUDAå›¾æ•è·å¤±è´¥</strong>ï¼šæ·»åŠ <code>--enforce-eager</code>å‚æ•°ç¦ç”¨CUDAå›¾ä¼˜åŒ–</li>
</ol>
<h2 id="-æœªæ¥ä¼˜åŒ–æ–¹å‘">ğŸ“ˆ æœªæ¥ä¼˜åŒ–æ–¹å‘</h2>
<ul>
<li>æ¢ç´¢ä½¿ç”¨FlashAttention-2åŠ é€Ÿæ³¨æ„åŠ›è®¡ç®—</li>
<li>å°è¯•AWQ/GPTQé‡åŒ–æŠ€æœ¯é™ä½å†…å­˜ä½¿ç”¨</li>
<li>é…ç½®LLM Routerå®ç°å¤šæ¨¡å‹è´Ÿè½½å‡è¡¡</li>
</ul>
<h2 id="-æ€»ç»“">ğŸ”š æ€»ç»“</h2>
<p>é€šè¿‡ç²¾ç»†è°ƒä¼˜vLLMéƒ¨ç½²å‚æ•°ï¼Œæˆ‘ä»¬æˆåŠŸåœ¨æœ‰é™ç¡¬ä»¶èµ„æºä¸‹éƒ¨ç½²äº†Qwen3-30Bæ¨¡å‹ï¼Œå®ç°äº†32Kä¸Šä¸‹æ–‡çª—å£çš„é«˜æ€§èƒ½æ¨ç†æœåŠ¡ã€‚è¿™å¥—é…ç½®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¡¨ç°ç¨³å®šï¼Œä¸ºå„ç±»åº”ç”¨æä¾›å¼ºå¤§çš„AIèƒ½åŠ›æ”¯æŒã€‚</p>
]]></content:encoded></item></channel></rss>