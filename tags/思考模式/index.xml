<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>思考模式 on Code Whispers</title><link>https://jackypanster.github.io/ai-stream/tags/%E6%80%9D%E8%80%83%E6%A8%A1%E5%BC%8F/</link><description>Recent content in 思考模式 on Code Whispers</description><generator>Hugo -- 0.148.1</generator><language>zh-cn</language><lastBuildDate>Wed, 04 Jun 2025 14:30:00 +0800</lastBuildDate><atom:link href="https://jackypanster.github.io/ai-stream/tags/%E6%80%9D%E8%80%83%E6%A8%A1%E5%BC%8F/index.xml" rel="self" type="application/rss+xml"/><item><title>Qwen3-30B 技术优化实践（二）：思考模式控制与15-20%性能提升</title><link>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-part2/</link><pubDate>Wed, 04 Jun 2025 14:30:00 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-part2/</guid><description>&lt;h1 id="qwen3-30b-技术优化实践二思考模式控制与性能提升">Qwen3-30B 技术优化实践（二）：思考模式控制与性能提升&lt;/h1>
&lt;blockquote>
&lt;p>本文是&lt;a href="blog-post.md">《从32K到131K：Qwen3-30B大模型上下文扩展实践》&lt;/a>的续篇，聚焦于模型性能调优特别是思考模式（reasoning mode）控制的技术细节与实践经验。&lt;/p>&lt;/blockquote>
&lt;p>在前文中，我们详细介绍了如何使用YaRN技术将Qwen3-30B的上下文长度从32K扩展到131K。今天，我们将深入探讨另一个关键优化维度：&lt;strong>思考模式控制&lt;/strong>及其对性能的影响。通过一系列实验和调优，我们发现禁用思考模式可以显著提升模型响应速度和内存效率，特别适合编程和直接输出类任务场景。&lt;/p>
&lt;h2 id="-思考模式reasoning-mode解析">🔍 思考模式（Reasoning Mode）解析&lt;/h2>
&lt;h3 id="什么是思考模式">什么是思考模式？&lt;/h3>
&lt;p>思考模式（Reasoning Mode，也称为Thinking Mode）是Qwen3系列模型的一个特性，让模型能够生成中间思考步骤，这些步骤被包含在&lt;code>&amp;lt;think&amp;gt;...&amp;lt;/think&amp;gt;&lt;/code>标签内。理论上，这种&amp;quot;思考过程&amp;quot;有助于模型进行更复杂的推理，但同时也引入了额外的计算和内存开销。&lt;/p>
&lt;p>在默认配置下，Qwen3模型会启用思考模式，产生类似以下的输出：&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;think&amp;gt;
首先，我需要分析用户的问题：如何实现一个简单的文件读写功能。
我应该使用Python的内置文件操作功能。
基本步骤应该是：
1. 打开文件（可以使用with语句自动管理资源）
2. 读取或写入内容
3. 确保文件正确关闭
&amp;lt;/think&amp;gt;
以下是一个简单的Python文件读写示例：
```python
# 写入文件
with open(&amp;#39;example.txt&amp;#39;, &amp;#39;w&amp;#39;) as file:
file.write(&amp;#39;Hello, World!&amp;#39;)
# 读取文件
with open(&amp;#39;example.txt&amp;#39;, &amp;#39;r&amp;#39;) as file:
content = file.read()
print(content)
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>
### 思考模式实现机制
vLLM部署Qwen3模型时，思考模式通过两种方式实现控制：
1. **服务器级控制**：通过部署参数`--enable-reasoning`和`--reasoning-parser deepseek_r1`启用
2. **API级控制**：通过API调用中的`chat_template_kwargs`参数或`enable_thinking`参数动态控制
我们的发现是，**仅删除服务器级别的参数并不足够完全禁用思考模式**，模型在某些情况下仍会产生思考过程。更彻底的解决方案是使用自定义聊天模板。
## 💡 禁用思考模式的技术实现
### 自定义聊天模板方案
经过研究Qwen官方文档和实验，我们发现使用自定义聊天模板是完全禁用思考模式的最可靠方法。我们创建了一个名为`qwen3_nonthinking.jinja`的模板文件：
```jinja
{% if messages %}
{% set loop_messages = messages %}
{% else %}
{% set loop_messages = [{&amp;#39;role&amp;#39;: &amp;#39;system&amp;#39;, &amp;#39;content&amp;#39;: &amp;#39;&amp;#39;}] %}
{% endif %}
{% for message in loop_messages %}
{% if message[&amp;#39;role&amp;#39;] == &amp;#39;user&amp;#39; %}
&amp;lt;|im_start|&amp;gt;user
{{ message[&amp;#39;content&amp;#39;] }}&amp;lt;|im_end|&amp;gt;
{% elif message[&amp;#39;role&amp;#39;] == &amp;#39;assistant&amp;#39; %}
&amp;lt;|im_start|&amp;gt;assistant
{{ message[&amp;#39;content&amp;#39;] }}&amp;lt;|im_end|&amp;gt;
{% elif message[&amp;#39;role&amp;#39;] == &amp;#39;system&amp;#39; %}
&amp;lt;|im_start|&amp;gt;system
{{ message[&amp;#39;content&amp;#39;] }}&amp;lt;|im_end|&amp;gt;
{% endif %}
{% endfor %}
&amp;lt;|im_start|&amp;gt;assistant
{% if add_generation_prompt is defined and add_generation_prompt %}{{ generation_prompt }}{% endif %}
&lt;/code>&lt;/pre>&lt;p>这个模板的关键点是&lt;strong>移除了所有与思考模式相关的标签和处理逻辑&lt;/strong>，确保模型无法生成&lt;code>&amp;lt;think&amp;gt;...&amp;lt;/think&amp;gt;&lt;/code>块，即使API请求中尝试启用思考模式。&lt;/p></description></item></channel></rss>