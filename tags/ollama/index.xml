<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Ollama on AI 避难所</title><link>https://jackypanster.github.io/ai-stream/tags/ollama/</link><description>Recent content in Ollama on AI 避难所</description><generator>Hugo -- 0.148.1</generator><language>en-us</language><lastBuildDate>Wed, 09 Jul 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://jackypanster.github.io/ai-stream/tags/ollama/index.xml" rel="self" type="application/rss+xml"/><item><title>从单卡瓶颈到四卡齐飞：一次完整的Ollama多GPU服务器性能优化实战</title><link>https://jackypanster.github.io/ai-stream/posts/the-ultimate-guide-to-multi-gpu-ollama-deployment/</link><pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/the-ultimate-guide-to-multi-gpu-ollama-deployment/</guid><description>记录如何将一台拥有4块RTX 2080 Ti的服务器，从最初的Ollama单点服务，逐步优化，最终搭建成一个高性能、高并发的负载均衡集群的全过程。</description><content:encoded><![CDATA[<h2 id="前言">前言</h2>
<p>服务器：4块NVIDIA RTX 2080 Ti，每张拥有22GB显存，总计88GB的VRAM。目标：让Ollama在这台机器上火力全开，为大语言模型提供强劲的推理服务。</p>
<p>然而，最初的想法——“如何用光所有显存？”——很快被证明是一个误区。真正的目标应该是：<strong>如何最高效地利用所有GPU资源，实现最大的吞吐量和最低的延迟？</strong></p>
<p>本文将完整记录从最初的配置探索，到发现并解决性能瓶颈，再到最终搭建起一个健壮的4-GPU负载均衡服务集群的全过程。这不仅是一份操作指南，更是一次充满洞见的性能优化之旅。</p>
<h2 id="第一章初探配置单卡运行的真相">第一章：初探配置，单卡运行的“真相”</h2>
<p>首先，确认硬件已被系统正确识别。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>$ nvidia-smi -L
</span></span><span style="display:flex;"><span>GPU 0: NVIDIA GeForce RTX <span style="color:#ae81ff">2080</span> Ti <span style="color:#f92672">(</span>UUID: GPU-b5040762-a75e-f78a-87eb-d288e4725f64<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>GPU 1: NVIDIA GeForce RTX <span style="color:#ae81ff">2080</span> Ti <span style="color:#f92672">(</span>UUID: GPU-651b0fe5-cdad-1851-df5d-e122f85ff10c<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>GPU 2: NVIDIA GeForce RTX <span style="color:#ae81ff">2080</span> Ti <span style="color:#f92672">(</span>UUID: GPU-7674114f-1e22-374d-982f-7446da0ce35f<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>GPU 3: NVIDIA GeForce RTX <span style="color:#ae81ff">2080</span> Ti <span style="color:#f92672">(</span>UUID: GPU-04eaff5d-28e2-94f3-3dd6-c0985dfdad24<span style="color:#f92672">)</span>
</span></span></code></pre></div><p>四张卡都在，一切正常。通过<code>systemd</code>来管理Ollama服务，并让它能“看到”所有GPU。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo systemctl edit ollama.service
</span></span></code></pre></div><p>在配置文件中，设置了以下关键环境变量：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-ini" data-lang="ini"><span style="display:flex;"><span><span style="color:#66d9ef">[Service]</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">Environment</span><span style="color:#f92672">=</span><span style="color:#e6db74">&#34;OLLAMA_HOST=0.0.0.0:9000&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">Environment</span><span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CUDA_VISIBLE_DEVICES=0,1,2,3&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 其他性能相关配置...</span>
</span></span></code></pre></div><p>重启服务后，运行了一个约7.5GB的<code>gemma3n</code>模型。通过<code>nvidia-smi</code>观察，一个关键现象出现了：只有一块GPU的显存被占用了！</p>
<p><strong>结论一：Ollama足够智能。</strong> 对于远小于单卡显存的模型，它会优先在单张卡内完成所有计算，以避免跨GPU通信带来的性能开销。这是最高效的做法，也打破了“必须用光所有显存”的迷思。</p>
<h2 id="第二章压力测试揭开软件瓶颈的面纱">第二章：压力测试，揭开软件瓶颈的面纱</h2>
<p>既然是单卡在工作，那它的性能极限在哪里？编写了一个Python异步压测脚本，模拟多个并发用户。</p>
<blockquote>
<p><strong>压测脚本 <code>benchmark.py</code> (核心逻辑)</strong>:
使用<code>asyncio</code>和<code>aiohttp</code>库，创建多个并发的worker，向Ollama的<code>/api/generate</code>流式端点发送请求，并收集成功率、首字响应时间（TTFT）和吞吐量（TPS）等指标。</p></blockquote>
<p>对当前配置（<code>OLLAMA_NUM_PARALLEL=4</code>）进行了测试。</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">并发用户数</th>
          <th style="text-align: left">平均首字响应 (TTFT)</th>
          <th style="text-align: left">整体服务吞吐量 (TPS)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">5</td>
          <td style="text-align: left">4.4 秒</td>
          <td style="text-align: left">89.06 tokens/秒</td>
      </tr>
      <tr>
          <td style="text-align: left">10</td>
          <td style="text-align: left"><strong>13.5 秒</strong></td>
          <td style="text-align: left"><strong>105.71 tokens/秒</strong></td>
      </tr>
      <tr>
          <td style="text-align: left">20</td>
          <td style="text-align: left"><strong>36.7 秒</strong></td>
          <td style="text-align: left"><strong>104.73 tokens/秒</strong></td>
      </tr>
  </tbody>
</table>
<p><strong>结果触目惊心！</strong></p>
<ol>
<li><strong>性能拐点</strong>：当并发数从10增加到20时，<strong>总吞吐量不再增长</strong>，稳定在约105 TPS。这是服务器达到性能上限的明确信号。</li>
<li><strong>延迟雪崩</strong>：与此同时，平均首字响应时间从13.5秒<strong>灾难性地飙升至36.7秒</strong>！这意味着用户体验已经差到无法接受。</li>
</ol>
<p><strong>瓶颈分析</strong>：硬件（单张2080 Ti）显然没有跑满，问题出在哪里？答案就在Ollama的配置里：<code>OLLAMA_NUM_PARALLEL=4</code>。这个参数限制了Ollama服务在同一时刻最多<strong>并行处理4个请求</strong>。当20个请求涌入时，有16个都在排队等待，导致了巨大的延迟。</p>
<p>我们找到了第一个真正的瓶颈：<strong>软件配置限制</strong>。</p>
<h2 id="第三章参数调优释放单卡全部潜力">第三章：参数调优，释放单卡全部潜力</h2>
<p>我们立即将瓶颈参数调整为一个更高的值。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># 在 systemd 配置文件中修改</span>
</span></span><span style="display:flex;"><span>Environment<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;OLLAMA_NUM_PARALLEL=16&#34;</span>
</span></span></code></pre></div><p>重启服务后，用同样的场景再次压测，结果令人振奋：</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">并发数</th>
          <th style="text-align: left">性能指标</th>
          <th style="text-align: left"><strong>优化前</strong> (Parallel=4)</th>
          <th style="text-align: left"><strong>优化后</strong> (Parallel=16)</th>
          <th style="text-align: left"><strong>性能提升幅度</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>20</strong></td>
          <td style="text-align: left"><strong>吞吐量 (TPS)</strong></td>
          <td style="text-align: left">104.73 tokens/秒</td>
          <td style="text-align: left"><strong>204.70 tokens/秒</strong></td>
          <td style="text-align: left"><strong>+ 95.5% (几乎翻倍)</strong></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>响应时间 (TTFT)</strong></td>
          <td style="text-align: left">36.7 秒</td>
          <td style="text-align: left"><strong>4.8 秒</strong></td>
          <td style="text-align: left"><strong>↓ 86.8% (速度提升7.5倍)</strong></td>
      </tr>
  </tbody>
</table>
<p><strong>结论二：一次教科书式的成功优化。</strong>
通过简单地调整一个参数，我们<strong>将单卡的吞吐能力翻了一番，同时将高并发下的延迟降低了87%</strong>。这证明了性能瓶颈已经成功地从软件队列转移到了更底层的硬件——即这块2080 Ti的原始计算能力。</p>
<h2 id="第四章终极形态构建4-gpu服务集群">第四章：终极形态，构建4-GPU服务集群</h2>
<p>单卡性能已优化到极限，但还有三张GPU在“旁观”。最佳方案是：<strong>为每张GPU部署一个独立的Ollama实例，并用Nginx实现负载均衡。</strong></p>
<h3 id="1-使用-systemd-模板单元">1. 使用 <code>systemd</code> 模板单元</h3>
<p>为了优雅地管理4个服务，我们使用<code>systemd</code>的模板功能，创建<code>ollama@.service</code>文件。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># 创建模板文件 /etc/systemd/system/ollama@.service</span>
</span></span><span style="display:flex;"><span>sudo nano /etc/systemd/system/ollama@.service
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-ini" data-lang="ini"><span style="display:flex;"><span><span style="color:#66d9ef">[Unit]</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">Description</span><span style="color:#f92672">=</span><span style="color:#e6db74">Ollama Service Instance for GPU %i</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">After</span><span style="color:#f92672">=</span><span style="color:#e6db74">network-online.target</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">[Service]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 使用 %i 动态计算端口号，并绑定到对应GPU</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">ExecStart</span><span style="color:#f92672">=</span><span style="color:#e6db74">/bin/bash -c &#39;OLLAMA_HOST=0.0.0.0:$(expr 9000 + %i) /usr/local/bin/ollama serve&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">User</span><span style="color:#f92672">=</span><span style="color:#e6db74">ollama</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">Group</span><span style="color:#f92672">=</span><span style="color:#e6db74">ollama</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">Restart</span><span style="color:#f92672">=</span><span style="color:#e6db74">always</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">RestartSec</span><span style="color:#f92672">=</span><span style="color:#e6db74">3</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">Environment</span><span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CUDA_VISIBLE_DEVICES=%i&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">Environment</span><span style="color:#f92672">=</span><span style="color:#e6db74">&#34;OLLAMA_NUM_PARALLEL=16&#34; # 每个实例都具备高并行处理能力</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ... 其他配置</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">[Install]</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">WantedBy</span><span style="color:#f92672">=</span><span style="color:#e6db74">multi-user.target</span>
</span></span></code></pre></div><p>然后用一个循环启动并启用所有实例：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># 先停用旧服务</span>
</span></span><span style="display:flex;"><span>sudo systemctl stop ollama.service
</span></span><span style="display:flex;"><span>sudo systemctl disable ollama.service
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 启动模板实例</span>
</span></span><span style="display:flex;"><span>sudo systemctl daemon-reload
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i in <span style="color:#f92672">{</span>0..3<span style="color:#f92672">}</span>; <span style="color:#66d9ef">do</span> sudo systemctl enable --now ollama@$i.service; <span style="color:#66d9ef">done</span>
</span></span></code></pre></div><h3 id="2-配置-nginx-负载均衡">2. 配置 Nginx 负载均衡</h3>
<p>让Nginx监听一个统一的入口端口<code>9999</code>，并将流量轮询分发给后端的4个Ollama实例。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># 在 /etc/nginx/conf.d/ 中创建配置文件</span>
</span></span><span style="display:flex;"><span>sudo nano /etc/nginx/conf.d/ollama-cluster.conf
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-nginx" data-lang="nginx"><span style="display:flex;"><span><span style="color:#75715e"># 定义Ollama后端服务器集群
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">upstream</span> <span style="color:#e6db74">ollama_backend</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">server</span> localhost:<span style="color:#ae81ff">9000</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">server</span> localhost:<span style="color:#ae81ff">9001</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">server</span> localhost:<span style="color:#ae81ff">9002</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">server</span> localhost:<span style="color:#ae81ff">9003</span>;
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">server</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 监听统一入口端口
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#f92672">listen</span> <span style="color:#ae81ff">9999</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">server_name</span> <span style="color:#e6db74">_</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">location</span> <span style="color:#e6db74">/</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">proxy_pass</span> <span style="color:#e6db74">http://ollama_backend</span>;
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 针对流式API的优化，关闭缓冲，增加超时
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        <span style="color:#f92672">proxy_read_timeout</span> <span style="color:#e6db74">3600s</span>;
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">proxy_buffering</span> <span style="color:#66d9ef">off</span>;
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 其他必要的代理头部设置...
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>测试并重启Nginx后，我们的服务集群就搭建完成了。</p>
<h2 id="第五章集群压测见证猛兽咆哮">第五章：集群压测，见证猛兽咆哮</h2>
<p>万事俱备，我们对最终的负载均衡入口 <code>http://localhost:9999</code> 发起了最后的总攻（20并发）。</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">性能指标</th>
          <th style="text-align: left"><strong>单GPU优化</strong> (c=20)</th>
          <th style="text-align: left"><strong>4-GPU集群</strong> (c=20)</th>
          <th style="text-align: left"><strong>性能提升幅度</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>吞吐量 (TPS)</strong></td>
          <td style="text-align: left">204.70 tokens/秒</td>
          <td style="text-align: left"><strong>366.01 tokens/秒</strong></td>
          <td style="text-align: left"><strong>+ 78.8%</strong></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>响应时间 (TTFT)</strong></td>
          <td style="text-align: left">4.8 秒</td>
          <td style="text-align: left"><strong>0.75 秒</strong></td>
          <td style="text-align: left"><strong>↓ 84.5% (速度提升6.4倍)</strong></td>
      </tr>
  </tbody>
</table>
<p><strong>最终结论：巨大成功！</strong></p>
<ol>
<li><strong>吞吐量</strong>：集群的总吞吐能力相比优化后的单卡，<strong>再次提升了近80%</strong>。这证明了负载均衡架构的有效性。</li>
<li><strong>响应延迟</strong>：<strong>TTFT从4.8秒骤降至0.75秒</strong>，几乎实现了瞬时响应。这对于任何交互式应用都是决定性的体验提升。</li>
</ol>
<p>成功地将一台拥有4块GPU的强大服务器，从一个单点服务演进为一个健壮、高性能、高并发的AI模型服务集群。它已经为承载生产级的应用请求做好了充分的准备。</p>
]]></content:encoded></item></channel></rss>