<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Awq on AI 避难所</title><link>https://jackypanster.github.io/ai-stream/tags/awq/</link><description>Recent content in Awq on AI 避难所</description><generator>Hugo -- 0.148.1</generator><language>en-us</language><lastBuildDate>Wed, 16 Jul 2025 13:19:48 +0800</lastBuildDate><atom:link href="https://jackypanster.github.io/ai-stream/tags/awq/index.xml" rel="self" type="application/rss+xml"/><item><title>Qwen3-32B-AWQ vLLM 多卡 2080 Ti 极限部署实战</title><link>https://jackypanster.github.io/ai-stream/posts/qwen3-32b-2080ti-vllm-deploy/</link><pubDate>Wed, 16 Jul 2025 13:19:48 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/qwen3-32b-2080ti-vllm-deploy/</guid><description>&lt;blockquote>
&lt;p>使用旧显卡也能跑 32B 大模型？本文手把手演示如何在 &lt;strong>4×RTX 2080 Ti (共 88 GB 显存)&lt;/strong> 服务器上，通过 vLLM 0.8.5 + AWQ 量化，跑起 &lt;strong>Qwen3-32B&lt;/strong> 并支持 &lt;strong>32 K tokens&lt;/strong> 超长上下文与高吞吐推理。全文记录了踩坑过程与参数权衡，希望给同样预算有限、硬件受限的工程师带来借鉴。&lt;/p>&lt;/blockquote>
&lt;p>{{.TableOfContents}}&lt;/p>
&lt;h2 id="1-项目背景">1 项目背景&lt;/h2>
&lt;ul>
&lt;li>主角：&lt;code>Qwen3-32B-AWQ&lt;/code> 量化模型 （≈ 18 GB）&lt;/li>
&lt;li>目标：在消费级 &lt;strong>Turing&lt;/strong> 架构显卡（2080 Ti）上最大化利用显存与吞吐。&lt;/li>
&lt;li>框架：&lt;code>vLLM 0.8.5&lt;/code> (openai-compatible server)&lt;/li>
&lt;li>取舍：牺牲部分延迟 / 稳定性 → 换取 &lt;strong>吞吐 + 上下文长度&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="2-硬件与系统环境">2 硬件与系统环境&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>组件&lt;/th>
&lt;th>规格&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>GPU&lt;/td>
&lt;td>4 × RTX 2080 Ti, 22 GB &lt;em>each&lt;/em>, Compute Capability 7.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CPU&lt;/td>
&lt;td>≥ 56 cores (vLLM 线程可吃满)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RAM&lt;/td>
&lt;td>512 GB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Storage&lt;/td>
&lt;td>NVMe SSD 2 TB (模型 + KV 缓冲)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OS&lt;/td>
&lt;td>Ubuntu 24.04&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Driver&lt;/td>
&lt;td>NVIDIA 570.153.02&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CUDA&lt;/td>
&lt;td>12.8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="21-nvidia-smi-基线信息">2.1 NVIDIA-SMI 基线信息&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>nvidia-smi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Wed Jul &lt;span style="color:#ae81ff">16&lt;/span> 13:27:17 &lt;span style="color:#ae81ff">2025&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----------------------------------------------------------------------------------------+
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| NVIDIA-SMI 570.153.02 Driver Version: 570.153.02 CUDA Version: 12.8 |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----------------------------------------------------------------------------------------+
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>可以看到驱动与 CUDA 版本与上表一致，确认环境无偏差。&lt;/p></description><content:encoded><![CDATA[<blockquote>
<p>使用旧显卡也能跑 32B 大模型？本文手把手演示如何在 <strong>4×RTX 2080 Ti (共 88 GB 显存)</strong> 服务器上，通过 vLLM 0.8.5 + AWQ 量化，跑起 <strong>Qwen3-32B</strong> 并支持 <strong>32 K tokens</strong> 超长上下文与高吞吐推理。全文记录了踩坑过程与参数权衡，希望给同样预算有限、硬件受限的工程师带来借鉴。</p></blockquote>
<p>{{.TableOfContents}}</p>
<h2 id="1-项目背景">1 项目背景</h2>
<ul>
<li>主角：<code>Qwen3-32B-AWQ</code> 量化模型  （≈ 18 GB）</li>
<li>目标：在消费级 <strong>Turing</strong> 架构显卡（2080 Ti）上最大化利用显存与吞吐。</li>
<li>框架：<code>vLLM 0.8.5</code> (openai-compatible server)</li>
<li>取舍：牺牲部分延迟 / 稳定性 → 换取 <strong>吞吐 + 上下文长度</strong></li>
</ul>
<h2 id="2-硬件与系统环境">2 硬件与系统环境</h2>
<table>
  <thead>
      <tr>
          <th>组件</th>
          <th>规格</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>GPU</td>
          <td>4 × RTX 2080 Ti, 22 GB <em>each</em>, Compute Capability 7.5</td>
      </tr>
      <tr>
          <td>CPU</td>
          <td>≥ 56 cores (vLLM 线程可吃满)</td>
      </tr>
      <tr>
          <td>RAM</td>
          <td>512 GB</td>
      </tr>
      <tr>
          <td>Storage</td>
          <td>NVMe SSD 2 TB (模型 + KV 缓冲)</td>
      </tr>
      <tr>
          <td>OS</td>
          <td>Ubuntu 24.04</td>
      </tr>
      <tr>
          <td>Driver</td>
          <td>NVIDIA 570.153.02</td>
      </tr>
      <tr>
          <td>CUDA</td>
          <td>12.8</td>
      </tr>
  </tbody>
</table>
<h3 id="21-nvidia-smi-基线信息">2.1 NVIDIA-SMI 基线信息</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>nvidia-smi
</span></span><span style="display:flex;"><span>Wed Jul <span style="color:#ae81ff">16</span> 13:27:17 <span style="color:#ae81ff">2025</span>
</span></span><span style="display:flex;"><span>+-----------------------------------------------------------------------------------------+
</span></span><span style="display:flex;"><span>| NVIDIA-SMI 570.153.02             Driver Version: 570.153.02     CUDA Version: 12.8     |
</span></span><span style="display:flex;"><span>+-----------------------------------------------------------------------------------------+
</span></span></code></pre></div><blockquote>
<p>可以看到驱动与 CUDA 版本与上表一致，确认环境无偏差。</p></blockquote>
<blockquote>
<p><strong>为什么 2080 Ti？</strong> 二手市场价格友好，但 Flash-Attention-2 不支持，需要自己编译 flash-attn-1 或使用 XFormers。</p></blockquote>
<h2 id="3-快速部署步骤概览">3 快速部署步骤概览</h2>
<ol>
<li>下载并解压 <code>Qwen3-32B-AWQ</code> 权重至 <code>/home/llm/model/qwen/Qwen3-32B-AWQ</code>。</li>
<li>（可选）编译 <code>flash-attn-1</code> 以替代原生 attention。</li>
<li>拉取官方 vLLM 镜像 <code>vllm/vllm-openai:v0.8.5</code>。</li>
<li>按下文 <strong>run.sh</strong> 参数启动容器。</li>
</ol>
<p>下面拆解每一步的技术细节。</p>
<h3 id="31-模型准备">3.1 模型准备</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>mkdir -p /home/llm/model/qwen
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 省略 huggingface-cli 登录步骤</span>
</span></span><span style="display:flex;"><span>huggingface-cli download Qwen/Qwen3-32B-AWQ --local-dir /home/llm/model/qwen/Qwen3-32B-AWQ --local-dir-use-symlinks False
</span></span></code></pre></div><h3 id="32-编译-flash-attention-12080-ti-专用">3.2 编译 Flash-Attention-1（2080 Ti 专用）</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># CUDA 12.x + Python 3.12 示例</span>
</span></span><span style="display:flex;"><span>python3 -m pip install --upgrade pip
</span></span><span style="display:flex;"><span>python3 -m pip install ninja packaging cmake
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 强制源码编译，确保生成 sm75 kernel</span>
</span></span><span style="display:flex;"><span>FLASH_ATTENTION_FORCE_BUILD<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  python3 -m pip install flash-attn --no-build-isolation --no-binary :all:
</span></span></code></pre></div><blockquote>
<p><strong>容器用户请注意</strong>：如果使用下文的官方 vLLM Docker 镜像，需在 <em>容器内部</em> 或自建 Dockerfile 完成同样的 flash-attn-1 编译（或将已编译好的 wheel 复制进镜像）。宿主机安装的 Python 包不会被容器环境读取。</p></blockquote>
<h4 id="321-无需重建大镜像的折中做法">3.2.1 无需重建大镜像的折中做法</h4>
<table>
  <thead>
      <tr>
          <th>做法</th>
          <th>说明</th>
          <th>额外体积</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>启动时临时 <code>--pip-install</code></strong></td>
          <td>vLLM ≥0.9 支持 <code>--pip</code> 参数，容器启动时即在线编译 <code>flash-attn</code></td>
          <td>0（编译产物缓存于 volume）</td>
      </tr>
      <tr>
          <td><strong>宿主机先编译 wheel</strong></td>
          <td><code>pip wheel flash-attn -w /tmp/wheels</code>，运行时挂载 <code>/tmp/wheels</code> 并 <code>pip install</code></td>
          <td>~30-40 MB</td>
      </tr>
      <tr>
          <td><strong>改用 XFormers</strong></td>
          <td>加 <code>--xformers</code>，性能略低于 flash-attn-1，但免编译</td>
          <td>0</td>
      </tr>
      <tr>
          <td><strong>保持默认 attention</strong></td>
          <td>对吞吐要求一般的场景可接受</td>
          <td>0</td>
      </tr>
  </tbody>
</table>
<blockquote>
<p>推荐顺序：临时 <code>--pip</code> &gt; wheel 挂载 &gt; XFormers &gt; 默认 Attention。按业务对性能 &amp; 简易度的权衡自行选择。</p></blockquote>
<p>验证：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python3 - <span style="color:#e6db74">&lt;&lt;&#39;PY&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">import flash_attn, torch, platform
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">print(&#39;flash-attn&#39;, flash_attn.__version__, &#39;torch&#39;, torch.__version__, &#39;python&#39;, platform.python_version())
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">PY</span>
</span></span></code></pre></div><h3 id="33-启动脚本-runsh">3.3 启动脚本 <code>run.sh</code></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e">#!/usr/bin/env bash
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>docker run -d <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --runtime<span style="color:#f92672">=</span>nvidia --gpus<span style="color:#f92672">=</span>all --name coder <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -v /home/llm/model/qwen/Qwen3-32B-AWQ:/model/Qwen3-32B-AWQ <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -p 8888:8000 --cpuset-cpus 0-55 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --ulimit memlock<span style="color:#f92672">=</span>-1 --ulimit stack<span style="color:#f92672">=</span><span style="color:#ae81ff">67108864</span> --restart always --ipc<span style="color:#f92672">=</span>host <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  vllm/vllm-openai:v0.8.5 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --model /model/Qwen3-32B-AWQ --served-model-name coder <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --tensor-parallel-size <span style="color:#ae81ff">4</span> --quantization awq --dtype auto <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --max-model-len <span style="color:#ae81ff">32768</span> --max-num-batched-tokens <span style="color:#ae81ff">32768</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --gpu-memory-utilization 0.96 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --block-size <span style="color:#ae81ff">16</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --enable-prefix-caching <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --swap-space <span style="color:#ae81ff">64</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --max-num-seqs <span style="color:#ae81ff">64</span>
</span></span></code></pre></div><blockquote>
<p><strong>容器 vs 本机</strong>：直接裸跑亦可，核心参数完全相同。容器便于复现与快速重启。</p></blockquote>
<h2 id="4-关键运行参数拆解">4 关键运行参数拆解</h2>
<table>
  <thead>
      <tr>
          <th>参数</th>
          <th>作用 / 调优思路</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><code>--tensor-parallel-size 4</code></td>
          <td>4 卡切分模型参数，2080 Ti 单卡显存有限必须拆分。</td>
      </tr>
      <tr>
          <td><code>--quantization awq</code></td>
          <td>启用 <strong>AWQ</strong> 权重量化，显存≈再降 40%。某些长文本场景下 FP16 仍更快，需实测。</td>
      </tr>
      <tr>
          <td><code>--max-model-len 32768</code></td>
          <td>支持 32 K tokens；大幅增加 KV Cache，需要配合 <code>--swap-space</code>。</td>
      </tr>
      <tr>
          <td><code>--max-num-batched-tokens 32768</code></td>
          <td>单批次 tokens 上限。吞吐 / 显存 trade-off。</td>
      </tr>
      <tr>
          <td><code>--gpu-memory-utilization 0.96</code></td>
          <td>近乎吃满显存，谨慎调；留 0.04 作余量。</td>
      </tr>
      <tr>
          <td><code>--block-size 16</code></td>
          <td>KV Cache 分块。块越小越灵活，管理开销稍增。</td>
      </tr>
      <tr>
          <td><code>--enable-prefix-caching</code></td>
          <td>高复用 prompt 命中率可&gt;90%，显著提升长对话吞吐。</td>
      </tr>
      <tr>
          <td><code>--swap-space 64</code></td>
          <td>允许 64 GB CPU RAM 作为 KV Cache 溢出。swap 大延迟高。</td>
      </tr>
      <tr>
          <td><code>--max-num-seqs 64</code></td>
          <td>控制并发序列数。越大吞吐高，长文本 OOM 风险也高。</td>
      </tr>
  </tbody>
</table>
<h2 id="5-api-调用范例">5 API 调用范例</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl http://localhost:8888/v1/chat/completions <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -H <span style="color:#e6db74">&#39;Content-Type: application/json&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -d <span style="color:#e6db74">&#39;{
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;model&#34;: &#34;coder&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;messages&#34;: [
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;你是一个聪明的 AI 助手。&#34;},
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;请写一个 Python 冒泡排序。&#34;}
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    ],
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;max_tokens&#34;: 512,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;temperature&#34;: 0.2
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  }&#39;</span>
</span></span></code></pre></div><ul>
<li><code>max_tokens</code> 建议 512 ~ 2048；极限 context 时过大易 <strong>OOM</strong>。</li>
<li><code>stream=true</code> 可获得流式输出；耗时更短，占用更低。</li>
</ul>
<h2 id="6-性能压榨技巧">6 性能压榨技巧</h2>
<ol>
<li><strong>AWQ vs FP16</strong>
<ul>
<li>某些推理阶段 AWQ kernel 尚未优化，🚀 结果 FP16 更快。实测二选一。</li>
</ul>
</li>
<li><strong>Flash-Attn-1 / XFormers</strong>
<ul>
<li>2080 Ti 无 <strong>Flash-Attn-2</strong>；编译 v1 或使用 XFormers 皆可。</li>
</ul>
</li>
<li><strong>KV Cache &amp; Swap</strong>
<ul>
<li>监控 <code>gpu_kv_cache</code> 与 <code>swap_used</code> 两项；长文本易炸。</li>
</ul>
</li>
<li><strong>多实例分卡</strong>
<ul>
<li>把 4 卡拆成 2 × 2 卡实例，可提高 GPU 利用率 (不同业务负载)。</li>
</ul>
</li>
<li><strong>自动降级</strong>
<ul>
<li>在 API 层检测 OOM → 自动缩短上下文 or 调小并发，保证可用性。</li>
</ul>
</li>
</ol>
<h2 id="7-常见问题速查">7 常见问题速查</h2>
<table>
  <thead>
      <tr>
          <th>症状</th>
          <th>解决方案</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>返回不完整/截断</strong></td>
          <td>增大 <code>max_tokens</code>；缩短输入；检查日志中 <code>context_window</code>。</td>
      </tr>
      <tr>
          <td><strong>CUDA OOM / 容器崩溃</strong></td>
          <td>降低 <code>max-model-len</code>、<code>max-num-batched-tokens</code>；增大 <code>swap-space</code>。</td>
      </tr>
      <tr>
          <td><strong>推理速度慢</strong></td>
          <td>确认 flash-attn-1 已启用；并发不要过高；尝试 FP16。</td>
      </tr>
      <tr>
          <td><strong>NCCL 死锁 / hang</strong></td>
          <td>加 <code>--disable-custom-all-reduce</code> 或升级 NCCL。</td>
      </tr>
  </tbody>
</table>
<h2 id="8-实战压测结果-10-并发--32-k-prompt">8 实战压测结果 (10 并发 · 32 K prompt)</h2>
<table>
  <thead>
      <tr>
          <th>指标</th>
          <th>数值</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Avg prompt throughput</td>
          <td><strong>63 K tokens/s</strong></td>
      </tr>
      <tr>
          <td>Avg generation throughput</td>
          <td><strong>57 tokens/s</strong></td>
      </tr>
      <tr>
          <td>平均响应时间</td>
          <td><strong>5.63 s</strong></td>
      </tr>
      <tr>
          <td>GPU KV Cache 占用</td>
          <td>15 %</td>
      </tr>
      <tr>
          <td>Prefix cache 命中率</td>
          <td>94 %</td>
      </tr>
      <tr>
          <td>错误 / OOM</td>
          <td>0</td>
      </tr>
  </tbody>
</table>
<blockquote>
<p>高吞吐归功于：1) prefix caching 2) AWQ 量化 3) 近乎满显存利用。</p></blockquote>
<h3 id="结果解读">结果解读</h3>
<ul>
<li><strong>吞吐</strong>：输入阶段 63K tokens/s，生成阶段 57 tokens/s，对 32B 模型非常可观。</li>
<li><strong>资源</strong>：GPU KV Cache 仅 15 %；系统还可上调并发 / 上下文。</li>
<li><strong>稳定</strong>：长时间压测无 OOM / pending；容器 restart=always 可兜底。</li>
</ul>
<h2 id="9-总结--建议">9 总结 &amp; 建议</h2>
<p>使用旧世代显卡并不意味着放弃大模型。通过 <strong>vLLM + AWQ + Prefix Cache</strong> 等组合拳，4×2080 Ti 依旧能够支撑 <strong>Qwen3-32B</strong> 的 32 K 超长上下文推理。</p>
<ul>
<li><strong>科研 / 测试</strong> 场景：强烈推荐该方案，可用最低成本探索大模型推理极限。</li>
<li><strong>生产</strong> 场景：需谨慎评估崩溃概率与延迟，做好监控与自动降级。</li>
</ul>
<p>⚙️ <strong>后续方向</strong></p>
<ol>
<li>迁移到 <strong>RTX 5000 Ada</strong> 等新卡，可解锁 Flash-Attn-2 与更高带宽。</li>
<li>关注 vLLM 后续对 AWQ Kernel 的优化；升级 &gt;=0.9 可能免去自己编译。</li>
<li>尝试 <strong>TensorRT-LLM</strong> 自动并行拆分，获得额外 10~20% 性能。</li>
</ol>
]]></content:encoded></item><item><title>Qwen3-32B-AWQ 高效部署：基于 vLLM 的深度实践与优化</title><link>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-32b-awq-vllm-guide/</link><pubDate>Tue, 10 Jun 2025 20:45:00 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-32b-awq-vllm-guide/</guid><description>本文详细介绍了如何使用 vLLM 高效部署 Qwen3-32B-AWQ 量化模型，实现 32K 上下文窗口、OpenAI 兼容 API，并禁用思考模式。通过对 Docker 及 vLLM 参数的精细调优，最大化模型在多 GPU 环境下的推理性能。</description><content:encoded><![CDATA[<ul>
<li><a href="#%E6%A6%82%E8%BF%B0">📋 概述</a></li>
<li><a href="#%E7%B3%BB%E7%BB%9F%E4%B8%8E%E7%8E%AF%E5%A2%83%E8%A6%81%E6%B1%82">🖥️ 系统与环境要求</a>
<ul>
<li><a href="#%E7%A1%AC%E4%BB%B6%E9%85%8D%E7%BD%AE">硬件配置</a></li>
<li><a href="#%E8%BD%AF%E4%BB%B6%E7%8E%AF%E5%A2%83">软件环境</a></li>
</ul>
</li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E4%B8%8E%E9%83%A8%E7%BD%B2%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90">🧠 模型与部署架构解析</a>
<ul>
<li><a href="#qwen3-32b-awq-%E6%A8%A1%E5%9E%8B%E7%89%B9%E6%80%A7">Qwen3-32B-AWQ 模型特性</a></li>
<li><a href="#vllm%E4%B8%BA%E4%BD%95%E9%80%89%E6%8B%A9%E5%AE%83">vLLM：为何选择它？</a></li>
<li><a href="#%E5%85%B3%E9%94%AE%E9%9C%80%E6%B1%82%E7%A6%81%E7%94%A8%E6%80%9D%E8%80%83%E6%A8%A1%E5%BC%8F-enable_thinkingfalse">关键需求：禁用思考模式 (enable_thinking=False)</a></li>
</ul>
</li>
<li><a href="#%E6%A0%B8%E5%BF%83%E9%83%A8%E7%BD%B2%E6%AD%A5%E9%AA%A4">🛠️ 核心部署步骤</a>
<ul>
<li><a href="#%E5%87%86%E5%A4%87%E8%87%AA%E5%AE%9A%E4%B9%89%E8%81%8A%E5%A4%A9%E6%A8%A1%E6%9D%BF">准备自定义聊天模板</a></li>
<li><a href="#docker-%E9%83%A8%E7%BD%B2%E5%91%BD%E4%BB%A4">Docker 部署命令</a></li>
</ul>
</li>
<li><a href="#%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3%E4%B8%8E%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5">⚙️ 参数详解与优化策略</a>
<ul>
<li><a href="#docker-%E5%AE%B9%E5%99%A8%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0">Docker 容器配置参数</a></li>
<li><a href="#vllm-%E5%BC%95%E6%93%8E%E6%A0%B8%E5%BF%83%E5%8F%82%E6%95%B0">vLLM 引擎核心参数</a></li>
</ul>
</li>
<li><a href="#%E9%83%A8%E7%BD%B2%E9%AA%8C%E8%AF%81%E4%B8%8E%E6%B5%8B%E8%AF%95">🧪 部署验证与测试</a>
<ul>
<li><a href="#%E6%A3%80%E6%9F%A5-docker-%E6%97%A5%E5%BF%97">检查 Docker 日志</a></li>
<li><a href="#%E4%BD%BF%E7%94%A8-python-%E8%84%9A%E6%9C%AC%E9%AA%8C%E8%AF%81-api">使用 Python 脚本验证 API</a>
<ul>
<li><a href="#%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87%E4%BD%BF%E7%94%A8-uv-%E7%AE%A1%E7%90%86%E4%BE%9D%E8%B5%96">环境准备：使用 uv 管理依赖</a></li>
<li><a href="#%E9%AA%8C%E8%AF%81%E8%84%9A%E6%9C%AC%E4%B8%8E%E9%A2%84%E6%9C%9F%E8%BE%93%E5%87%BA">验证脚本与预期输出</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E9%A1%B9%E7%9B%AE%E6%BA%90%E7%A0%81">🔗 项目源码</a></li>
<li><a href="#%E6%80%BB%E7%BB%93">🔚 总结</a></li>
</ul>
<h2 id="-概述"><!-- raw HTML omitted --><!-- raw HTML omitted -->📋 概述</h2>
<p>随着大语言模型 (LLM) 的飞速发展，如何在有限的硬件资源下高效部署这些庞然大物，成为了业界关注的焦点。本文将聚焦于阿里巴巴通义千问团队最新推出的 <code>Qwen3-32B-AWQ</code> 模型，详细阐述如何利用 vLLM 这一高性能推理引擎，在多 GPU 环境下实现其高效、稳定的部署。我们将覆盖从环境准备、模型特性解析、部署命令调优，到最终的功能验证与 API 测试的全过程，特别关注 32K 长上下文处理、AWQ (Activation-aware Weight Quantization) 量化模型的特性，以及如何通过自定义聊天模板禁用模型的“思考模式” (即 <code>&lt;think&gt;...&lt;/think&gt;</code> 标签的输出)。</p>
<p>本文旨在为希望在生产环境中部署 Qwen3 系列模型的工程师提供一份详尽的实践指南和优化参考。项目完整代码已开源，欢迎交流：<a href="https://github.com/jackypanster/deploy-qwen3-32b-awq">https://github.com/jackypanster/deploy-qwen3-32b-awq</a></p>
<h2 id="-系统与环境要求"><!-- raw HTML omitted --><!-- raw HTML omitted -->🖥️ 系统与环境要求</h2>
<h3 id="硬件配置"><!-- raw HTML omitted --><!-- raw HTML omitted -->硬件配置</h3>
<ul>
<li><strong>GPU</strong>: 4块 NVIDIA GPU (每块至少 22GB VRAM，总计约 88GB，推荐 Ampere 架构及以上，但本项目在 Volta/Turing 架构验证通过)</li>
<li><strong>系统内存</strong>: 建议 512GB 及以上</li>
<li><strong>存储</strong>: 建议 2TB 高速 SSD (模型文件约 60-70GB，加上 Docker 镜像和日志等)</li>
<li><strong>CPU</strong>: 建议 56 核及以上 (用于数据预处理、Tokenizer 池等)</li>
</ul>
<h3 id="软件环境"><!-- raw HTML omitted --><!-- raw HTML omitted -->软件环境</h3>
<ul>
<li><strong>操作系统</strong>: Ubuntu 24.04 (或其它兼容的 Linux 发行版)</li>
<li><strong>NVIDIA 驱动</strong>: 570.153.02 (或更高版本，需与 CUDA 12.8 兼容)</li>
<li><strong>CUDA 版本</strong>: 12.8 (vLLM 依赖)</li>
<li><strong>Docker</strong>: 最新稳定版，并已安装 NVIDIA Container Toolkit</li>
<li><strong>vLLM Docker 镜像</strong>: <code>vllm/vllm-openai:v0.8.5</code> (或项目验证时使用的最新兼容版本)</li>
</ul>
<h2 id="-模型与部署架构解析"><!-- raw HTML omitted --><!-- raw HTML omitted -->🧠 模型与部署架构解析</h2>
<h3 id="qwen3-32b-awq-模型特性"><!-- raw HTML omitted --><!-- raw HTML omitted -->Qwen3-32B-AWQ 模型特性</h3>
<p><code>Qwen3-32B-AWQ</code> 是 Qwen3 系列中的 320 亿参数规模的模型，并采用了 AWQ 量化技术。</p>
<ul>
<li><strong>32B 参数</strong>: 在性能和资源消耗之间取得了较好的平衡。</li>
<li><strong>AWQ 量化</strong>: Activation-aware Weight Quantization 是一种先进的量化技术，它能够在显著降低模型显存占用和加速推理的同时，最大限度地保持模型精度。相比于传统的 FP16/BF16 推理，AWQ 模型通常能以 INT4/INT8 混合精度运行，对硬件要求更低。</li>
<li><strong>32K 上下文长度</strong>: 原生支持高达 32,768 个 token 的上下文长度，使其能够处理更复杂的长文本任务。</li>
<li><strong>禁用思考模式</strong>: 对于某些应用场景，我们不希望模型输出中间的思考过程 (如 Qwen 系列特有的 <code>&lt;think&gt;...&lt;/think&gt;</code> 标签)。本项目通过自定义 Jinja 聊天模板在服务端强制禁用了此功能。</li>
</ul>
<h3 id="vllm为何选择它"><!-- raw HTML omitted --><!-- raw HTML omitted -->vLLM：为何选择它？</h3>
<p>vLLM 是一个专为 LLM 推理设计的高性能引擎，其核心优势包括：</p>
<ul>
<li><strong>PagedAttention</strong>: 一种新颖的注意力算法，有效管理 KV 缓存，显著减少内存浪费和碎片，从而支持更长的序列和更大的批处理大小。</li>
<li><strong>连续批处理 (Continuous Batching)</strong>: 请求无需等待批处理中的所有序列完成，可以动态插入新的请求，大幅提高 GPU 利用率和吞吐量。</li>
<li><strong>张量并行</strong>: 自动且高效地将模型权重和计算任务分布到多个 GPU 上，简化了多 GPU 部署的复杂性。</li>
<li><strong>OpenAI 兼容 API</strong>: 提供与 OpenAI API 一致的接口，使得现有应用可以无缝迁移。</li>
<li><strong>广泛的模型支持和社区活跃</strong>: 支持包括 Qwen 在内的众多主流模型，并且社区活跃，迭代迅速。</li>
</ul>
<h3 id="关键需求禁用思考模式-enable_thinkingfalse"><!-- raw HTML omitted --><!-- raw HTML omitted -->关键需求：禁用思考模式 (enable_thinking=False)</h3>
<p>Qwen 模型在某些情况下会输出包含 <code>&lt;think&gt;...&lt;/think&gt;</code> 标签的中间思考过程。在我们的应用场景中，这并非期望行为。为了确保 API 输出的纯净性，我们采用了自定义 Jinja 聊天模板的方式。该模板在服务端处理用户输入时，不会引导模型进入“思考”流程。相比于在客户端每次请求时传递 <code>enable_thinking=False</code> 参数，服务端模板的方式更为彻底和统一。</p>
<h2 id="-核心部署步骤"><!-- raw HTML omitted --><!-- raw HTML omitted -->🛠️ 核心部署步骤</h2>
<h3 id="准备自定义聊天模板"><!-- raw HTML omitted --><!-- raw HTML omitted -->准备自定义聊天模板</h3>
<p>在项目根目录下创建 <code>qwen3_nonthinking.jinja</code> 文件，内容如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-jinja" data-lang="jinja"><span style="display:flex;"><span><span style="color:#75715e">{%</span> <span style="color:#66d9ef">for</span> message <span style="color:#66d9ef">in</span> messages <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">if</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;role&#39;</span><span style="color:#f92672">]</span> <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;system&#39;</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">{{</span><span style="color:#e6db74">&#39;&lt;|im_start|&gt;system
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">+</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;content&#39;</span><span style="color:#f92672">]</span> <span style="color:#f92672">+</span> <span style="color:#e6db74">&#39;&lt;|im_end|&gt;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;</span><span style="color:#75715e">}}</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">elif</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;role&#39;</span><span style="color:#f92672">]</span> <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;user&#39;</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">{{</span><span style="color:#e6db74">&#39;&lt;|im_start|&gt;user
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">+</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;content&#39;</span><span style="color:#f92672">]</span> <span style="color:#f92672">+</span> <span style="color:#e6db74">&#39;&lt;|im_end|&gt;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;</span><span style="color:#75715e">}}</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">elif</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;role&#39;</span><span style="color:#f92672">]</span> <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;assistant&#39;</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">{{</span><span style="color:#e6db74">&#39;&lt;|im_start|&gt;assistant
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">+</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;content&#39;</span><span style="color:#f92672">]</span> <span style="color:#f92672">+</span> <span style="color:#e6db74">&#39;&lt;|im_end|&gt;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;</span><span style="color:#75715e">}}</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">endif</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">{%</span> <span style="color:#66d9ef">endfor</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">{%</span> <span style="color:#66d9ef">if</span> add_generation_prompt <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{{</span><span style="color:#e6db74">&#39;&lt;|im_start|&gt;assistant
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;</span><span style="color:#75715e">}}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">{%</span> <span style="color:#66d9ef">endif</span> <span style="color:#75715e">%}</span>
</span></span></code></pre></div><p>此模板移除了可能触发思考模式的特殊指令。</p>
<h3 id="docker-部署命令"><!-- raw HTML omitted --><!-- raw HTML omitted -->Docker 部署命令</h3>
<p>假设模型文件已下载到宿主机的 <code>/home/llm/model/qwen/Qwen3-32B-AWQ</code> 目录，从项目工作区根目录执行以下命令：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run -d <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --runtime<span style="color:#f92672">=</span>nvidia <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --gpus<span style="color:#f92672">=</span>all <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --name coder <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -v /home/llm/model/qwen/Qwen3-32B-AWQ:/model/Qwen3-32B-AWQ <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -v <span style="color:#66d9ef">$(</span>pwd<span style="color:#66d9ef">)</span>/qwen3_nonthinking.jinja:/app/qwen3_nonthinking.jinja <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -p 8000:8000 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --cpuset-cpus 0-55 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --ulimit memlock<span style="color:#f92672">=</span>-1 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --ulimit stack<span style="color:#f92672">=</span><span style="color:#ae81ff">67108864</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --restart always <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --ipc<span style="color:#f92672">=</span>host <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  vllm/vllm-openai:v0.8.5 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --model /model/Qwen3-32B-AWQ <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --served-model-name coder <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --tensor-parallel-size <span style="color:#ae81ff">4</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --dtype half <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --quantization awq <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --max-model-len <span style="color:#ae81ff">32768</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --max-num-batched-tokens <span style="color:#ae81ff">4096</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --gpu-memory-utilization 0.93 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --block-size <span style="color:#ae81ff">32</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --enable-chunked-prefill <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --swap-space <span style="color:#ae81ff">16</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --tokenizer-pool-size <span style="color:#ae81ff">56</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --disable-custom-all-reduce <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --chat-template /app/qwen3_nonthinking.jinja
</span></span></code></pre></div><h2 id="-参数详解与优化策略"><!-- raw HTML omitted --><!-- raw HTML omitted -->⚙️ 参数详解与优化策略</h2>
<h3 id="docker-容器配置参数"><!-- raw HTML omitted --><!-- raw HTML omitted -->Docker 容器配置参数</h3>
<ul>
<li><code>-d</code>: 后台运行容器。</li>
<li><code>--runtime=nvidia --gpus=all</code>: 使用 NVIDIA runtime 并分配所有 GPU。</li>
<li><code>--name coder</code>: 为容器命名，方便管理。</li>
<li><code>-v /home/llm/model/qwen/Qwen3-32B-AWQ:/model/Qwen3-32B-AWQ</code>: 挂载本地模型目录到容器内。</li>
<li><code>-v $(pwd)/qwen3_nonthinking.jinja:/app/qwen3_nonthinking.jinja</code>: 挂载自定义聊天模板。</li>
<li><code>-p 8000:8000</code>: 映射端口。</li>
<li><code>--cpuset-cpus 0-55</code>: 绑定 CPU核心，避免资源争抢。</li>
<li><code>--ulimit memlock=-1 --ulimit stack=67108864</code>: 解除内存锁定限制，设置较大堆栈空间，对性能和稳定性有益。</li>
<li><code>--restart always</code>: 容器异常退出时自动重启。</li>
<li><code>--ipc=host</code>: 使用宿主机 IPC 命名空间，对 NCCL 通信（多GPU协同）至关重要，能显著提高性能。</li>
</ul>
<h3 id="vllm-引擎核心参数"><!-- raw HTML omitted --><!-- raw HTML omitted -->vLLM 引擎核心参数</h3>
<ul>
<li><code>--model /model/Qwen3-32B-AWQ</code>: 指定容器内模型的路径。</li>
<li><code>--served-model-name coder</code>: API 服务时使用的模型名称。</li>
<li><code>--tensor-parallel-size 4</code>: 设置张量并行数为 4，即使用 4 块 GPU 协同推理。根据模型大小和 GPU 显存调整。</li>
<li><code>--dtype half</code>: AWQ 模型通常以半精度 (FP16) 加载权重以获得最佳性能和显存平衡。尽管 AWQ 内部可能使用更低精度，但 vLLM 加载时通常指定 <code>half</code> 或 <code>auto</code>。</li>
<li><code>--quantization awq</code>: 明确告知 vLLM 模型是 AWQ 量化类型。</li>
<li><code>--max-model-len 32768</code>: 设置模型能处理的最大序列长度，与 Qwen3-32B 的能力匹配。</li>
<li><code>--max-num-batched-tokens 4096</code>: 单个批次中处理的最大 token 数量。此值影响并发能力和显存占用，需根据实际负载调整。</li>
<li><code>--gpu-memory-utilization 0.93</code>: 设置 GPU 显存使用率。保留一部分（这里是 7%）是为了应对突发显存需求和避免 OOM。对于 AWQ 模型，由于 KV 缓存依然是 FP16，这部分显存占用不可忽视。</li>
<li><code>--block-size 32</code>: PagedAttention 中 KV 缓存块的大小。通常 16 或 32 是较优选择。</li>
<li><code>--enable-chunked-prefill</code>: 对于长序列（如 32K 上下文），启用分块预填充可以有效降低峰值显存，提高长序列处理的稳定性。</li>
<li><code>--swap-space 16</code>: 分配 16GB 的 CPU RAM 作为 GPU KV 缓存的交换空间。当 GPU 显存不足以容纳所有活跃请求的 KV 缓存时，vLLM 会将部分冷数据交换到 CPU RAM。</li>
<li><code>--tokenizer-pool-size 56</code>: 设置 Tokenizer 工作池的大小，建议与 CPU 核心数接近，以充分利用 CPU 并行处理能力进行文本编码解码。</li>
<li><code>--disable-custom-all-reduce</code>: 在某些多于 2 个纯 PCIe 连接的 GPU 配置中，vLLM 的自定义 all-reduce 内核可能存在兼容性或性能问题。禁用它可以回退到 NCCL 默认实现，通常更稳定。</li>
<li><code>--chat-template /app/qwen3_nonthinking.jinja</code>: 指定使用我们自定义的聊天模板文件。</li>
</ul>
<h2 id="-部署验证与测试"><!-- raw HTML omitted --><!-- raw HTML omitted -->🧪 部署验证与测试</h2>
<h3 id="检查-docker-日志"><!-- raw HTML omitted --><!-- raw HTML omitted -->检查 Docker 日志</h3>
<p>部署启动后，首先通过 <code>docker logs -f coder</code> 查看 vLLM 服务启动日志。关键信息包括：</p>
<ul>
<li>GPU 检测和显存分配情况。</li>
<li>模型分片加载情况。</li>
<li>PagedAttention KV 缓存块计算和可用数量。</li>
<li>API 服务启动成功，监听 <code>0.0.0.0:8000</code>。</li>
</ul>
<h3 id="使用-python-脚本验证-api"><!-- raw HTML omitted --><!-- raw HTML omitted -->使用 Python 脚本验证 API</h3>
<p>为了确保模型正常响应并且自定义聊天模板生效（不输出 <code>&lt;think&gt;</code> 标签），我们编写一个简单的 Python 脚本进行测试。</p>
<h4 id="环境准备使用-uv-管理依赖"><!-- raw HTML omitted --><!-- raw HTML omitted -->环境准备：使用 uv 管理依赖</h4>
<p>我们推荐使用 <code>uv</code> 这一新兴的快速 Python 包管理工具来创建虚拟环境和安装依赖。</p>
<ol>
<li><strong>创建虚拟环境</strong>: 在项目根目录运行 <code>uv venv</code>。这将创建一个名为 <code>.venv</code> 的虚拟环境。</li>
<li><strong>安装 <code>openai</code> 包</strong>: 运行 <code>uv pip install openai</code>。</li>
</ol>
<h4 id="验证脚本与预期输出"><!-- raw HTML omitted --><!-- raw HTML omitted -->验证脚本与预期输出</h4>
<p>在项目根目录创建 <code>verify_llm.py</code>：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> openai
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 根据实际vLLM服务器IP和端口配置</span>
</span></span><span style="display:flex;"><span>SERVER_IP <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;10.49.121.127&#34;</span> <span style="color:#75715e"># 或者 localhost</span>
</span></span><span style="display:flex;"><span>SERVER_PORT <span style="color:#f92672">=</span> <span style="color:#ae81ff">8000</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>client <span style="color:#f92672">=</span> openai<span style="color:#f92672">.</span>OpenAI(
</span></span><span style="display:flex;"><span>    base_url<span style="color:#f92672">=</span><span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;http://</span><span style="color:#e6db74">{</span>SERVER_IP<span style="color:#e6db74">}</span><span style="color:#e6db74">:</span><span style="color:#e6db74">{</span>SERVER_PORT<span style="color:#e6db74">}</span><span style="color:#e6db74">/v1&#34;</span>,
</span></span><span style="display:flex;"><span>    api_key<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;dummy-key&#34;</span>  <span style="color:#75715e"># vLLM 默认不需要 API key</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>messages <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;system&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;You are a helpful assistant.&#34;</span>},
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;你好，请介绍一下你自己。&#34;</span>}
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Sending request to the LLM...</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">try</span>:
</span></span><span style="display:flex;"><span>    completion <span style="color:#f92672">=</span> client<span style="color:#f92672">.</span>chat<span style="color:#f92672">.</span>completions<span style="color:#f92672">.</span>create(
</span></span><span style="display:flex;"><span>        model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;coder&#34;</span>,  <span style="color:#75715e"># 对应 --served-model-name</span>
</span></span><span style="display:flex;"><span>        messages<span style="color:#f92672">=</span>messages,
</span></span><span style="display:flex;"><span>        temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">0.7</span>,
</span></span><span style="display:flex;"><span>        max_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">150</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    response_content <span style="color:#f92672">=</span> completion<span style="color:#f92672">.</span>choices[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>message<span style="color:#f92672">.</span>content
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;LLM Response:&#34;</span>)
</span></span><span style="display:flex;"><span>    print(response_content)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#34;&lt;think&gt;&#34;</span> <span style="color:#f92672">in</span> response_content <span style="color:#f92672">or</span> <span style="color:#e6db74">&#34;&lt;/think&gt;&#34;</span> <span style="color:#f92672">in</span> response_content:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">VERIFICATION FAILED: &#39;&lt;think&gt;&#39; tags found in the response.&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">VERIFICATION SUCCESSFUL: No &#39;&lt;think&gt;&#39; tags found. &#39;enable_thinking=False&#39; is working as expected.&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">except</span> openai<span style="color:#f92672">.</span>APIConnectionError <span style="color:#66d9ef">as</span> e:
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Failed to connect to the server: </span><span style="color:#e6db74">{</span>e<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Please ensure the vLLM server is running and accessible at http://</span><span style="color:#e6db74">{</span>SERVER_IP<span style="color:#e6db74">}</span><span style="color:#e6db74">:</span><span style="color:#e6db74">{</span>SERVER_PORT<span style="color:#e6db74">}</span><span style="color:#e6db74">.&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">except</span> <span style="color:#a6e22e">Exception</span> <span style="color:#66d9ef">as</span> e:
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;An error occurred: </span><span style="color:#e6db74">{</span>e<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p>使用 <code>uv run python3 verify_llm.py</code> 运行此脚本。预期输出应包含模型的自我介绍，并且明确提示 <code>VERIFICATION SUCCESSFUL: No '&lt;think&gt;' tags found</code>。</p>
<h2 id="-项目源码"><!-- raw HTML omitted --><!-- raw HTML omitted -->🔗 项目源码</h2>
<p>本项目的所有配置文件、脚本和详细文档均已在 GitHub 开源：
<a href="https://github.com/jackypanster/deploy-qwen3-32b-awq">https://github.com/jackypanster/deploy-qwen3-32b-awq</a></p>
<h2 id="-总结"><!-- raw HTML omitted --><!-- raw HTML omitted -->🔚 总结</h2>
<p>通过本文的详细步骤和参数解析，我们成功地在多 GPU 环境下使用 vLLM 高效部署了 Qwen3-32B-AWQ 模型。关键的优化点包括针对 AWQ 模型的参数配置、32K 长上下文处理、以及通过自定义聊天模板实现“无思考模式”输出。这套部署方案兼顾了性能、资源利用率和特定业务需求，为基于 Qwen3 大模型的应用开发提供了坚实的基础。</p>
]]></content:encoded></item></channel></rss>