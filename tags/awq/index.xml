<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>AWQ on Code Whispers</title><link>https://jackypanster.github.io/ai-stream/tags/awq/</link><description>Recent content in AWQ on Code Whispers</description><generator>Hugo -- 0.148.2</generator><language>zh-cn</language><lastBuildDate>Wed, 16 Jul 2025 14:00:00 +0800</lastBuildDate><atom:link href="https://jackypanster.github.io/ai-stream/tags/awq/index.xml" rel="self" type="application/rss+xml"/><item><title>Qwen3-32B AWQ 在 4×RTX 2080 Ti 上的极限部署实战</title><link>https://jackypanster.github.io/ai-stream/posts/qwen3-32b-awq-2080ti-vllm-deploy/</link><pubDate>Wed, 16 Jul 2025 14:00:00 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/qwen3-32b-awq-2080ti-vllm-deploy/</guid><description>&lt;blockquote>
&lt;p>使用旧显卡也能跑 32B 大模型？本文手把手演示如何在 4×RTX 2080 Ti (共 88 GB 显存) 服务器上，通过 vLLM 0.8.5 + AWQ 量化，跑起 Qwen3-32B 并支持 200K+ tokens 超长上下文与高吞吐推理。全文记录了踩坑过程与参数权衡，希望给同样预算有限、硬件受限的工程师带来借鉴。&lt;/p>&lt;/blockquote>
&lt;h2 id="项目背景">项目背景&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>主角&lt;/strong>：Qwen3-32B-AWQ 量化模型 （≈ 18 GB）&lt;/li>
&lt;li>&lt;strong>目标&lt;/strong>：在消费级 Turing 架构显卡（2080 Ti）上最大化利用显存与吞吐。&lt;/li>
&lt;li>&lt;strong>框架&lt;/strong>：vLLM 0.8.5 (openai-compatible server)&lt;/li>
&lt;li>&lt;strong>取舍&lt;/strong>：牺牲部分延迟 / 稳定性 → 换取 吞吐 + 上下文长度&lt;/li>
&lt;/ul>
&lt;h2 id="硬件与系统环境">硬件与系统环境&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>组件&lt;/th>
&lt;th>规格&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>GPU&lt;/td>
&lt;td>4 × RTX 2080 Ti, 22 GB each, Compute Capability 7.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CPU&lt;/td>
&lt;td>≥ 56 cores (vLLM 线程可吃满)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RAM&lt;/td>
&lt;td>512 GB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Storage&lt;/td>
&lt;td>NVMe SSD 2 TB (模型 + KV 缓冲)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OS&lt;/td>
&lt;td>Ubuntu 24.04&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Driver&lt;/td>
&lt;td>NVIDIA 570.153.02&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CUDA&lt;/td>
&lt;td>12.8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="nvidia-smi-基线信息">NVIDIA-SMI 基线信息&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>nvidia-smi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Wed Jul &lt;span style="color:#ae81ff">16&lt;/span> 13:27:17 &lt;span style="color:#ae81ff">2025&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----------------------------------------------------------------------------------------+
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| NVIDIA-SMI 570.153.02 Driver Version: 570.153.02 CUDA Version: 12.8 |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----------------------------------------------------------------------------------------+
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>可以看到驱动与 CUDA 版本与上表一致，确认环境无偏差。&lt;/p></description></item><item><title>Qwen3-32B-AWQ vLLM 多卡 2080 Ti 极限部署实战</title><link>https://jackypanster.github.io/ai-stream/posts/qwen3-32b-2080ti-vllm-deploy/</link><pubDate>Wed, 16 Jul 2025 13:19:48 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/qwen3-32b-2080ti-vllm-deploy/</guid><description>&lt;blockquote>
&lt;p>使用旧显卡也能跑 32B 大模型？本文手把手演示如何在 &lt;strong>4×RTX 2080 Ti (共 88 GB 显存)&lt;/strong> 服务器上，通过 vLLM 0.8.5 + AWQ 量化，跑起 &lt;strong>Qwen3-32B&lt;/strong> 并支持 &lt;strong>32 K tokens&lt;/strong> 超长上下文与高吞吐推理。全文记录了踩坑过程与参数权衡，希望给同样预算有限、硬件受限的工程师带来借鉴。&lt;/p>&lt;/blockquote>
&lt;p>{{.TableOfContents}}&lt;/p>
&lt;h2 id="1-项目背景">1 项目背景&lt;/h2>
&lt;ul>
&lt;li>主角：&lt;code>Qwen3-32B-AWQ&lt;/code> 量化模型 （≈ 18 GB）&lt;/li>
&lt;li>目标：在消费级 &lt;strong>Turing&lt;/strong> 架构显卡（2080 Ti）上最大化利用显存与吞吐。&lt;/li>
&lt;li>框架：&lt;code>vLLM 0.8.5&lt;/code> (openai-compatible server)&lt;/li>
&lt;li>取舍：牺牲部分延迟 / 稳定性 → 换取 &lt;strong>吞吐 + 上下文长度&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="2-硬件与系统环境">2 硬件与系统环境&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>组件&lt;/th>
&lt;th>规格&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>GPU&lt;/td>
&lt;td>4 × RTX 2080 Ti, 22 GB &lt;em>each&lt;/em>, Compute Capability 7.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CPU&lt;/td>
&lt;td>≥ 56 cores (vLLM 线程可吃满)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RAM&lt;/td>
&lt;td>512 GB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Storage&lt;/td>
&lt;td>NVMe SSD 2 TB (模型 + KV 缓冲)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OS&lt;/td>
&lt;td>Ubuntu 24.04&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Driver&lt;/td>
&lt;td>NVIDIA 570.153.02&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CUDA&lt;/td>
&lt;td>12.8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="21-nvidia-smi-基线信息">2.1 NVIDIA-SMI 基线信息&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>nvidia-smi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Wed Jul &lt;span style="color:#ae81ff">16&lt;/span> 13:27:17 &lt;span style="color:#ae81ff">2025&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----------------------------------------------------------------------------------------+
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| NVIDIA-SMI 570.153.02 Driver Version: 570.153.02 CUDA Version: 12.8 |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+-----------------------------------------------------------------------------------------+
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>可以看到驱动与 CUDA 版本与上表一致，确认环境无偏差。&lt;/p></description></item><item><title>Qwen3-32B-AWQ 高效部署：基于 vLLM 的深度实践与优化</title><link>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-32b-awq-vllm-guide/</link><pubDate>Tue, 10 Jun 2025 20:45:00 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-32b-awq-vllm-guide/</guid><description>本文详细介绍了如何使用 vLLM 高效部署 Qwen3-32B-AWQ 量化模型，实现 32K 上下文窗口、OpenAI 兼容 API，并禁用思考模式。通过对 Docker 及 vLLM 参数的精细调优，最大化模型在多 GPU 环境下的推理性能。</description></item></channel></rss>