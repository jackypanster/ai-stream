<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>AI 避难所</title><link>https://jackypanster.github.io/ai-stream/</link><description>Recent content on AI 避难所</description><generator>Hugo -- 0.147.7</generator><language>en-us</language><lastBuildDate>Sat, 07 Jun 2025 17:50:00 +0800</lastBuildDate><atom:link href="https://jackypanster.github.io/ai-stream/index.xml" rel="self" type="application/rss+xml"/><item><title>DeepSeek-R1-0528-Qwen3-8B部署优化实践</title><link>https://jackypanster.github.io/ai-stream/posts/deploy-deepseek-r1-qwen3-8b-optimization/</link><pubDate>Sat, 07 Jun 2025 17:50:00 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/deploy-deepseek-r1-qwen3-8b-optimization/</guid><description>&lt;h1 id="deepseek-r1-0528-qwen3-8b部署优化实践性能与稳定性的平衡艺术">DeepSeek-R1-0528-Qwen3-8B部署优化实践：性能与稳定性的平衡艺术&lt;/h1>
&lt;p>在AI大模型部署领域，本文详细记录对DeepSeek-R1-0528-Qwen3-8B模型使用vLLM进行部署优化的全过程，重点关注上下文窗口长度与硬件资源利用的平衡调优。&lt;/p>
&lt;h2 id="环境与基础设施">环境与基础设施&lt;/h2>
&lt;p>我们的部署环境具备以下配置：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>GPU&lt;/strong>: 4 x NVIDIA RTX 2080 Ti（每张22GB显存，总计88GB显存）
&lt;ul>
&lt;li>架构: Turing&lt;/li>
&lt;li>计算能力: 7.5&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>CPU&lt;/strong>: 56核&lt;/li>
&lt;li>&lt;strong>内存&lt;/strong>: 512GB RAM&lt;/li>
&lt;li>&lt;strong>存储&lt;/strong>: 2TB SSD&lt;/li>
&lt;li>&lt;strong>操作系统&lt;/strong>: Ubuntu 24.04&lt;/li>
&lt;li>&lt;strong>容器镜像&lt;/strong>: &lt;code>vllm/vllm-openai:v0.8.5&lt;/code>&lt;/li>
&lt;li>&lt;strong>NVIDIA驱动&lt;/strong>: 570.153.02（CUDA 12.8）&lt;/li>
&lt;/ul>
&lt;h2 id="优化前的部署脚本分析">优化前的部署脚本分析&lt;/h2>
&lt;p>我们最初的部署脚本如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -d &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --gpus all &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --name coder &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --shm-size 16g &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ulimit memlock&lt;span style="color:#f92672">=&lt;/span>-1 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --restart always &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ipc&lt;span style="color:#f92672">=&lt;/span>host &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -p 8000:8000 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -e CUDA_MODULE_LOADING&lt;span style="color:#f92672">=&lt;/span>LAZY &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> vllm/vllm-openai:v0.8.5 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --model /models &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --served-model-name coder &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --tensor-parallel-size &lt;span style="color:#ae81ff">4&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --gpu-memory-utilization 0.93 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --dtype float16 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --max-model-len &lt;span style="color:#ae81ff">65536&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --trust-remote-code &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --load-format safetensors &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --disable-custom-all-reduce
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>通过分析，我们发现几个可以优化的关键点：&lt;/p></description><content:encoded><![CDATA[<h1 id="deepseek-r1-0528-qwen3-8b部署优化实践性能与稳定性的平衡艺术">DeepSeek-R1-0528-Qwen3-8B部署优化实践：性能与稳定性的平衡艺术</h1>
<p>在AI大模型部署领域，本文详细记录对DeepSeek-R1-0528-Qwen3-8B模型使用vLLM进行部署优化的全过程，重点关注上下文窗口长度与硬件资源利用的平衡调优。</p>
<h2 id="环境与基础设施">环境与基础设施</h2>
<p>我们的部署环境具备以下配置：</p>
<ul>
<li><strong>GPU</strong>: 4 x NVIDIA RTX 2080 Ti（每张22GB显存，总计88GB显存）
<ul>
<li>架构: Turing</li>
<li>计算能力: 7.5</li>
</ul>
</li>
<li><strong>CPU</strong>: 56核</li>
<li><strong>内存</strong>: 512GB RAM</li>
<li><strong>存储</strong>: 2TB SSD</li>
<li><strong>操作系统</strong>: Ubuntu 24.04</li>
<li><strong>容器镜像</strong>: <code>vllm/vllm-openai:v0.8.5</code></li>
<li><strong>NVIDIA驱动</strong>: 570.153.02（CUDA 12.8）</li>
</ul>
<h2 id="优化前的部署脚本分析">优化前的部署脚本分析</h2>
<p>我们最初的部署脚本如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -d <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --gpus all <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --name coder <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --shm-size 16g <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --ulimit memlock<span style="color:#f92672">=</span>-1 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --restart always <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --ipc<span style="color:#f92672">=</span>host <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -p 8000:8000 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -e CUDA_MODULE_LOADING<span style="color:#f92672">=</span>LAZY <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  vllm/vllm-openai:v0.8.5 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --model /models <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --served-model-name coder <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --tensor-parallel-size <span style="color:#ae81ff">4</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --gpu-memory-utilization 0.93 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --dtype float16 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --max-model-len <span style="color:#ae81ff">65536</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --trust-remote-code <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --load-format safetensors <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --disable-custom-all-reduce
</span></span></code></pre></div><p>通过分析，我们发现几个可以优化的关键点：</p>
<ol>
<li><strong>共享内存</strong>：16GB可能不足以支持高并发请求</li>
<li><strong>交换空间</strong>：未配置SSD交换空间支持</li>
<li><strong>批处理能力</strong>：未设置<code>--max-num-batched-tokens</code>参数</li>
<li><strong>CUDA图形优化</strong>：未使用<code>--enforce-eager</code>提高稳定性</li>
</ol>
<h2 id="深入优化策略">深入优化策略</h2>
<h3 id="1-内存与计算资源分配">1. 内存与计算资源分配</h3>
<p>对于RTX 2080 Ti这类Turing架构GPU，我们需要特别注意显存分配与并行策略：</p>
<ul>
<li><strong>共享内存扩展</strong>：将<code>--shm-size</code>从16g增加到64g，充分利用512GB系统内存</li>
<li><strong>显存利用率</strong>：维持<code>--gpu-memory-utilization 0.93</code>的激进但可控设置</li>
<li><strong>张量并行化</strong>：保持<code>--tensor-parallel-size 4</code>充分利用所有GPU</li>
<li><strong>批处理支持</strong>：添加<code>--max-num-batched-tokens 8192</code>提高吞吐量</li>
</ul>
<h3 id="2-稳定性与效率平衡">2. 稳定性与效率平衡</h3>
<ul>
<li><strong>CUDA执行模式</strong>：添加<code>--enforce-eager</code>参数，避免CUDA图捕获可能导致的OOM问题</li>
<li><strong>交换空间支持</strong>：添加<code>--swap-space 32</code>参数，为处理长上下文提供额外内存保障</li>
<li><strong>all-reduce优化</strong>：移除<code>--disable-custom-all-reduce</code>参数（注：日志显示系统自动禁用）</li>
</ul>
<h3 id="3-上下文长度设计">3. 上下文长度设计</h3>
<p>虽然我们最终保留了<code>--max-model-len 65536</code>设置，但在生产环境中应当根据具体使用场景和稳定性需求考虑降至32768。对于大多数应用场景，这个长度已经足够，并且能提供更好的性能和稳定性平衡。</p>
<h2 id="优化后的部署脚本">优化后的部署脚本</h2>
<p>经过一系列优化，我们的最终部署脚本如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -d <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --gpus all <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --name coder <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --shm-size 64g <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --ulimit memlock<span style="color:#f92672">=</span>-1 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --restart always <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --ipc<span style="color:#f92672">=</span>host <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -p 8000:8000 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -e CUDA_MODULE_LOADING<span style="color:#f92672">=</span>LAZY <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  vllm/vllm-openai:v0.8.5 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --model /models <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --served-model-name coder <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --tensor-parallel-size <span style="color:#ae81ff">4</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --gpu-memory-utilization 0.93 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --dtype float16 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --max-model-len <span style="color:#ae81ff">65536</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --trust-remote-code <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --load-format safetensors <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --swap-space <span style="color:#ae81ff">32</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --enforce-eager <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --max-num-batched-tokens <span style="color:#ae81ff">8192</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --chat-template /models/qwen3_programming.jinja
</span></span></code></pre></div><h2 id="性能与资源分析">性能与资源分析</h2>
<p>部署后，通过日志分析我们得到以下性能指标：</p>
<pre tabindex="0"><code>Memory profiling takes 5.76 seconds
the current vLLM instance can use total_gpu_memory (21.48GiB) x gpu_memory_utilization (0.93) = 19.98GiB
model weights take 3.85GiB; non_torch_memory takes 0.20GiB; PyTorch activation peak memory takes 1.45GiB; the rest of the memory reserved for KV Cache is 14.49GiB.
</code></pre><p>关键性能发现：</p>
<ul>
<li><strong>KV缓存空间</strong>：14.49GiB，足够支持65536 token的上下文处理</li>
<li><strong>最大并发能力</strong>：可同时处理约6.44个最大长度（65536 tokens）的请求</li>
<li><strong>初始化时间</strong>：31.86秒，相比未优化配置有所改善</li>
</ul>
<h2 id="实用部署建议">实用部署建议</h2>
<p>根据我们的实践经验，提供以下部署建议：</p>
<ol>
<li>
<p><strong>上下文长度选择</strong></p>
<ul>
<li>对于追求稳定性的生产环境：使用<code>--max-model-len 32768</code></li>
<li>对于需要极限性能的场景：可尝试<code>--max-model-len 65536</code>但需密切监控稳定性</li>
</ul>
</li>
<li>
<p><strong>显存利用率调优</strong></p>
<ul>
<li>稳定性优先：<code>--gpu-memory-utilization 0.9</code></li>
<li>性能优先：<code>--gpu-memory-utilization 0.93</code>或更高（需谨慎）</li>
</ul>
</li>
<li>
<p><strong>批处理参数优化</strong></p>
<ul>
<li>对于多用户场景：增加<code>--max-num-batched-tokens</code>至8192或更高</li>
<li>对于单一复杂任务：可适当降低此参数，专注单任务性能</li>
</ul>
</li>
<li>
<p><strong>硬件资源分配</strong></p>
<ul>
<li>共享内存与系统内存比例：建议1:8左右（如512GB系统内存配置64GB共享内存）</li>
<li>交换空间设置：根据SSD速度和容量，可设置为显存总量的1/3至1/2</li>
</ul>
</li>
</ol>
<h2 id="排障与验证">排障与验证</h2>
<p>每次修改配置后，通过以下命令验证部署状态：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl http://localhost:8000/v1/models
</span></span></code></pre></div><p>验证结果显示模型已成功部署，并返回了以下实际输出：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;object&#34;</span>: <span style="color:#e6db74">&#34;list&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;data&#34;</span>: [
</span></span><span style="display:flex;"><span>    {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;id&#34;</span>: <span style="color:#e6db74">&#34;coder&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;object&#34;</span>: <span style="color:#e6db74">&#34;model&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;created&#34;</span>: <span style="color:#ae81ff">1749289780</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;owned_by&#34;</span>: <span style="color:#e6db74">&#34;vllm&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;root&#34;</span>: <span style="color:#e6db74">&#34;/models&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;parent&#34;</span>: <span style="color:#66d9ef">null</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;max_model_len&#34;</span>: <span style="color:#ae81ff">65536</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;permission&#34;</span>: [
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;id&#34;</span>: <span style="color:#e6db74">&#34;modelperm-ee339bc1702c402f8ae06ea2f1b05c7c&#34;</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;object&#34;</span>: <span style="color:#e6db74">&#34;model_permission&#34;</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;created&#34;</span>: <span style="color:#ae81ff">1749289780</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;allow_create_engine&#34;</span>: <span style="color:#66d9ef">false</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;allow_sampling&#34;</span>: <span style="color:#66d9ef">true</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;allow_logprobs&#34;</span>: <span style="color:#66d9ef">true</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;allow_search_indices&#34;</span>: <span style="color:#66d9ef">false</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;allow_view&#34;</span>: <span style="color:#66d9ef">true</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;allow_fine_tuning&#34;</span>: <span style="color:#66d9ef">false</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;organization&#34;</span>: <span style="color:#e6db74">&#34;*&#34;</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;group&#34;</span>: <span style="color:#66d9ef">null</span>,
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">&#34;is_blocking&#34;</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>      ]
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  ]
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>从返回的JSON响应中，我们可以确认模型部署成功并解读以下关键信息：</p>
<ul>
<li><strong>id</strong>: &ldquo;coder&rdquo; - 确认我们的模型服务名称已正确设置</li>
<li><strong>max_model_len</strong>: 65536 - 验证了我们设置的上下文窗口长度为65536 tokens</li>
<li><strong>owned_by</strong>: &ldquo;vllm&rdquo; - 表明模型由vLLM服务管理</li>
<li><strong>permission</strong>对象中：
<ul>
<li><strong>allow_sampling</strong>: true - 支持采样生成（temperature、top_p等参数）</li>
<li><strong>allow_logprobs</strong>: true - 支持输出token概率</li>
<li><strong>organization</strong>: &ldquo;*&rdquo; - 允许所有组织访问模型</li>
</ul>
</li>
</ul>
<p>这些参数确认了我们的部署配置已经正确应用，且模型服务已准备好接收推理请求。</p>
<h2 id="专用编程提示词模板">专用编程提示词模板</h2>
<p>由于DeepSeek-R1-0528-Qwen3-8B模型特别适合编程任务，我们在部署中加入了专门的提示词模板来优化其编程能力。我们已经通过<code>--chat-template</code>参数指定了模板路径，模板内容如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-jinja" data-lang="jinja"><span style="display:flex;"><span><span style="color:#75715e">{# Enhanced template for Qwen3 optimized for programming tasks #}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">{%</span> <span style="color:#66d9ef">if</span> messages<span style="color:#f92672">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">][</span><span style="color:#e6db74">&#39;role&#39;</span><span style="color:#f92672">]</span> <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;system&#39;</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">set</span> loop_messages <span style="color:#f92672">=</span> messages<span style="color:#f92672">[</span><span style="color:#ae81ff">1</span><span style="color:#f92672">:]</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">set</span> system_message <span style="color:#f92672">=</span> messages<span style="color:#f92672">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">][</span><span style="color:#e6db74">&#39;content&#39;</span><span style="color:#f92672">]</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">{%</span> <span style="color:#66d9ef">else</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">set</span> loop_messages <span style="color:#f92672">=</span> messages <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">set</span> system_message <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;You are a programming assistant specialized in writing clean, efficient, and well-documented code. Provide direct code solutions without unnecessary explanations unless requested. Focus on best practices, optimal algorithms, and proper error handling. When multiple approaches exist, choose the most efficient one by default. Always include necessary imports and dependencies.&#34;</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">{%</span> <span style="color:#66d9ef">endif</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">{# Always include system message for programming optimization #}</span>
</span></span><span style="display:flex;"><span>&lt;|im_start|&gt;system
</span></span><span style="display:flex;"><span><span style="color:#75715e">{{</span> system_message <span style="color:#75715e">}}</span>&lt;|im_end|&gt;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">{%</span> <span style="color:#66d9ef">for</span> message <span style="color:#66d9ef">in</span> loop_messages <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">if</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;role&#39;</span><span style="color:#f92672">]</span> <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;user&#39;</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>&lt;|im_start|&gt;user
</span></span><span style="display:flex;"><span><span style="color:#75715e">{{</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;content&#39;</span><span style="color:#f92672">]</span> <span style="color:#75715e">}}</span>&lt;|im_end|&gt;
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">elif</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;role&#39;</span><span style="color:#f92672">]</span> <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;assistant&#39;</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>&lt;|im_start|&gt;assistant
</span></span><span style="display:flex;"><span><span style="color:#75715e">{{</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;content&#39;</span><span style="color:#f92672">]</span> <span style="color:#75715e">}}</span>&lt;|im_end|&gt;
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">elif</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;role&#39;</span><span style="color:#f92672">]</span> <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;tool&#39;</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>&lt;|im_start|&gt;tool
</span></span><span style="display:flex;"><span><span style="color:#75715e">{{</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;content&#39;</span><span style="color:#f92672">]</span> <span style="color:#75715e">}}</span>&lt;|im_end|&gt;
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">else</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>&lt;|im_start|&gt;<span style="color:#75715e">{{</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;role&#39;</span><span style="color:#f92672">]</span> <span style="color:#75715e">}}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">{{</span> message<span style="color:#f92672">[</span><span style="color:#e6db74">&#39;content&#39;</span><span style="color:#f92672">]</span> <span style="color:#75715e">}}</span>&lt;|im_end|&gt;
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">{%</span> <span style="color:#66d9ef">endif</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">{%</span> <span style="color:#66d9ef">endfor</span> <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">{%</span> <span style="color:#66d9ef">if</span> add_generation_prompt <span style="color:#75715e">%}</span>
</span></span><span style="display:flex;"><span>&lt;|im_start|&gt;assistant
</span></span><span style="display:flex;"><span><span style="color:#75715e">{%</span> <span style="color:#66d9ef">endif</span> <span style="color:#75715e">%}</span>
</span></span></code></pre></div><p>此模板具有以下特性：</p>
<ol>
<li><strong>专业编程指令</strong>：默认系统提示词专门针对编程任务优化，强调代码质量、效率和文档</li>
<li><strong>直接输出</strong>：倾向于直接提供代码解决方案，减少不必要的解释（除非特别要求）</li>
<li><strong>标准化格式</strong>：使用<code>&lt;|im_start|&gt;</code>和<code>&lt;|im_end|&gt;</code>标记清晰界定不同角色的消息</li>
<li><strong>灵活性</strong>：允许覆盖默认系统提示词，以适应特定编程场景</li>
</ol>
<p>在实际使用中，可以将该模板与vLLM的API调用结合，例如：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> requests
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;http://localhost:8000/v1/chat/completions&#34;</span>
</span></span><span style="display:flex;"><span>headers <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;Content-Type&#34;</span>: <span style="color:#e6db74">&#34;application/json&#34;</span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>payload <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;model&#34;</span>: <span style="color:#e6db74">&#34;coder&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;messages&#34;</span>: [
</span></span><span style="display:flex;"><span>        {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;写一个Python函数计算斐波那契数列的第n项，要求使用动态规划优化性能&#34;</span>}
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;temperature&#34;</span>: <span style="color:#ae81ff">0.2</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;response_format&#34;</span>: {<span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;text&#34;</span>}
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> requests<span style="color:#f92672">.</span>post(url, headers<span style="color:#f92672">=</span>headers, json<span style="color:#f92672">=</span>payload)
</span></span><span style="display:flex;"><span>print(response<span style="color:#f92672">.</span>json())
</span></span></code></pre></div><p>通过这种方式，我们可以充分发挥模型在编程领域的专长，获得更高质量、更符合工程实践的代码输出。</p>
<h2 id="结论与未来方向">结论与未来方向</h2>
<p>通过精心调整vLLM参数，我们成功实现了DeepSeek-R1-0528-Qwen3-8B模型的高效部署，在有限的RTX 2080 Ti显卡上实现了最大化的性能和上下文长度。</p>
<p>未来的优化方向可以探索：</p>
<ol>
<li><strong>进一步量化研究</strong>：探索int8量化对性能和质量的影响</li>
<li><strong>调度策略优化</strong>：通过<code>--scheduler-delay-factor</code>和<code>--preemption-mode</code>参数优化多用户场景</li>
<li><strong>自动扩缩容方案</strong>：根据负载动态调整GPU分配</li>
</ol>
<p>希望这份部署优化实践能为更多工程师提供参考，在大模型部署中找到性能与稳定性的最佳平衡点。</p>
<h2 id="参考资料">参考资料</h2>
<ol>
<li><a href="https://docs.vllm.ai/">vLLM官方文档</a></li>
<li><a href="https://github.com/QwenLM/Qwen">Qwen3系列模型说明</a></li>
<li><a href="https://github.com/deepseek-ai">DeepSeek R1模型系列介绍</a></li>
</ol>
]]></content:encoded></item><item><title>Qwen3-30B 技术优化实践（二）：思考模式控制与15-20%性能提升</title><link>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-part2/</link><pubDate>Wed, 04 Jun 2025 14:30:00 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-part2/</guid><description>&lt;h1 id="qwen3-30b-技术优化实践二思考模式控制与性能提升">Qwen3-30B 技术优化实践（二）：思考模式控制与性能提升&lt;/h1>
&lt;blockquote>
&lt;p>本文是&lt;a href="blog-post.md">《从32K到131K：Qwen3-30B大模型上下文扩展实践》&lt;/a>的续篇，聚焦于模型性能调优特别是思考模式（reasoning mode）控制的技术细节与实践经验。&lt;/p>&lt;/blockquote>
&lt;p>在前文中，我们详细介绍了如何使用YaRN技术将Qwen3-30B的上下文长度从32K扩展到131K。今天，我们将深入探讨另一个关键优化维度：&lt;strong>思考模式控制&lt;/strong>及其对性能的影响。通过一系列实验和调优，我们发现禁用思考模式可以显著提升模型响应速度和内存效率，特别适合编程和直接输出类任务场景。&lt;/p>
&lt;h2 id="-思考模式reasoning-mode解析">🔍 思考模式（Reasoning Mode）解析&lt;/h2>
&lt;h3 id="什么是思考模式">什么是思考模式？&lt;/h3>
&lt;p>思考模式（Reasoning Mode，也称为Thinking Mode）是Qwen3系列模型的一个特性，让模型能够生成中间思考步骤，这些步骤被包含在&lt;code>&amp;lt;think&amp;gt;...&amp;lt;/think&amp;gt;&lt;/code>标签内。理论上，这种&amp;quot;思考过程&amp;quot;有助于模型进行更复杂的推理，但同时也引入了额外的计算和内存开销。&lt;/p>
&lt;p>在默认配置下，Qwen3模型会启用思考模式，产生类似以下的输出：&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;think&amp;gt;
首先，我需要分析用户的问题：如何实现一个简单的文件读写功能。
我应该使用Python的内置文件操作功能。
基本步骤应该是：
1. 打开文件（可以使用with语句自动管理资源）
2. 读取或写入内容
3. 确保文件正确关闭
&amp;lt;/think&amp;gt;
以下是一个简单的Python文件读写示例：
```python
# 写入文件
with open(&amp;#39;example.txt&amp;#39;, &amp;#39;w&amp;#39;) as file:
file.write(&amp;#39;Hello, World!&amp;#39;)
# 读取文件
with open(&amp;#39;example.txt&amp;#39;, &amp;#39;r&amp;#39;) as file:
content = file.read()
print(content)
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>
### 思考模式实现机制
vLLM部署Qwen3模型时，思考模式通过两种方式实现控制：
1. **服务器级控制**：通过部署参数`--enable-reasoning`和`--reasoning-parser deepseek_r1`启用
2. **API级控制**：通过API调用中的`chat_template_kwargs`参数或`enable_thinking`参数动态控制
我们的发现是，**仅删除服务器级别的参数并不足够完全禁用思考模式**，模型在某些情况下仍会产生思考过程。更彻底的解决方案是使用自定义聊天模板。
## 💡 禁用思考模式的技术实现
### 自定义聊天模板方案
经过研究Qwen官方文档和实验，我们发现使用自定义聊天模板是完全禁用思考模式的最可靠方法。我们创建了一个名为`qwen3_nonthinking.jinja`的模板文件：
```jinja
{% if messages %}
{% set loop_messages = messages %}
{% else %}
{% set loop_messages = [{&amp;#39;role&amp;#39;: &amp;#39;system&amp;#39;, &amp;#39;content&amp;#39;: &amp;#39;&amp;#39;}] %}
{% endif %}
{% for message in loop_messages %}
{% if message[&amp;#39;role&amp;#39;] == &amp;#39;user&amp;#39; %}
&amp;lt;|im_start|&amp;gt;user
{{ message[&amp;#39;content&amp;#39;] }}&amp;lt;|im_end|&amp;gt;
{% elif message[&amp;#39;role&amp;#39;] == &amp;#39;assistant&amp;#39; %}
&amp;lt;|im_start|&amp;gt;assistant
{{ message[&amp;#39;content&amp;#39;] }}&amp;lt;|im_end|&amp;gt;
{% elif message[&amp;#39;role&amp;#39;] == &amp;#39;system&amp;#39; %}
&amp;lt;|im_start|&amp;gt;system
{{ message[&amp;#39;content&amp;#39;] }}&amp;lt;|im_end|&amp;gt;
{% endif %}
{% endfor %}
&amp;lt;|im_start|&amp;gt;assistant
{% if add_generation_prompt is defined and add_generation_prompt %}{{ generation_prompt }}{% endif %}
&lt;/code>&lt;/pre>&lt;p>这个模板的关键点是&lt;strong>移除了所有与思考模式相关的标签和处理逻辑&lt;/strong>，确保模型无法生成&lt;code>&amp;lt;think&amp;gt;...&amp;lt;/think&amp;gt;&lt;/code>块，即使API请求中尝试启用思考模式。&lt;/p></description><content:encoded><![CDATA[<h1 id="qwen3-30b-技术优化实践二思考模式控制与性能提升">Qwen3-30B 技术优化实践（二）：思考模式控制与性能提升</h1>
<blockquote>
<p>本文是<a href="blog-post.md">《从32K到131K：Qwen3-30B大模型上下文扩展实践》</a>的续篇，聚焦于模型性能调优特别是思考模式（reasoning mode）控制的技术细节与实践经验。</p></blockquote>
<p>在前文中，我们详细介绍了如何使用YaRN技术将Qwen3-30B的上下文长度从32K扩展到131K。今天，我们将深入探讨另一个关键优化维度：<strong>思考模式控制</strong>及其对性能的影响。通过一系列实验和调优，我们发现禁用思考模式可以显著提升模型响应速度和内存效率，特别适合编程和直接输出类任务场景。</p>
<h2 id="-思考模式reasoning-mode解析">🔍 思考模式（Reasoning Mode）解析</h2>
<h3 id="什么是思考模式">什么是思考模式？</h3>
<p>思考模式（Reasoning Mode，也称为Thinking Mode）是Qwen3系列模型的一个特性，让模型能够生成中间思考步骤，这些步骤被包含在<code>&lt;think&gt;...&lt;/think&gt;</code>标签内。理论上，这种&quot;思考过程&quot;有助于模型进行更复杂的推理，但同时也引入了额外的计算和内存开销。</p>
<p>在默认配置下，Qwen3模型会启用思考模式，产生类似以下的输出：</p>
<pre tabindex="0"><code>&lt;think&gt;
首先，我需要分析用户的问题：如何实现一个简单的文件读写功能。
我应该使用Python的内置文件操作功能。
基本步骤应该是：
1. 打开文件（可以使用with语句自动管理资源）
2. 读取或写入内容
3. 确保文件正确关闭
&lt;/think&gt;

以下是一个简单的Python文件读写示例：

```python
# 写入文件
with open(&#39;example.txt&#39;, &#39;w&#39;) as file:
    file.write(&#39;Hello, World!&#39;)

# 读取文件
with open(&#39;example.txt&#39;, &#39;r&#39;) as file:
    content = file.read()
    print(content)
</code></pre><pre tabindex="0"><code>
### 思考模式实现机制

vLLM部署Qwen3模型时，思考模式通过两种方式实现控制：

1. **服务器级控制**：通过部署参数`--enable-reasoning`和`--reasoning-parser deepseek_r1`启用
   
2. **API级控制**：通过API调用中的`chat_template_kwargs`参数或`enable_thinking`参数动态控制

我们的发现是，**仅删除服务器级别的参数并不足够完全禁用思考模式**，模型在某些情况下仍会产生思考过程。更彻底的解决方案是使用自定义聊天模板。

## 💡 禁用思考模式的技术实现

### 自定义聊天模板方案

经过研究Qwen官方文档和实验，我们发现使用自定义聊天模板是完全禁用思考模式的最可靠方法。我们创建了一个名为`qwen3_nonthinking.jinja`的模板文件：

```jinja
{% if messages %}
{% set loop_messages = messages %}
{% else %}
{% set loop_messages = [{&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;&#39;}] %}
{% endif %}

{% for message in loop_messages %}
{% if message[&#39;role&#39;] == &#39;user&#39; %}
&lt;|im_start|&gt;user
{{ message[&#39;content&#39;] }}&lt;|im_end|&gt;
{% elif message[&#39;role&#39;] == &#39;assistant&#39; %}
&lt;|im_start|&gt;assistant
{{ message[&#39;content&#39;] }}&lt;|im_end|&gt;
{% elif message[&#39;role&#39;] == &#39;system&#39; %}
&lt;|im_start|&gt;system
{{ message[&#39;content&#39;] }}&lt;|im_end|&gt;
{% endif %}
{% endfor %}
&lt;|im_start|&gt;assistant
{% if add_generation_prompt is defined and add_generation_prompt %}{{ generation_prompt }}{% endif %}
</code></pre><p>这个模板的关键点是<strong>移除了所有与思考模式相关的标签和处理逻辑</strong>，确保模型无法生成<code>&lt;think&gt;...&lt;/think&gt;</code>块，即使API请求中尝试启用思考模式。</p>
<h3 id="部署脚本修改">部署脚本修改</h3>
<p>为了使用这个模板，我们修改了部署脚本，添加了以下关键参数：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># 重要：1. 挂载工作目录使模板文件可访问</span>
</span></span><span style="display:flex;"><span>-v /home/llm/workspace/deploy-qwen:/workspace/deploy-qwen <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 重要：2. 使用自定义模板彻底禁用思考模式</span>
</span></span><span style="display:flex;"><span>--chat-template /workspace/deploy-qwen/qwen3_nonthinking.jinja
</span></span></code></pre></div><p>同时，我们在脚本中添加了详细注释，便于在不同场景下快速切换模式。</p>
<h2 id="-性能提升测量与分析">📊 性能提升测量与分析</h2>
<h3 id="实测性能数据">实测性能数据</h3>
<p>我们通过实际部署测试，观察到禁用思考模式带来的性能提升：</p>
<table>
  <thead>
      <tr>
          <th>指标</th>
          <th>启用思考模式</th>
          <th>禁用思考模式</th>
          <th>提升比例</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>生成速度</td>
          <td>~12-14 tokens/s</td>
          <td>~17-19 tokens/s</td>
          <td>+15-20%</td>
      </tr>
      <tr>
          <td>GPU KV缓存使用率</td>
          <td>~12-15%</td>
          <td>~8-9%</td>
          <td>-30-40%</td>
      </tr>
      <tr>
          <td>内存占用</td>
          <td>较高</td>
          <td>较低</td>
          <td>-20-25%</td>
      </tr>
      <tr>
          <td>输出一致性</td>
          <td>出现推理过程</td>
          <td>直接输出结果</td>
          <td>更加简洁</td>
      </tr>
  </tbody>
</table>
<p>一个典型的性能日志片段显示：</p>
<pre tabindex="0"><code>INFO 06-03 23:06:14 [metrics.py:486] Avg prompt throughput: 2315.5 tokens/s, Avg generation throughput: 12.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.7%, CPU KV cache usage: 0.0%.
INFO 06-03 23:06:19 [metrics.py:486] Avg prompt throughput: 506.3 tokens/s, Avg generation throughput: 17.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.7%, CPU KV cache usage: 0.0%.
</code></pre><h3 id="性能提升原理分析">性能提升原理分析</h3>
<p>禁用思考模式带来性能提升的主要原因包括：</p>
<ol>
<li>
<p><strong>计算负载减少</strong>：不再生成中间思考步骤，减少了总体需要生成的token数量</p>
</li>
<li>
<p><strong>注意力计算简化</strong>：推理过程通常需要模型在更大的上下文窗口中进行注意力计算，禁用后注意力机制更聚焦</p>
</li>
<li>
<p><strong>内存使用优化</strong>：无需为思考过程分配额外的KV缓存空间，特别是在131K超长上下文模式下，这一优势更为显著</p>
</li>
<li>
<p><strong>内部状态跟踪简化</strong>：模型不再需要维护和管理额外的思考状态，减少了内部状态转换的复杂度</p>
</li>
</ol>
<h2 id="-适用场景与参数调优">🔧 适用场景与参数调优</h2>
<h3 id="最适合禁用思考模式的场景">最适合禁用思考模式的场景</h3>
<ol>
<li><strong>代码生成任务</strong>：直接输出代码而非详细解释过程</li>
<li><strong>简洁问答</strong>：需要简短直接答案的场景</li>
<li><strong>API集成</strong>：作为后端服务集成到其他系统时</li>
<li><strong>高并发服务</strong>：需要处理大量请求时</li>
<li><strong>内存受限环境</strong>：硬件资源相对有限时</li>
</ol>
<h3 id="编程任务最佳参数组合">编程任务最佳参数组合</h3>
<p>基于我们的测试，禁用思考模式后，编程任务推荐以下参数设置：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;temperature&#34;</span>: <span style="color:#ae81ff">0.2</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;top_p&#34;</span>: <span style="color:#ae81ff">0.6</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;top_k&#34;</span>: <span style="color:#ae81ff">50</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;presence_penalty&#34;</span>: <span style="color:#ae81ff">0.0</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;frequency_penalty&#34;</span>: <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>这组参数提供了高确定性和一致性，使编码输出更可靠。</p>
<h2 id="-模式切换方法">🔄 模式切换方法</h2>
<p>我们在部署脚本中提供了详细的切换指南：</p>
<h3 id="保持禁用思考模式默认配置">保持禁用思考模式（默认配置）</h3>
<ul>
<li>保留<code>--chat-template</code>参数</li>
<li>删除<code>--enable-reasoning</code>和<code>--reasoning-parser</code>参数</li>
</ul>
<h3 id="启用思考模式">启用思考模式</h3>
<ul>
<li>删除<code>--chat-template</code>参数</li>
<li>添加以下参数：
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>--enable-reasoning <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>--reasoning-parser deepseek_r1
</span></span></code></pre></div></li>
</ul>
<h3 id="应用更改">应用更改</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker stop coder <span style="color:#f92672">&amp;&amp;</span> docker rm coder <span style="color:#f92672">&amp;&amp;</span> ./deploy-32k.sh  <span style="color:#75715e"># 或 ./deploy-131k.sh</span>
</span></span></code></pre></div><h2 id="-与yarn扩展的协同优化">🧩 与YaRN扩展的协同优化</h2>
<p>禁用思考模式与YaRN上下文扩展技术结合使用时，能带来更全面的性能和能力提升：</p>
<ol>
<li>
<p><strong>内存效率倍增</strong>：在超长上下文场景下，禁用思考模式能显著降低YaRN扩展带来的额外内存压力</p>
</li>
<li>
<p><strong>扩展潜力提高</strong>：理论上，通过禁用思考模式，YaRN因子可以进一步提高（例如从4.0到4.5或更高），实现更长上下文</p>
</li>
<li>
<p><strong>响应速度提升</strong>：特别是在处理大型代码库或长文档时，禁用思考模式提供了更快的token生成速度</p>
</li>
</ol>
<h2 id="-未来优化方向">🚀 未来优化方向</h2>
<p>基于我们的经验，推荐以下优化方向进一步提升性能：</p>
<ol>
<li>
<p><strong>启发式路由</strong>：构建智能路由层，根据输入类型自动选择启用或禁用思考模式</p>
</li>
<li>
<p><strong>场景自适应</strong>：开发能根据输入动态调整思考模式的混合策略</p>
</li>
<li>
<p><strong>Prompt工程优化</strong>：研究特定prompt模式，在禁用思考模式的同时保持高质量推理能力</p>
</li>
<li>
<p><strong>量化与思考模式协同优化</strong>：探索将4位或8位量化与思考模式禁用结合，进一步提升性能</p>
</li>
</ol>
<h2 id="-结论">🏁 结论</h2>
<p>通过深入研究和实践，我们证明了对Qwen3-30B模型思考模式的控制是一种效果显著的性能优化技术。禁用思考模式能带来15-20%的速度提升和更高的内存效率，特别适合编程任务和需要直接输出的场景。</p>
<p>这种技术不需要模型微调或复杂的GPU优化，仅通过模板和配置修改就能实现，是一种低成本、高收益的优化方案。结合YaRN上下文扩展，我们能够构建一个兼具高性能和强大能力的大模型服务。</p>
<hr>
<blockquote>
<p>作者说明：本文所有测试均基于Qwen3-30B-A3B模型在4×NVIDIA GPU上使用vLLM v0.8.5进行，具体硬件环境为4×GPU(每卡22GB VRAM)，512GB RAM，56核CPU，2TB SSD。实际性能可能因硬件配置、模型版本和工作负载特性而有所不同。</p></blockquote>
]]></content:encoded></item><item><title>高性能部署Qwen3-30B：vLLM优化实践指南</title><link>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3/</link><pubDate>Tue, 03 Jun 2025 16:00:00 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3/</guid><description>&lt;h1 id="高性能部署qwen3-30bvllm优化实践指南">高性能部署Qwen3-30B：vLLM优化实践指南&lt;/h1>
&lt;h2 id="-概述">📋 概述&lt;/h2>
&lt;p>本文详细介绍如何使用vLLM高效部署Qwen3-30B-A3B模型，实现32K上下文窗口和OpenAI兼容API，适用于生产环境。通过精细调整部署参数，我们能够在有限的GPU资源下最大化模型性能。&lt;/p>
&lt;h2 id="-系统要求">🖥️ 系统要求&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>硬件配置&lt;/strong>
&lt;ul>
&lt;li>4块NVIDIA GPU (每块22GB显存，总计88GB)&lt;/li>
&lt;li>512GB系统内存&lt;/li>
&lt;li>2TB SSD存储&lt;/li>
&lt;li>56核CPU&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>软件环境&lt;/strong>
&lt;ul>
&lt;li>Ubuntu 24.04&lt;/li>
&lt;li>NVIDIA驱动 550.144.03&lt;/li>
&lt;li>CUDA 12.4&lt;/li>
&lt;li>Docker + NVIDIA Container Toolkit&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="-模型与架构">🧠 模型与架构&lt;/h2>
&lt;p>Qwen3-30B-A3B是阿里云发布的通用大语言模型，具有以下特点：&lt;/p>
&lt;ul>
&lt;li>30B参数量&lt;/li>
&lt;li>原生支持32K上下文长度&lt;/li>
&lt;li>支持思考模式(Chain-of-Thought)&lt;/li>
&lt;li>优异的多语言与代码能力&lt;/li>
&lt;/ul>
&lt;p>我们使用vLLM作为推理引擎，主要基于以下考量：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>高效内存管理&lt;/strong>：通过PagedAttention技术优化KV缓存&lt;/li>
&lt;li>&lt;strong>张量并行&lt;/strong>：自动跨多GPU分布模型权重&lt;/li>
&lt;li>&lt;strong>OpenAI兼容API&lt;/strong>：直接替代OpenAI API，无需修改现有应用&lt;/li>
&lt;li>&lt;strong>动态批处理&lt;/strong>：自动批处理多请求，提高吞吐量&lt;/li>
&lt;/ol>
&lt;h2 id="-部署脚本">🐳 部署脚本&lt;/h2>
&lt;p>以下是我们用于部署的Docker命令，经过精心调优以平衡性能与资源利用：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run -d &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --runtime&lt;span style="color:#f92672">=&lt;/span>nvidia &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --gpus&lt;span style="color:#f92672">=&lt;/span>all &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --name coder &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -v /home/llm/model/qwen/qwen3-30b-a3b:/qwen/qwen3-30b-a3b &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -p 8000:8000 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --cpuset-cpus 0-55 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ulimit memlock&lt;span style="color:#f92672">=&lt;/span>-1 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ulimit stack&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">67108864&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --restart always &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ipc&lt;span style="color:#f92672">=&lt;/span>host &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> vllm/vllm-openai:v0.8.5 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --model /qwen/qwen3-30b-a3b &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --served-model-name coder &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --tensor-parallel-size &lt;span style="color:#ae81ff">4&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --dtype half &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --max-model-len &lt;span style="color:#ae81ff">32768&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --max-num-batched-tokens &lt;span style="color:#ae81ff">4096&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --gpu-memory-utilization 0.93 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --block-size &lt;span style="color:#ae81ff">32&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --enable-chunked-prefill &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --swap-space &lt;span style="color:#ae81ff">16&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --tokenizer-pool-size &lt;span style="color:#ae81ff">56&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --disable-custom-all-reduce
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="-参数详解与优化策略">🔧 参数详解与优化策略&lt;/h2>
&lt;h3 id="docker容器配置">Docker容器配置&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>参数&lt;/th>
&lt;th>值&lt;/th>
&lt;th>作用&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>--runtime=nvidia&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;td>启用NVIDIA容器运行时&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>--gpus=all&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;td>将所有GPU暴露给容器&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>--cpuset-cpus&lt;/code>&lt;/td>
&lt;td>&lt;code>0-55&lt;/code>&lt;/td>
&lt;td>限制容器使用0-55号CPU核心&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>--ulimit memlock=-1&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;td>移除内存锁定限制，提高性能&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>--ipc=host&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;td>使用主机IPC命名空间，对共享内存很重要&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="vllm引擎配置">vLLM引擎配置&lt;/h3>
&lt;h4 id="1-张量并行策略">1. 张量并行策略&lt;/h4>
&lt;pre tabindex="0">&lt;code>--tensor-parallel-size 4
&lt;/code>&lt;/pre>&lt;p>我们使用4路张量并行，将模型分布在4块GPU上。这是基于实验得出的最佳配置 - 在我们的硬件上，每块22GB显存的GPU无法单独加载完整的30B模型。&lt;/p></description><content:encoded><![CDATA[<h1 id="高性能部署qwen3-30bvllm优化实践指南">高性能部署Qwen3-30B：vLLM优化实践指南</h1>
<h2 id="-概述">📋 概述</h2>
<p>本文详细介绍如何使用vLLM高效部署Qwen3-30B-A3B模型，实现32K上下文窗口和OpenAI兼容API，适用于生产环境。通过精细调整部署参数，我们能够在有限的GPU资源下最大化模型性能。</p>
<h2 id="-系统要求">🖥️ 系统要求</h2>
<ul>
<li><strong>硬件配置</strong>
<ul>
<li>4块NVIDIA GPU (每块22GB显存，总计88GB)</li>
<li>512GB系统内存</li>
<li>2TB SSD存储</li>
<li>56核CPU</li>
</ul>
</li>
<li><strong>软件环境</strong>
<ul>
<li>Ubuntu 24.04</li>
<li>NVIDIA驱动 550.144.03</li>
<li>CUDA 12.4</li>
<li>Docker + NVIDIA Container Toolkit</li>
</ul>
</li>
</ul>
<h2 id="-模型与架构">🧠 模型与架构</h2>
<p>Qwen3-30B-A3B是阿里云发布的通用大语言模型，具有以下特点：</p>
<ul>
<li>30B参数量</li>
<li>原生支持32K上下文长度</li>
<li>支持思考模式(Chain-of-Thought)</li>
<li>优异的多语言与代码能力</li>
</ul>
<p>我们使用vLLM作为推理引擎，主要基于以下考量：</p>
<ol>
<li><strong>高效内存管理</strong>：通过PagedAttention技术优化KV缓存</li>
<li><strong>张量并行</strong>：自动跨多GPU分布模型权重</li>
<li><strong>OpenAI兼容API</strong>：直接替代OpenAI API，无需修改现有应用</li>
<li><strong>动态批处理</strong>：自动批处理多请求，提高吞吐量</li>
</ol>
<h2 id="-部署脚本">🐳 部署脚本</h2>
<p>以下是我们用于部署的Docker命令，经过精心调优以平衡性能与资源利用：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run -d <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --runtime<span style="color:#f92672">=</span>nvidia <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --gpus<span style="color:#f92672">=</span>all <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --name coder <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -v /home/llm/model/qwen/qwen3-30b-a3b:/qwen/qwen3-30b-a3b <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -p 8000:8000 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --cpuset-cpus 0-55 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --ulimit memlock<span style="color:#f92672">=</span>-1 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --ulimit stack<span style="color:#f92672">=</span><span style="color:#ae81ff">67108864</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --restart always <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --ipc<span style="color:#f92672">=</span>host <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  vllm/vllm-openai:v0.8.5 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --model /qwen/qwen3-30b-a3b <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --served-model-name coder <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --tensor-parallel-size <span style="color:#ae81ff">4</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --dtype half <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --max-model-len <span style="color:#ae81ff">32768</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --max-num-batched-tokens <span style="color:#ae81ff">4096</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --gpu-memory-utilization 0.93 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --block-size <span style="color:#ae81ff">32</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --enable-chunked-prefill <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --swap-space <span style="color:#ae81ff">16</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --tokenizer-pool-size <span style="color:#ae81ff">56</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --disable-custom-all-reduce
</span></span></code></pre></div><h2 id="-参数详解与优化策略">🔧 参数详解与优化策略</h2>
<h3 id="docker容器配置">Docker容器配置</h3>
<table>
  <thead>
      <tr>
          <th>参数</th>
          <th>值</th>
          <th>作用</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><code>--runtime=nvidia</code></td>
          <td></td>
          <td>启用NVIDIA容器运行时</td>
      </tr>
      <tr>
          <td><code>--gpus=all</code></td>
          <td></td>
          <td>将所有GPU暴露给容器</td>
      </tr>
      <tr>
          <td><code>--cpuset-cpus</code></td>
          <td><code>0-55</code></td>
          <td>限制容器使用0-55号CPU核心</td>
      </tr>
      <tr>
          <td><code>--ulimit memlock=-1</code></td>
          <td></td>
          <td>移除内存锁定限制，提高性能</td>
      </tr>
      <tr>
          <td><code>--ipc=host</code></td>
          <td></td>
          <td>使用主机IPC命名空间，对共享内存很重要</td>
      </tr>
  </tbody>
</table>
<h3 id="vllm引擎配置">vLLM引擎配置</h3>
<h4 id="1-张量并行策略">1. 张量并行策略</h4>
<pre tabindex="0"><code>--tensor-parallel-size 4
</code></pre><p>我们使用4路张量并行，将模型分布在4块GPU上。这是基于实验得出的最佳配置 - 在我们的硬件上，每块22GB显存的GPU无法单独加载完整的30B模型。</p>
<h4 id="2-内存优化">2. 内存优化</h4>
<pre tabindex="0"><code>--dtype half
--gpu-memory-utilization 0.93
--block-size 32
--swap-space 16
</code></pre><ul>
<li><code>half</code>精度(FP16)相比<code>bfloat16</code>能进一步节省内存，且在我们的场景中精度损失可接受</li>
<li>GPU内存利用率93%留出一定缓冲空间防止OOM错误</li>
<li>KV缓存块大小设为32，平衡内存使用与计算效率</li>
<li>16GB的CPU-GPU交换空间支持处理超长序列</li>
</ul>
<h4 id="3-上下文长度与批处理">3. 上下文长度与批处理</h4>
<pre tabindex="0"><code>--max-model-len 32768
--max-num-batched-tokens 4096
--enable-chunked-prefill
</code></pre><p>我们将上下文长度从默认的16K增加到32K，以支持更长输入和输出。为了平衡资源使用，相应地将批处理令牌数从8192减少到4096，这是一个经过测试的合理折中方案。</p>
<p>启用分块预填充(<code>chunked-prefill</code>)对于处理长上下文尤为重要，它将长序列分解为更小的块进行处理，减少显存峰值使用。</p>
<h4 id="4-其他性能调优">4. 其他性能调优</h4>
<pre tabindex="0"><code>--tokenizer-pool-size 56
--disable-custom-all-reduce
</code></pre><ul>
<li>令牌化工作池大小与CPU核心数匹配，优化并行处理能力</li>
<li>禁用自定义all-reduce操作，解决某些硬件配置上的兼容性问题</li>
</ul>
<h2 id="-性能分析">📊 性能分析</h2>
<p>部署后，我们可以通过<code>docker logs -f coder</code>查看服务状态，关键性能指标如下：</p>
<pre tabindex="0"><code>INFO 06-03 02:01:19 [worker.py:287] the current vLLM instance can use total_gpu_memory (21.66GiB) x gpu_memory_utilization (0.93) = 20.15GiB
INFO 06-03 02:01:19 [worker.py:287] model weights take 14.25GiB; non_torch_memory takes 0.20GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 4.30GiB.
INFO 06-03 02:01:20 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 5.73x
</code></pre><p>这表明：</p>
<ul>
<li>每个GPU使用约20.15GB内存</li>
<li>模型权重占用14.25GB</li>
<li>对于32K令牌请求，系统可以并发处理5.73倍的请求</li>
</ul>
<p>在我们的生产环境中，这个配置能够处理每分钟约15-20个并发对话，满足中小型应用需求。</p>
<h2 id="-api使用示例">📝 API使用示例</h2>
<p>服务启动后，可以通过OpenAI兼容的API在本地端口8000访问：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl http://localhost:8000/v1/chat/completions <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -H <span style="color:#e6db74">&#34;Content-Type: application/json&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -d <span style="color:#e6db74">&#39;{
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;model&#34;: &#34;coder&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;messages&#34;: [
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;请解释一下量子计算的基本原理&#34;}
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    ],
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;temperature&#34;: 0.7,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;max_tokens&#34;: 2000
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  }&#39;</span>
</span></span></code></pre></div><p>使用Python客户端：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> openai <span style="color:#f92672">import</span> OpenAI
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>client <span style="color:#f92672">=</span> OpenAI(
</span></span><span style="display:flex;"><span>    base_url<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;http://localhost:8000/v1&#34;</span>,
</span></span><span style="display:flex;"><span>    api_key<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;not-needed&#34;</span>  <span style="color:#75715e"># vLLM不要求API密钥</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> client<span style="color:#f92672">.</span>chat<span style="color:#f92672">.</span>completions<span style="color:#f92672">.</span>create(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;coder&#34;</span>,
</span></span><span style="display:flex;"><span>    messages<span style="color:#f92672">=</span>[
</span></span><span style="display:flex;"><span>        {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: <span style="color:#e6db74">&#34;写一个Python函数计算斐波那契数列&#34;</span>}
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>    temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">0.7</span>,
</span></span><span style="display:flex;"><span>    max_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(response<span style="color:#f92672">.</span>choices[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>message<span style="color:#f92672">.</span>content)
</span></span></code></pre></div><h2 id="-扩展到更长上下文">🚀 扩展到更长上下文</h2>
<p>Qwen3-30B原生支持32K上下文，但如需扩展到更长上下文(如131K令牌)，可以使用YaRN技术，通过在vLLM参数中添加：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>--rope-scaling <span style="color:#e6db74">&#39;{&#34;rope_type&#34;:&#34;yarn&#34;,&#34;factor&#34;:4.0,&#34;original_max_position_embeddings&#34;:32768}&#39;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>--max-model-len <span style="color:#ae81ff">131072</span>
</span></span></code></pre></div><p>注意这会增加内存使用，可能需要进一步调整其他参数以平衡资源。</p>
<h2 id="-常见问题排查">🔍 常见问题排查</h2>
<ol>
<li><strong>OOM错误</strong>：减小<code>gpu-memory-utilization</code>或<code>max-num-batched-tokens</code></li>
<li><strong>推理速度慢</strong>：检查GPU利用率，考虑增加batch大小或减小<code>max-model-len</code></li>
<li><strong>CUDA图捕获失败</strong>：添加<code>--enforce-eager</code>参数禁用CUDA图优化</li>
</ol>
<h2 id="-未来优化方向">📈 未来优化方向</h2>
<ul>
<li>探索使用FlashAttention-2加速注意力计算</li>
<li>尝试AWQ/GPTQ量化技术降低内存使用</li>
<li>配置LLM Router实现多模型负载均衡</li>
</ul>
<h2 id="-总结">🔚 总结</h2>
<p>通过精细调优vLLM部署参数，我们成功在有限硬件资源下部署了Qwen3-30B模型，实现了32K上下文窗口的高性能推理服务。这套配置在生产环境中表现稳定，为各类应用提供强大的AI能力支持。</p>
]]></content:encoded></item><item><title>AI辅助编程实战：从需求到实现的完整对话记录</title><link>https://jackypanster.github.io/ai-stream/posts/coding-with-ai/</link><pubDate>Tue, 03 Jun 2025 13:28:50 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/coding-with-ai/</guid><description>&lt;h1 id="ai辅助编程实战从需求到实现的完整对话记录">AI辅助编程实战：从需求到实现的完整对话记录&lt;/h1>
&lt;h2 id="引言">引言&lt;/h2>
&lt;p>本文真实还原了我与AI助手协作实现Docker镜像大小监控功能的全过程。通过这次实践，可以看到AI不仅能辅助开发者梳理需求、设计方案，还能在代码实现、文档编写等环节提供高效帮助。整个流程自然流畅，极大提升了开发体验。&lt;/p>
&lt;h2 id="1-需求分析与方案设计">1. 需求分析与方案设计&lt;/h2>
&lt;h3 id="初始需求描述">初始需求描述&lt;/h3>
&lt;pre tabindex="0">&lt;code>用户：
我们需要在Jenkins共享库中增加Docker镜像大小监控功能，当镜像超过4GB时发出警告。请帮忙分析一下实现思路。
&lt;/code>&lt;/pre>&lt;h3 id="ai的初步分析与建议">AI的初步分析与建议&lt;/h3>
&lt;pre tabindex="0">&lt;code>AI：
让我先看看项目的结构……
这是一个Jenkins共享库项目，核心代码都在`vars/`目录。针对镜像大小监控，我建议直接在`buildDocker`方法里加逻辑：
1. 镜像构建完成后用`du`命令获取大小
2. 与4GB阈值比较
3. 超过阈值就输出警告
4. 相关指标写日志，方便后续分析
这个方案您觉得合适吗？有无补充？
&lt;/code>&lt;/pre>&lt;h3 id="需求确认与细化">需求确认与细化&lt;/h3>
&lt;pre tabindex="0">&lt;code>用户：
这个思路可以，不过还需要：
1. 阈值要可配置，默认4GB
2. 日志里要有镜像名、大小、时间戳等详细信息
3. 输出优化建议
另外，异常处理也要考虑得更优雅些。
&lt;/code>&lt;/pre>&lt;h2 id="2-代码实现">2. 代码实现&lt;/h2>
&lt;h3 id="初始代码实现">初始代码实现&lt;/h3>
&lt;pre tabindex="0">&lt;code>AI：
明白，我来实现。会在`buildDocker`方法里：
1. 用`getEnvOrDefault`支持阈值配置
2. 详细记录指标
3. 加强错误处理
4. 输出优化建议
实现逻辑您看还有什么要调整的吗？
&lt;/code>&lt;/pre>&lt;h3 id="代码审查与优化">代码审查与优化&lt;/h3>
&lt;pre tabindex="0">&lt;code>用户：
整体不错，不过警告要改成抛异常直接中断构建。另外，能不能顺便加上基础镜像大小检测？
&lt;/code>&lt;/pre>&lt;h3 id="异常处理优化">异常处理优化&lt;/h3>
&lt;pre tabindex="0">&lt;code>AI：
明白您的需求，代码我会这样优化：
1. 超标直接抛异常中断流水线
2. 增加基础镜像大小检测
3. 明确区分业务异常（比如镜像过大）和技术异常（比如命令失败）
基础镜像检测这块，我建议：
- 解析Dockerfile的FROM
- 拉取并检测基础镜像大小
- 超限就给出alpine等轻量镜像建议
这样处理您觉得OK吗？
&lt;/code>&lt;/pre>&lt;h2 id="3-测试与验证">3. 测试与验证&lt;/h2>
&lt;h3 id="测试方案讨论">测试方案讨论&lt;/h3>
&lt;pre tabindex="0">&lt;code>用户：
怎么验证这个功能？
AI：
建议这样测试：
1. 镜像大小正常
2. 恰好等于阈值
3. 超过阈值
4. Dockerfile解析异常
也可以临时把阈值调小，快速验证告警逻辑。
&lt;/code>&lt;/pre>&lt;h2 id="4-文档与沟通">4. 文档与沟通&lt;/h2>
&lt;h3 id="变更日志生成">变更日志生成&lt;/h3>
&lt;pre tabindex="0">&lt;code>AI：
我已为您生成详细CHANGELOG，包括：
- 功能说明
- 技术实现细节
- 配置参数说明
- 后续优化计划
如需调整，随时告诉我。
&lt;/code>&lt;/pre>&lt;h3 id="团队通知">团队通知&lt;/h3>
&lt;pre tabindex="0">&lt;code>用户：
要发个通知给团队，说明我们要限制镜像大小。
AI：
建议分阶段推进：
1. 先监控和通知
2. 再进入警告期
3. 最后强制限制
这样大家有缓冲期，可以逐步优化Dockerfile。需要帮忙起草通知吗？
&lt;/code>&lt;/pre>&lt;h2 id="5-经验总结">5. 经验总结&lt;/h2>
&lt;h3 id="技术收获">技术收获&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>AI辅助效率&lt;/strong>&lt;/p></description><content:encoded><![CDATA[<h1 id="ai辅助编程实战从需求到实现的完整对话记录">AI辅助编程实战：从需求到实现的完整对话记录</h1>
<h2 id="引言">引言</h2>
<p>本文真实还原了我与AI助手协作实现Docker镜像大小监控功能的全过程。通过这次实践，可以看到AI不仅能辅助开发者梳理需求、设计方案，还能在代码实现、文档编写等环节提供高效帮助。整个流程自然流畅，极大提升了开发体验。</p>
<h2 id="1-需求分析与方案设计">1. 需求分析与方案设计</h2>
<h3 id="初始需求描述">初始需求描述</h3>
<pre tabindex="0"><code>用户：
我们需要在Jenkins共享库中增加Docker镜像大小监控功能，当镜像超过4GB时发出警告。请帮忙分析一下实现思路。
</code></pre><h3 id="ai的初步分析与建议">AI的初步分析与建议</h3>
<pre tabindex="0"><code>AI：
让我先看看项目的结构……

这是一个Jenkins共享库项目，核心代码都在`vars/`目录。针对镜像大小监控，我建议直接在`buildDocker`方法里加逻辑：

1. 镜像构建完成后用`du`命令获取大小
2. 与4GB阈值比较
3. 超过阈值就输出警告
4. 相关指标写日志，方便后续分析

这个方案您觉得合适吗？有无补充？
</code></pre><h3 id="需求确认与细化">需求确认与细化</h3>
<pre tabindex="0"><code>用户：
这个思路可以，不过还需要：
1. 阈值要可配置，默认4GB
2. 日志里要有镜像名、大小、时间戳等详细信息
3. 输出优化建议

另外，异常处理也要考虑得更优雅些。
</code></pre><h2 id="2-代码实现">2. 代码实现</h2>
<h3 id="初始代码实现">初始代码实现</h3>
<pre tabindex="0"><code>AI：
明白，我来实现。会在`buildDocker`方法里：

1. 用`getEnvOrDefault`支持阈值配置
2. 详细记录指标
3. 加强错误处理
4. 输出优化建议

实现逻辑您看还有什么要调整的吗？
</code></pre><h3 id="代码审查与优化">代码审查与优化</h3>
<pre tabindex="0"><code>用户：
整体不错，不过警告要改成抛异常直接中断构建。另外，能不能顺便加上基础镜像大小检测？
</code></pre><h3 id="异常处理优化">异常处理优化</h3>
<pre tabindex="0"><code>AI：
明白您的需求，代码我会这样优化：

1. 超标直接抛异常中断流水线
2. 增加基础镜像大小检测
3. 明确区分业务异常（比如镜像过大）和技术异常（比如命令失败）

基础镜像检测这块，我建议：
- 解析Dockerfile的FROM
- 拉取并检测基础镜像大小
- 超限就给出alpine等轻量镜像建议

这样处理您觉得OK吗？
</code></pre><h2 id="3-测试与验证">3. 测试与验证</h2>
<h3 id="测试方案讨论">测试方案讨论</h3>
<pre tabindex="0"><code>用户：
怎么验证这个功能？

AI：
建议这样测试：
1. 镜像大小正常
2. 恰好等于阈值
3. 超过阈值
4. Dockerfile解析异常

也可以临时把阈值调小，快速验证告警逻辑。
</code></pre><h2 id="4-文档与沟通">4. 文档与沟通</h2>
<h3 id="变更日志生成">变更日志生成</h3>
<pre tabindex="0"><code>AI：
我已为您生成详细CHANGELOG，包括：
- 功能说明
- 技术实现细节
- 配置参数说明
- 后续优化计划

如需调整，随时告诉我。
</code></pre><h3 id="团队通知">团队通知</h3>
<pre tabindex="0"><code>用户：
要发个通知给团队，说明我们要限制镜像大小。

AI：
建议分阶段推进：
1. 先监控和通知
2. 再进入警告期
3. 最后强制限制

这样大家有缓冲期，可以逐步优化Dockerfile。需要帮忙起草通知吗？
</code></pre><h2 id="5-经验总结">5. 经验总结</h2>
<h3 id="技术收获">技术收获</h3>
<ol>
<li>
<p><strong>AI辅助效率</strong></p>
<ul>
<li>代码实现速度提升显著</li>
<li>自动生成文档节省时间</li>
</ul>
</li>
<li>
<p><strong>代码质量</strong></p>
<ul>
<li>异常处理更完善</li>
<li>配置更灵活</li>
</ul>
</li>
<li>
<p><strong>团队协作</strong></p>
<ul>
<li>文档更规范</li>
<li>知识传递更高效</li>
</ul>
</li>
</ol>
<h3 id="建议与展望">建议与展望</h3>
<ol>
<li>建立AI辅助编码规范</li>
<li>完善提示工程实践</li>
<li>探索更多自动化场景</li>
</ol>
<h2 id="结语">结语</h2>
<p>通过这次实践，AI能陪伴开发者一起思考、决策和落地实现。期待未来AI在更多软件开发场景中发挥更大作用。</p>
]]></content:encoded></item><item><title>基于FunAudioLLM/SenseVoiceSmall搭建高效语音转录服务的实践之路</title><link>https://jackypanster.github.io/ai-stream/posts/howto-use-sensevoicesmall/</link><pubDate>Wed, 21 May 2025 15:43:08 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/howto-use-sensevoicesmall/</guid><description>&lt;h1 id="基于funaudiollmsensevoicesmall搭建高效语音转录服务的实践之路">基于FunAudioLLM/SenseVoiceSmall搭建高效语音转录服务的实践之路&lt;/h1>
&lt;h2 id="项目概述">项目概述&lt;/h2>
&lt;p>实现一个语音转录文本（ASR）的服务，目标是能够高效地将用户上传的音频文件转换为文字。出于中文语音的考虑，选择了来自 &lt;code>FunAudioLLM&lt;/code> 的 &lt;code>SenseVoiceSmall&lt;/code> 模型，它以其多语种支持、高效率以及集成的语音理解能力（如情感识别、事件检测）吸引了我。本文将详细记录从环境配置、核心功能实现到踩坑解决的全过程，并分享一些关于模型选型的思考。&lt;/p>
&lt;p>完整代码已开源在 GitHub 仓库：&lt;a href="https://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall">https://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall&lt;/a>&lt;/p>
&lt;p>项目需求文档（&lt;code>prd.md&lt;/code>）关键信息如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>模型&lt;/strong>: FunAudioLLM/SenseVoice (具体为 &lt;code>SenseVoiceSmall&lt;/code>)&lt;/li>
&lt;li>&lt;strong>本地模型路径&lt;/strong>: &lt;code>/home/llm/model/iic/SenseVoiceSmall&lt;/code> (从 ModelScope 下载)&lt;/li>
&lt;li>&lt;strong>API框架&lt;/strong>: FastAPI&lt;/li>
&lt;li>&lt;strong>Python环境管理&lt;/strong>: &lt;code>uv&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="环境配置">环境配置&lt;/h2>
&lt;p>为了保持开发环境的纯净和高效，采用了 &lt;code>uv&lt;/code> 来管理 Python 依赖。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>创建虚拟环境&lt;/strong> (如果尚未创建):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>uv venv .venv
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>source .venv/bin/activate
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>&lt;strong>安装核心依赖&lt;/strong>:
初始的 &lt;code>requirements.txt&lt;/code> 包含了 &lt;code>fastapi&lt;/code>, &lt;code>uvicorn&lt;/code>, &lt;code>python-multipart&lt;/code> 等基础库。后续根据模型加载和处理的需求，逐步添加了 &lt;code>torch&lt;/code>, &lt;code>torchaudio&lt;/code>, &lt;code>numpy&lt;/code>, &lt;code>transformers&lt;/code>, &lt;code>sentencepiece&lt;/code>, 以及最终解决模型加载问题的核心库 &lt;code>funasr&lt;/code>。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>uv pip install -r requirements.txt
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;h2 id="核心功能实现概览">核心功能实现概览&lt;/h2>
&lt;h3 id="项目结构">项目结构&lt;/h3>
&lt;p>项目的主要结构包括：&lt;/p>
&lt;ul>
&lt;li>&lt;code>app/main.py&lt;/code>: FastAPI 应用入口，定义 API 路由和应用生命周期事件（如模型加载）。&lt;/li>
&lt;li>&lt;code>app/models/sensevoice_loader.py&lt;/code>: 负责加载 &lt;code>SenseVoiceSmall&lt;/code> 模型，采用单例模式。&lt;/li>
&lt;li>&lt;code>app/services/asr_service.py&lt;/code>: 封装语音处理和模型推理的核心逻辑。&lt;/li>
&lt;li>&lt;code>app/schemas.py&lt;/code>: 定义 API 的请求和响应数据模型 (Pydantic models)。&lt;/li>
&lt;/ul>
&lt;h3 id="api-端点">API 端点&lt;/h3>
&lt;p>关键的 API 端点设计为：&lt;/p></description><content:encoded><![CDATA[<h1 id="基于funaudiollmsensevoicesmall搭建高效语音转录服务的实践之路">基于FunAudioLLM/SenseVoiceSmall搭建高效语音转录服务的实践之路</h1>
<h2 id="项目概述">项目概述</h2>
<p>实现一个语音转录文本（ASR）的服务，目标是能够高效地将用户上传的音频文件转换为文字。出于中文语音的考虑，选择了来自 <code>FunAudioLLM</code> 的 <code>SenseVoiceSmall</code> 模型，它以其多语种支持、高效率以及集成的语音理解能力（如情感识别、事件检测）吸引了我。本文将详细记录从环境配置、核心功能实现到踩坑解决的全过程，并分享一些关于模型选型的思考。</p>
<p>完整代码已开源在 GitHub 仓库：<a href="https://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall">https://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall</a></p>
<p>项目需求文档（<code>prd.md</code>）关键信息如下：</p>
<ul>
<li><strong>模型</strong>: FunAudioLLM/SenseVoice (具体为 <code>SenseVoiceSmall</code>)</li>
<li><strong>本地模型路径</strong>: <code>/home/llm/model/iic/SenseVoiceSmall</code> (从 ModelScope 下载)</li>
<li><strong>API框架</strong>: FastAPI</li>
<li><strong>Python环境管理</strong>: <code>uv</code></li>
</ul>
<h2 id="环境配置">环境配置</h2>
<p>为了保持开发环境的纯净和高效，采用了 <code>uv</code> 来管理 Python 依赖。</p>
<ol>
<li>
<p><strong>创建虚拟环境</strong> (如果尚未创建):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>uv venv .venv
</span></span><span style="display:flex;"><span>source .venv/bin/activate
</span></span></code></pre></div></li>
<li>
<p><strong>安装核心依赖</strong>:
初始的 <code>requirements.txt</code> 包含了 <code>fastapi</code>, <code>uvicorn</code>, <code>python-multipart</code> 等基础库。后续根据模型加载和处理的需求，逐步添加了 <code>torch</code>, <code>torchaudio</code>, <code>numpy</code>, <code>transformers</code>, <code>sentencepiece</code>, 以及最终解决模型加载问题的核心库 <code>funasr</code>。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>uv pip install -r requirements.txt
</span></span></code></pre></div></li>
</ol>
<h2 id="核心功能实现概览">核心功能实现概览</h2>
<h3 id="项目结构">项目结构</h3>
<p>项目的主要结构包括：</p>
<ul>
<li><code>app/main.py</code>: FastAPI 应用入口，定义 API 路由和应用生命周期事件（如模型加载）。</li>
<li><code>app/models/sensevoice_loader.py</code>: 负责加载 <code>SenseVoiceSmall</code> 模型，采用单例模式。</li>
<li><code>app/services/asr_service.py</code>: 封装语音处理和模型推理的核心逻辑。</li>
<li><code>app/schemas.py</code>: 定义 API 的请求和响应数据模型 (Pydantic models)。</li>
</ul>
<h3 id="api-端点">API 端点</h3>
<p>关键的 API 端点设计为：</p>
<h4 id="post-asr_pure">POST /asr_pure</h4>
<ul>
<li><strong>Content-Type</strong>: <code>multipart/form-data</code></li>
<li><strong>Body</strong>: <code>file</code> (音频文件)</li>
</ul>
<p>返回转录后的文本及处理时间。</p>
<h2 id="踩坑与解决之路模型加载的曲折历程">踩坑与解决之路：模型加载的曲折历程</h2>
<p>在项目推进过程中，模型加载部分是遇到问题最多的地方，也是收获最多的地方。</p>
<h3 id="坑1hugging-face-autoclass-的-unrecognized-model">坑1：Hugging Face <code>AutoClass</code> 的 &ldquo;Unrecognized model&rdquo;</h3>
<p>最初，尝试使用 Hugging Face <code>transformers</code> 库通用的 <code>AutoProcessor.from_pretrained()</code> 和 <code>AutoModelForSpeechSeq2Seq.from_pretrained()</code> 来加载本地的 <code>SenseVoiceSmall</code> 模型文件。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># app/models/sensevoice_loader.py (早期尝试)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ...</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># self.processor = AutoProcessor.from_pretrained(MODEL_PATH)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># self.model = AutoModelForSpeechSeq2Seq.from_pretrained(MODEL_PATH)</span>
</span></span></code></pre></div><p>然而，服务启动时立即报错：</p>
<pre tabindex="0"><code>ValueError: Unrecognized model in /home/llm/model/iic/SenseVoiceSmall. Should have a model_type key in its config.json...
</code></pre><p>这个错误表明 <code>transformers</code> 的自动发现机制无法识别模型类型，通常是因为模型目录下的 <code>config.json</code> 文件缺少 <code>model_type</code> 字段，或者该模型需要特定的加载类。</p>
<h3 id="坑2转向-funasr-与-trust_remote_code-的初步探索">坑2：转向 <code>funasr</code> 与 <code>trust_remote_code</code> 的初步探索</h3>
<p>查阅 <code>FunAudioLLM/SenseVoice</code> 的官方文档后发现，推荐使用 <code>funasr</code> 库的 <code>AutoModel</code> 来加载 <code>SenseVoice</code> 系列模型。于是调整了代码：</p>
<ol>
<li><strong>添加 <code>funasr</code> 到 <code>requirements.txt</code></strong>。</li>
<li><strong>修改 <code>SenseVoiceLoader</code></strong>:
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># app/models/sensevoice_loader.py (引入 funasr)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> funasr <span style="color:#f92672">import</span> AutoModel
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ...</span>
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> AutoModel(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span>FUNASR_MODEL_NAME_OR_PATH, <span style="color:#75715e"># 即本地路径</span>
</span></span><span style="display:flex;"><span>    trust_remote_code<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    device<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>device
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div>同时，<code>asr_service.py</code> 中的推理逻辑也相应调整为调用 <code>funasr</code> 模型对象的 <code>.generate()</code> 方法。</li>
</ol>
<p>本以为这样能解决问题，但启动时又遇到了新的日志：</p>
<pre tabindex="0"><code>Loading remote code failed: model, No module named &#39;model&#39;
</code></pre><p>尽管这条日志出现，但后续的 API 调用测试居然成功了！这让我非常困惑。</p>
<h3 id="坑3remote_code-参数与-modelpy-文件的幻影">坑3：<code>remote_code</code> 参数与 <code>model.py</code> 文件的“幻影”</h3>
<p>深入研究 <code>funasr</code> 和 <code>SenseVoice</code> 的文档，注意到对于包含自定义代码（如 <code>model.py</code>）的模型，除了 <code>trust_remote_code=True</code>，有时还需要明确指定 <code>remote_code</code> 参数。</p>
<p>我检查了 Hugging Face 仓库 <code>FunAudioLLM/SenseVoiceSmall</code> (<a href="https://huggingface.co/FunAudioLLM/SenseVoiceSmall/tree/main">https://huggingface.co/FunAudioLLM/SenseVoiceSmall/tree/main</a>)，发现其文件列表中确实包含一个 <code>model.py</code>。因此，我尝试在 <code>AutoModel</code> 调用中加入 <code>remote_code=&quot;model.py&quot;</code>。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># app/models/sensevoice_loader.py (尝试指定 remote_code)</span>
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> AutoModel(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span>FUNASR_MODEL_NAME_OR_PATH,
</span></span><span style="display:flex;"><span>    trust_remote_code<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    remote_code<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;model.py&#34;</span>, <span style="color:#75715e"># &lt;--- 新增</span>
</span></span><span style="display:flex;"><span>    device<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>device
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>结果，<code>No module named 'model'</code> 的错误依旧。</p>
<h3 id="解决方案澄清-modelscope-与-hugging-face-的模型文件差异">解决方案：澄清 ModelScope 与 Hugging Face 的模型文件差异</h3>
<p>本地模型 <code>/home/llm/model/iic/SenseVoiceSmall</code> 是从 <strong>ModelScope</strong> (<a href="https://www.modelscope.cn/models/iic/SenseVoiceSmall/files">https://www.modelscope.cn/models/iic/SenseVoiceSmall/files</a>) 下载的，而非直接 clone Hugging Face 的仓库。通过 <code>ls -al /home/llm/model/iic/SenseVoiceSmall/</code> 查看本地文件，<strong>发现确实没有 <code>model.py</code> 文件！</strong></p>
<p>这解释了为什么指定 <code>remote_code=&quot;model.py&quot;</code> 依然报错。ModelScope 提供的模型包可能与 Hugging Face 仓库中的文件结构不完全一致，特别是对于这种依赖 <code>funasr</code> 特定加载方式的模型。</p>
<p><strong>最终的正确配置</strong>：移除 <code>remote_code</code> 参数，但保留 <code>trust_remote_code=True</code>。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># app/models/sensevoice_loader.py (最终正确配置)</span>
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> AutoModel(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span>FUNASR_MODEL_NAME_OR_PATH,
</span></span><span style="display:flex;"><span>    trust_remote_code<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, <span style="color:#75715e"># 保留，funasr 可能仍需此权限处理 ModelScope 模型</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># remote_code=&#34;model.py&#34;, # 移除，因为本地 ModelScope 版本无此文件</span>
</span></span><span style="display:flex;"><span>    device<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>device
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>这样修改后，服务启动时仍然会打印 <code>Loading remote code failed: model, No module named 'model'</code>，但 API 调用完全正常！</p>
<p><strong>原因分析</strong>：<code>funasr</code> 在 <code>trust_remote_code=True</code> 时，会优先尝试加载自定义代码。如果本地模型路径（如从 ModelScope 下载的）没有 <code>model.py</code>，这个尝试会失败并打印日志。但随后，<code>funasr</code> 能够识别出这是一个有效的 ModelScope 模型路径，并转用其内部的标准加载流程成功加载模型。因此，该日志在这种情况下是良性的。</p>
<h2 id="模型对比与选型思考">模型对比与选型思考</h2>
<p>在解决问题的过程中，也探讨了 <code>FunAudioLLM/SenseVoiceSmall</code> 与其他主流 ASR 模型的对比：</p>
<ul>
<li>
<p><strong>OpenAI Whisper 系列</strong> (如 <code>whisper-large-v3</code>):</p>
<ul>
<li><strong>优势</strong>: 极高的准确率，强大的多语言能力，庞大的社区。</li>
<li><strong>劣势</strong>: 推理速度相对较慢（尤其大模型），不直接提供情感/事件检测。</li>
</ul>
</li>
<li>
<p><strong>Wav2Vec2 系列</strong>:</p>
<ul>
<li><strong>优势</strong>: 自监督学习典范，大量特定语言微调模型。</li>
<li><strong>劣势</strong>: 基础模型功能相对单一。</li>
</ul>
</li>
</ul>
<h3 id="sensevoicesmall"><strong><code>SenseVoiceSmall</code> 的核心优势</strong></h3>
<ol>
<li>
<p><strong>高效推理</strong>：其模型卡声称采用非自回归端到端框架，比 Whisper-Large 快15倍。这对于需要低延迟的应用至关重要。</p>
</li>
<li>
<p><strong>多任务集成</strong>：内置 ASR、LID（语种识别）、SER（情感识别）、AED（事件检测）。如果应用场景需要这些附加信息，<code>SenseVoiceSmall</code> 提供了一站式解决方案。</p>
</li>
<li>
<p><strong>特定语言优化</strong>：在中文、粤语等语言上表现突出。</p>
</li>
</ol>
<h3 id="结论"><strong>结论</strong></h3>
<p>没有绝对的“最好”，只有“最适合”。</p>
<ul>
<li>若追求极致准确性和最广语言覆盖，且对延迟不敏感，Whisper 仍是首选。</li>
<li>若对<strong>推理效率、集成的多任务语音理解（特别是情感/事件）或中文等特定场景有高要求</strong>，<code>SenseVoiceSmall</code> 是一个极具竞争力的选择。</li>
</ul>
<p>目前选择的 <code>SenseVoiceSmall</code>，尤其是在确认了其 ModelScope 版本能够顺畅运行后，对于我的项目目标来说是一个合适的起点。</p>
<h2 id="当前状态与展望">当前状态与展望</h2>
<p>目前，基于 <code>FunAudioLLM/SenseVoiceSmall</code> 和 FastAPI 的语音转录服务已成功搭建并能正确处理请求。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>$ curl -X POST <span style="color:#e6db74">&#34;http://&lt;your_server_ip&gt;:8888/asr_pure&#34;</span> -F <span style="color:#e6db74">&#34;file=@test_audio.wav&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">{</span><span style="color:#e6db74">&#34;text&#34;</span>:<span style="color:#e6db74">&#34;太好了，那接下来咱们可以试试其他功能了。比如说你想测试一下语音合成的效果怎么样，或者是看看有没有什么新的语音处理功能出来啦。😔&#34;</span>,<span style="color:#e6db74">&#34;status&#34;</span>:<span style="color:#e6db74">&#34;success&#34;</span>,<span style="color:#e6db74">&#34;processing_time_ms&#34;</span>:503.39...<span style="color:#f92672">}</span>
</span></span></code></pre></div><h3 id="后续可优化的方向"><strong>后续可优化的方向</strong></h3>
<ul>
<li><strong>性能优化</strong>：进一步测试并发处理能力，考虑多 worker 配置。</li>
<li><strong>错误处理与日志</strong>：完善更细致的错误捕获和日志记录。</li>
<li><strong>功能扩展</strong>：如果需要，可以利用 <code>SenseVoiceSmall</code> 的情感识别和事件检测能力。</li>
<li><strong>VAD 集成</strong>：对于长音频，考虑在 <code>funasr.AutoModel</code> 加载时集成 VAD (Voice Activity Detection) 功能，以实现自动分段处理，提升长音频处理的稳定性和效率。</li>
<li><strong>异步处理与队列</strong>：对于高并发场景，引入消息队列和异步任务处理。</li>
</ul>
]]></content:encoded></item><item><title>如何使用Qwen2.5-Omni实现文本转语音(TTS)和语音转文本(ASR)</title><link>https://jackypanster.github.io/ai-stream/posts/how-to-use-qwen-omni-tts-asr/</link><pubDate>Tue, 20 May 2025 20:24:06 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/how-to-use-qwen-omni-tts-asr/</guid><description>&lt;h1 id="如何使用qwen25-omni实现文本转语音tts和语音转文本asr">如何使用Qwen2.5-Omni实现文本转语音(TTS)和语音转文本(ASR)&lt;/h1>
&lt;h2 id="项目概述">项目概述&lt;/h2>
&lt;p>本项目基于Qwen2.5-Omni-7B模型，实现了两个核心功能：&lt;/p>
&lt;ol>
&lt;li>文本转语音（TTS）：将输入文本转换为自然流畅的语音&lt;/li>
&lt;li>语音转文本（ASR）：将语音文件转换为文本，支持标准ASR和纯ASR两种模式&lt;/li>
&lt;/ol>
&lt;p>项目地址：&lt;a href="https://github.com/jackypanster/qwen-omni">https://github.com/jackypanster/qwen-omni&lt;/a>&lt;/p>
&lt;h2 id="环境配置">环境配置&lt;/h2>
&lt;p>推荐使用conda管理Python环境，确保依赖安装的稳定性：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 创建并激活环境&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>conda create -n qwen-tts python&lt;span style="color:#f92672">=&lt;/span>3.10
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>conda activate qwen-tts
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 安装PyTorch（GPU版本）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>conda install pytorch&lt;span style="color:#f92672">=&lt;/span>2.5.1 pytorch-cuda&lt;span style="color:#f92672">=&lt;/span>12.1 -c pytorch -c nvidia
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>conda install torchvision torchaudio -c pytorch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># 安装其他依赖&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>conda install streamlit python-soundfile -c conda-forge
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pip install git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pip install qwen-omni-utils
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="核心功能实现">核心功能实现&lt;/h2>
&lt;h3 id="1-文本转语音tts">1. 文本转语音（TTS）&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">text_to_speech&lt;/span>(text_input, output_audio_path&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;output/output.wav&amp;#34;&lt;/span>, speaker&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Chelsie&amp;#34;&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 加载模型和处理器&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model &lt;span style="color:#f92672">=&lt;/span> Qwen2_5OmniForConditionalGeneration&lt;span style="color:#f92672">.&lt;/span>from_pretrained(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model_path,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config&lt;span style="color:#f92672">=&lt;/span>config,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> torch_dtype&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;auto&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> device_map&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;auto&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> processor &lt;span style="color:#f92672">=&lt;/span> Qwen2_5OmniProcessor&lt;span style="color:#f92672">.&lt;/span>from_pretrained(model_path)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 构造对话&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> conversation &lt;span style="color:#f92672">=&lt;/span> [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {&lt;span style="color:#e6db74">&amp;#34;role&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;system&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;content&amp;#34;&lt;/span>: [{&lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;You are Qwen...&amp;#34;&lt;/span>}]},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {&lt;span style="color:#e6db74">&amp;#34;role&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;user&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;content&amp;#34;&lt;/span>: [{&lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>: text_input}]}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 生成语音&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">with&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>no_grad():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text_ids, audio &lt;span style="color:#f92672">=&lt;/span> model&lt;span style="color:#f92672">.&lt;/span>generate(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">**&lt;/span>inputs,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> speaker&lt;span style="color:#f92672">=&lt;/span>speaker,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> do_sample&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> temperature&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.8&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> top_p&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.95&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> max_new_tokens&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1024&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="2-语音转文本asr">2. 语音转文本（ASR）&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">audio_to_text&lt;/span>(audio_path: str) &lt;span style="color:#f92672">-&amp;gt;&lt;/span> str:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 标准ASR模式&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> conversation &lt;span style="color:#f92672">=&lt;/span> [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {&lt;span style="color:#e6db74">&amp;#34;role&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;system&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;content&amp;#34;&lt;/span>: [{&lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;你是Qwen...&amp;#34;&lt;/span>}]},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {&lt;span style="color:#e6db74">&amp;#34;role&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;user&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;content&amp;#34;&lt;/span>: [{&lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;audio&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;audio&amp;#34;&lt;/span>: audio_path}]}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 生成文本&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">with&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>no_grad():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text_ids &lt;span style="color:#f92672">=&lt;/span> model&lt;span style="color:#f92672">.&lt;/span>generate(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">**&lt;/span>inputs,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> do_sample&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> max_new_tokens&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1024&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> return_audio&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="web界面实现">Web界面实现&lt;/h2>
&lt;p>使用Streamlit构建了简洁的Web界面：&lt;/p></description><content:encoded><![CDATA[<h1 id="如何使用qwen25-omni实现文本转语音tts和语音转文本asr">如何使用Qwen2.5-Omni实现文本转语音(TTS)和语音转文本(ASR)</h1>
<h2 id="项目概述">项目概述</h2>
<p>本项目基于Qwen2.5-Omni-7B模型，实现了两个核心功能：</p>
<ol>
<li>文本转语音（TTS）：将输入文本转换为自然流畅的语音</li>
<li>语音转文本（ASR）：将语音文件转换为文本，支持标准ASR和纯ASR两种模式</li>
</ol>
<p>项目地址：<a href="https://github.com/jackypanster/qwen-omni">https://github.com/jackypanster/qwen-omni</a></p>
<h2 id="环境配置">环境配置</h2>
<p>推荐使用conda管理Python环境，确保依赖安装的稳定性：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># 创建并激活环境</span>
</span></span><span style="display:flex;"><span>conda create -n qwen-tts python<span style="color:#f92672">=</span>3.10
</span></span><span style="display:flex;"><span>conda activate qwen-tts
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 安装PyTorch（GPU版本）</span>
</span></span><span style="display:flex;"><span>conda install pytorch<span style="color:#f92672">=</span>2.5.1 pytorch-cuda<span style="color:#f92672">=</span>12.1 -c pytorch -c nvidia
</span></span><span style="display:flex;"><span>conda install torchvision torchaudio -c pytorch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 安装其他依赖</span>
</span></span><span style="display:flex;"><span>conda install streamlit python-soundfile -c conda-forge
</span></span><span style="display:flex;"><span>pip install git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview
</span></span><span style="display:flex;"><span>pip install qwen-omni-utils
</span></span></code></pre></div><h2 id="核心功能实现">核心功能实现</h2>
<h3 id="1-文本转语音tts">1. 文本转语音（TTS）</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">text_to_speech</span>(text_input, output_audio_path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;output/output.wav&#34;</span>, speaker<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Chelsie&#34;</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 加载模型和处理器</span>
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> Qwen2_5OmniForConditionalGeneration<span style="color:#f92672">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>        model_path, 
</span></span><span style="display:flex;"><span>        config<span style="color:#f92672">=</span>config, 
</span></span><span style="display:flex;"><span>        torch_dtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;auto&#34;</span>, 
</span></span><span style="display:flex;"><span>        device_map<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;auto&#34;</span>
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    processor <span style="color:#f92672">=</span> Qwen2_5OmniProcessor<span style="color:#f92672">.</span>from_pretrained(model_path)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 构造对话</span>
</span></span><span style="display:flex;"><span>    conversation <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>        {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;system&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: [{<span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;text&#34;</span>, <span style="color:#e6db74">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;You are Qwen...&#34;</span>}]},
</span></span><span style="display:flex;"><span>        {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: [{<span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;text&#34;</span>, <span style="color:#e6db74">&#34;text&#34;</span>: text_input}]}
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 生成语音</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        text_ids, audio <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>generate(
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">**</span>inputs,
</span></span><span style="display:flex;"><span>            speaker<span style="color:#f92672">=</span>speaker,
</span></span><span style="display:flex;"><span>            do_sample<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>            temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">0.8</span>,
</span></span><span style="display:flex;"><span>            top_p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.95</span>,
</span></span><span style="display:flex;"><span>            max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>
</span></span><span style="display:flex;"><span>        )
</span></span></code></pre></div><h3 id="2-语音转文本asr">2. 语音转文本（ASR）</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">audio_to_text</span>(audio_path: str) <span style="color:#f92672">-&gt;</span> str:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 标准ASR模式</span>
</span></span><span style="display:flex;"><span>    conversation <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>        {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;system&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: [{<span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;text&#34;</span>, <span style="color:#e6db74">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;你是Qwen...&#34;</span>}]},
</span></span><span style="display:flex;"><span>        {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: [{<span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;audio&#34;</span>, <span style="color:#e6db74">&#34;audio&#34;</span>: audio_path}]}
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 生成文本</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        text_ids <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>generate(
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">**</span>inputs,
</span></span><span style="display:flex;"><span>            do_sample<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>,
</span></span><span style="display:flex;"><span>            max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>,
</span></span><span style="display:flex;"><span>            return_audio<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>        )
</span></span></code></pre></div><h2 id="web界面实现">Web界面实现</h2>
<p>使用Streamlit构建了简洁的Web界面：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 文本输入</span>
</span></span><span style="display:flex;"><span>text_input <span style="color:#f92672">=</span> st<span style="color:#f92672">.</span>text_area(<span style="color:#e6db74">&#34;请输入要合成的文本：&#34;</span>, height<span style="color:#f92672">=</span><span style="color:#ae81ff">120</span>, max_chars<span style="color:#f92672">=</span><span style="color:#ae81ff">200</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 发音人选择</span>
</span></span><span style="display:flex;"><span>speaker <span style="color:#f92672">=</span> st<span style="color:#f92672">.</span>selectbox(<span style="color:#e6db74">&#34;请选择发音人：&#34;</span>, [<span style="color:#e6db74">&#34;Chelsie&#34;</span>, <span style="color:#e6db74">&#34;Ethan&#34;</span>], index<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 生成按钮</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> st<span style="color:#f92672">.</span>button(<span style="color:#e6db74">&#34;生成语音&#34;</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 生成语音并播放</span>
</span></span><span style="display:flex;"><span>    audio_path <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(OUTPUT_DIR, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;tts_</span><span style="color:#e6db74">{</span>uuid<span style="color:#f92672">.</span>uuid4()<span style="color:#f92672">.</span>hex<span style="color:#e6db74">}</span><span style="color:#e6db74">.wav&#34;</span>)
</span></span><span style="display:flex;"><span>    text_to_speech(text_input, output_audio_path<span style="color:#f92672">=</span>audio_path, speaker<span style="color:#f92672">=</span>speaker)
</span></span><span style="display:flex;"><span>    st<span style="color:#f92672">.</span>audio(audio_path, format<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;audio/wav&#34;</span>)
</span></span></code></pre></div><h2 id="restful-api实现">RESTful API实现</h2>
<p>使用FastAPI构建了RESTful API接口：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@app.post</span>(<span style="color:#e6db74">&#34;/tts&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">tts</span>(request: TTSRequest):
</span></span><span style="display:flex;"><span>    audio_filename <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;tts_</span><span style="color:#e6db74">{</span>uuid<span style="color:#f92672">.</span>uuid4()<span style="color:#f92672">.</span>hex<span style="color:#e6db74">}</span><span style="color:#e6db74">.wav&#34;</span>
</span></span><span style="display:flex;"><span>    audio_path <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(OUTPUT_DIR, audio_filename)
</span></span><span style="display:flex;"><span>    text_to_speech(request<span style="color:#f92672">.</span>text, audio_path, request<span style="color:#f92672">.</span>speaker)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#34;audio_url&#34;</span>: <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;/output/</span><span style="color:#e6db74">{</span>audio_filename<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@app.post</span>(<span style="color:#e6db74">&#34;/asr&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">asr</span>(file: UploadFile <span style="color:#f92672">=</span> File(<span style="color:#f92672">...</span>)):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 处理上传的音频文件</span>
</span></span><span style="display:flex;"><span>    audio_path <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(OUTPUT_DIR, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;asr_</span><span style="color:#e6db74">{</span>uuid<span style="color:#f92672">.</span>uuid4()<span style="color:#f92672">.</span>hex<span style="color:#e6db74">}</span><span style="color:#e6db74">.wav&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> open(audio_path, <span style="color:#e6db74">&#34;wb&#34;</span>) <span style="color:#66d9ef">as</span> buffer:
</span></span><span style="display:flex;"><span>        shutil<span style="color:#f92672">.</span>copyfileobj(file<span style="color:#f92672">.</span>file, buffer)
</span></span><span style="display:flex;"><span>    text <span style="color:#f92672">=</span> audio_to_text(audio_path)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#34;text&#34;</span>: text}
</span></span></code></pre></div><h2 id="使用说明">使用说明</h2>
<ol>
<li>启动Web界面：</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>streamlit run app_text2audio.py
</span></span></code></pre></div><ol start="2">
<li>启动API服务：</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>uvicorn fastapi_app:app --host 0.0.0.0 --port <span style="color:#ae81ff">8000</span>
</span></span></code></pre></div><h2 id="注意事项">注意事项</h2>
<ol>
<li>模型文件较大，建议提前下载并配置好模型路径</li>
<li>使用conda安装依赖可以避免大多数环境问题</li>
<li>音频文件会保存在output目录下</li>
<li>API接口支持文件上传和文本转写</li>
</ol>
<h2 id="后续优化方向">后续优化方向</h2>
<ol>
<li>支持更多发音人选项</li>
<li>优化模型加载速度</li>
<li>添加批量处理功能</li>
<li>支持更多音频格式</li>
<li>添加历史记录功能</li>
</ol>
<h2 id="参考资源">参考资源</h2>
<ul>
<li><a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B">Qwen2.5-Omni-7B官方文档</a></li>
<li><a href="https://docs.streamlit.io/">Streamlit文档</a></li>
<li><a href="https://fastapi.tiangolo.com/">FastAPI文档</a></li>
</ul>
]]></content:encoded></item><item><title>Qwen2.5-Omni-7B 文本转语音部署指南</title><link>https://jackypanster.github.io/ai-stream/posts/how-to-setup-qwen-omni/</link><pubDate>Mon, 19 May 2025 10:54:50 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/how-to-setup-qwen-omni/</guid><description>&lt;p>&lt;img alt="Qwen2.5-Omni-7B TTS" loading="lazy" src="https://via.placeholder.com/800x400.png/007bff/ffffff?text=Qwen2.5-Omni-7B+TTS">&lt;/p>
&lt;h2 id="概述">概述&lt;/h2>
&lt;p>本脚本基于 Qwen2.5-Omni-7B 多模态模型实现文本转语音（TTS）功能，支持生成自然流畅的中文 / 英文语音，并提供两种语音类型（女性 “Chelsie”、男性 “Ethan”）。脚本可将输入文本转换为音频文件（.wav格式），适用于语音助手、内容创作、无障碍服务等场景。&lt;/p>
&lt;h2 id="主要特性">主要特性&lt;/h2>
&lt;ul>
&lt;li>🎙️ 支持自然流畅的 &lt;strong>中文/英文&lt;/strong> 语音合成&lt;/li>
&lt;li>👥 提供两种语音类型选择：
&lt;ul>
&lt;li>女性声线：&amp;ldquo;Chelsie&amp;rdquo;&lt;/li>
&lt;li>男性声线：&amp;ldquo;Ethan&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>💾 输出格式：标准 &lt;code>.wav&lt;/code> 音频文件&lt;/li>
&lt;li>🚀 高性能推理，适用于生产环境&lt;/li>
&lt;/ul>
&lt;h2 id="应用场景">应用场景&lt;/h2>
&lt;ul>
&lt;li>智能语音助手开发&lt;/li>
&lt;li>内容创作与播客制作&lt;/li>
&lt;li>无障碍服务&lt;/li>
&lt;li>教育类应用&lt;/li>
&lt;li>多媒体内容生成&lt;/li>
&lt;/ul>
&lt;h2 id="开始使用">开始使用&lt;/h2>
&lt;blockquote>
&lt;p>💡 在开始之前，请确保您的系统满足以下要求：&lt;/p>
&lt;ul>
&lt;li>Python 3.8+&lt;/li>
&lt;li>CUDA 11.7+ (如需GPU加速)&lt;/li>
&lt;li>至少16GB可用内存&lt;/li>
&lt;/ul>&lt;/blockquote>
&lt;p>安装依赖库&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>uv init
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uv add git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uv add accelerate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uv add qwen-omni-utils&lt;span style="color:#f92672">[&lt;/span>decord&lt;span style="color:#f92672">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uv add soundfile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uv add torchvision
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>uv sync
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>完整脚本代码（main_text2audio.py）&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> os
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> soundfile &lt;span style="color:#66d9ef">as&lt;/span> sf
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> torch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> transformers &lt;span style="color:#f92672">import&lt;/span> Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> qwen_omni_utils &lt;span style="color:#f92672">import&lt;/span> process_mm_info
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> transformers &lt;span style="color:#f92672">import&lt;/span> AutoConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">text_to_speech&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text_input: str,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> output_audio_path: str &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;output/test_audio.wav&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> speaker: str &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;Chelsie&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model_path: str &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;/home/llm/model/qwen/Omni/&amp;#34;&lt;/span> &lt;span style="color:#75715e"># 改为本地路径或远程路径&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> 文本转语音核心函数
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> :param text_input: 输入文本（支持中文/英文）
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> :param output_audio_path: 音频输出路径（含文件名）
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> :param speaker: 语音类型（&amp;#34;Chelsie&amp;#34;女性/&amp;#34;Ethan&amp;#34;男性）
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> :param model_path: 模型路径（本地/远程）
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 1. 加载模型配置（修复ROPE参数兼容性）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config &lt;span style="color:#f92672">=&lt;/span> AutoConfig&lt;span style="color:#f92672">.&lt;/span>from_pretrained(model_path, local_files_only&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> hasattr(config, &lt;span style="color:#e6db74">&amp;#34;rope_scaling&amp;#34;&lt;/span>) &lt;span style="color:#f92672">and&lt;/span> &lt;span style="color:#e6db74">&amp;#34;mrope_section&amp;#34;&lt;/span> &lt;span style="color:#f92672">in&lt;/span> config&lt;span style="color:#f92672">.&lt;/span>rope_scaling:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config&lt;span style="color:#f92672">.&lt;/span>rope_scaling&lt;span style="color:#f92672">.&lt;/span>pop(&lt;span style="color:#e6db74">&amp;#34;mrope_section&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 2. 加载模型（支持GPU自动分配）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model &lt;span style="color:#f92672">=&lt;/span> Qwen2_5OmniForConditionalGeneration&lt;span style="color:#f92672">.&lt;/span>from_pretrained(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model_path,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config&lt;span style="color:#f92672">=&lt;/span>config,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> torch_dtype&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;auto&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> device_map&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;auto&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> local_files_only&lt;span style="color:#f92672">=&lt;/span>(model_path &lt;span style="color:#f92672">!=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;Qwen/Qwen2.5-Omni-7B&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> processor &lt;span style="color:#f92672">=&lt;/span> Qwen2_5OmniProcessor&lt;span style="color:#f92672">.&lt;/span>from_pretrained(model_path)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 3. 系统提示（必须包含语音生成能力声明）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> system_prompt &lt;span style="color:#f92672">=&lt;/span> [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;role&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;system&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;content&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {&lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&amp;#34;&lt;/span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 4. 构建对话（纯文本输入）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> conversation &lt;span style="color:#f92672">=&lt;/span> system_prompt &lt;span style="color:#f92672">+&lt;/span> [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {&lt;span style="color:#e6db74">&amp;#34;role&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;user&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;content&amp;#34;&lt;/span>: [{&lt;span style="color:#e6db74">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;text&amp;#34;&lt;/span>: text_input}]}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 5. 处理输入数据&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text &lt;span style="color:#f92672">=&lt;/span> processor&lt;span style="color:#f92672">.&lt;/span>apply_chat_template(conversation, add_generation_prompt&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>, tokenize&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> audios, images, videos &lt;span style="color:#f92672">=&lt;/span> process_mm_info(conversation, use_audio_in_video&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 6. 生成语音&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> inputs &lt;span style="color:#f92672">=&lt;/span> processor(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text&lt;span style="color:#f92672">=&lt;/span>text, audio&lt;span style="color:#f92672">=&lt;/span>audios, images&lt;span style="color:#f92672">=&lt;/span>images, videos&lt;span style="color:#f92672">=&lt;/span>videos,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> return_tensors&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;pt&amp;#34;&lt;/span>, padding&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>, use_audio_in_video&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )&lt;span style="color:#f92672">.&lt;/span>to(model&lt;span style="color:#f92672">.&lt;/span>device, model&lt;span style="color:#f92672">.&lt;/span>dtype)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">with&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>no_grad():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text_ids, audio &lt;span style="color:#f92672">=&lt;/span> model&lt;span style="color:#f92672">.&lt;/span>generate(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">**&lt;/span>inputs,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> speaker&lt;span style="color:#f92672">=&lt;/span>speaker,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> do_sample&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>, &lt;span style="color:#75715e"># 启用采样模式以使用temperature/top_p&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> temperature&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.8&lt;/span>, &lt;span style="color:#75715e"># 控制随机性（0.5-1.0较自然）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> top_p&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0.95&lt;/span>, &lt;span style="color:#75715e"># 核采样参数&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> max_new_tokens&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1024&lt;/span>, &lt;span style="color:#75715e"># 控制语音时长（约15秒）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> use_audio_in_video&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 7. 保存结果&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> os&lt;span style="color:#f92672">.&lt;/span>makedirs(os&lt;span style="color:#f92672">.&lt;/span>path&lt;span style="color:#f92672">.&lt;/span>dirname(output_audio_path), exist_ok&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sf&lt;span style="color:#f92672">.&lt;/span>write(output_audio_path, audio&lt;span style="color:#f92672">.&lt;/span>reshape(&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>)&lt;span style="color:#f92672">.&lt;/span>cpu()&lt;span style="color:#f92672">.&lt;/span>numpy(), samplerate&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">24000&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(&lt;span style="color:#e6db74">f&lt;/span>&lt;span style="color:#e6db74">&amp;#34;✅ 生成完成：&lt;/span>&lt;span style="color:#e6db74">{&lt;/span>output_audio_path&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(&lt;span style="color:#e6db74">f&lt;/span>&lt;span style="color:#e6db74">&amp;#34;📄 生成文本：&lt;/span>&lt;span style="color:#e6db74">{&lt;/span>processor&lt;span style="color:#f92672">.&lt;/span>batch_decode(text_ids, skip_special_tokens&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>)[&lt;span style="color:#ae81ff">0&lt;/span>]&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">if&lt;/span> __name__ &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#e6db74">&amp;#34;__main__&amp;#34;&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 示例输入（可替换为任意文本）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> input_text &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;你好，这是Qwen2.5-Omni的文本转语音示例。祝你使用愉快！&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 调用函数（指定输出路径和语音类型）&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> text_to_speech(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> input_text,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> output_audio_path&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;output/hello_qwen.wav&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> speaker&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Chelsie&amp;#34;&lt;/span> &lt;span style="color:#75715e"># 可选&amp;#34;Ethan&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>运行脚本&lt;/p></description><content:encoded><![CDATA[<p><img alt="Qwen2.5-Omni-7B TTS" loading="lazy" src="https://via.placeholder.com/800x400.png/007bff/ffffff?text=Qwen2.5-Omni-7B+TTS"></p>
<h2 id="概述">概述</h2>
<p>本脚本基于 Qwen2.5-Omni-7B 多模态模型实现文本转语音（TTS）功能，支持生成自然流畅的中文 / 英文语音，并提供两种语音类型（女性 “Chelsie”、男性 “Ethan”）。脚本可将输入文本转换为音频文件（.wav格式），适用于语音助手、内容创作、无障碍服务等场景。</p>
<h2 id="主要特性">主要特性</h2>
<ul>
<li>🎙️ 支持自然流畅的 <strong>中文/英文</strong> 语音合成</li>
<li>👥 提供两种语音类型选择：
<ul>
<li>女性声线：&ldquo;Chelsie&rdquo;</li>
<li>男性声线：&ldquo;Ethan&rdquo;</li>
</ul>
</li>
<li>💾 输出格式：标准 <code>.wav</code> 音频文件</li>
<li>🚀 高性能推理，适用于生产环境</li>
</ul>
<h2 id="应用场景">应用场景</h2>
<ul>
<li>智能语音助手开发</li>
<li>内容创作与播客制作</li>
<li>无障碍服务</li>
<li>教育类应用</li>
<li>多媒体内容生成</li>
</ul>
<h2 id="开始使用">开始使用</h2>
<blockquote>
<p>💡 在开始之前，请确保您的系统满足以下要求：</p>
<ul>
<li>Python 3.8+</li>
<li>CUDA 11.7+ (如需GPU加速)</li>
<li>至少16GB可用内存</li>
</ul></blockquote>
<p>安装依赖库</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>uv init
</span></span><span style="display:flex;"><span>uv add git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview
</span></span><span style="display:flex;"><span>uv add accelerate
</span></span><span style="display:flex;"><span>uv add qwen-omni-utils<span style="color:#f92672">[</span>decord<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>uv add soundfile
</span></span><span style="display:flex;"><span>uv add torchvision
</span></span><span style="display:flex;"><span>uv sync
</span></span></code></pre></div><p>完整脚本代码（main_text2audio.py）</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> soundfile <span style="color:#66d9ef">as</span> sf
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> qwen_omni_utils <span style="color:#f92672">import</span> process_mm_info
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoConfig
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">text_to_speech</span>(
</span></span><span style="display:flex;"><span>    text_input: str,
</span></span><span style="display:flex;"><span>    output_audio_path: str <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;output/test_audio.wav&#34;</span>,
</span></span><span style="display:flex;"><span>    speaker: str <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Chelsie&#34;</span>,
</span></span><span style="display:flex;"><span>    model_path: str <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/home/llm/model/qwen/Omni/&#34;</span>  <span style="color:#75715e"># 改为本地路径或远程路径</span>
</span></span><span style="display:flex;"><span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    文本转语音核心函数
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param text_input: 输入文本（支持中文/英文）
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param output_audio_path: 音频输出路径（含文件名）
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param speaker: 语音类型（&#34;Chelsie&#34;女性/&#34;Ethan&#34;男性）
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    :param model_path: 模型路径（本地/远程）
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 1. 加载模型配置（修复ROPE参数兼容性）</span>
</span></span><span style="display:flex;"><span>    config <span style="color:#f92672">=</span> AutoConfig<span style="color:#f92672">.</span>from_pretrained(model_path, local_files_only<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> hasattr(config, <span style="color:#e6db74">&#34;rope_scaling&#34;</span>) <span style="color:#f92672">and</span> <span style="color:#e6db74">&#34;mrope_section&#34;</span> <span style="color:#f92672">in</span> config<span style="color:#f92672">.</span>rope_scaling:
</span></span><span style="display:flex;"><span>        config<span style="color:#f92672">.</span>rope_scaling<span style="color:#f92672">.</span>pop(<span style="color:#e6db74">&#34;mrope_section&#34;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 2. 加载模型（支持GPU自动分配）</span>
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> Qwen2_5OmniForConditionalGeneration<span style="color:#f92672">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>        model_path,
</span></span><span style="display:flex;"><span>        config<span style="color:#f92672">=</span>config,
</span></span><span style="display:flex;"><span>        torch_dtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;auto&#34;</span>,
</span></span><span style="display:flex;"><span>        device_map<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;auto&#34;</span>,
</span></span><span style="display:flex;"><span>        local_files_only<span style="color:#f92672">=</span>(model_path <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#34;Qwen/Qwen2.5-Omni-7B&#34;</span>)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    processor <span style="color:#f92672">=</span> Qwen2_5OmniProcessor<span style="color:#f92672">.</span>from_pretrained(model_path)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 3. 系统提示（必须包含语音生成能力声明）</span>
</span></span><span style="display:flex;"><span>    system_prompt <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;system&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;content&#34;</span>: [
</span></span><span style="display:flex;"><span>                {<span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;text&#34;</span>, <span style="color:#e6db74">&#34;text&#34;</span>: <span style="color:#e6db74">&#34;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&#34;</span>}
</span></span><span style="display:flex;"><span>            ]
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 4. 构建对话（纯文本输入）</span>
</span></span><span style="display:flex;"><span>    conversation <span style="color:#f92672">=</span> system_prompt <span style="color:#f92672">+</span> [
</span></span><span style="display:flex;"><span>        {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: [{<span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;text&#34;</span>, <span style="color:#e6db74">&#34;text&#34;</span>: text_input}]}
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 5. 处理输入数据</span>
</span></span><span style="display:flex;"><span>    text <span style="color:#f92672">=</span> processor<span style="color:#f92672">.</span>apply_chat_template(conversation, add_generation_prompt<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, tokenize<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>    audios, images, videos <span style="color:#f92672">=</span> process_mm_info(conversation, use_audio_in_video<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 6. 生成语音</span>
</span></span><span style="display:flex;"><span>    inputs <span style="color:#f92672">=</span> processor(
</span></span><span style="display:flex;"><span>        text<span style="color:#f92672">=</span>text, audio<span style="color:#f92672">=</span>audios, images<span style="color:#f92672">=</span>images, videos<span style="color:#f92672">=</span>videos,
</span></span><span style="display:flex;"><span>        return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pt&#34;</span>, padding<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, use_audio_in_video<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>    )<span style="color:#f92672">.</span>to(model<span style="color:#f92672">.</span>device, model<span style="color:#f92672">.</span>dtype)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        text_ids, audio <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>generate(
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">**</span>inputs,
</span></span><span style="display:flex;"><span>            speaker<span style="color:#f92672">=</span>speaker,
</span></span><span style="display:flex;"><span>            do_sample<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,  <span style="color:#75715e"># 启用采样模式以使用temperature/top_p</span>
</span></span><span style="display:flex;"><span>            temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">0.8</span>,  <span style="color:#75715e"># 控制随机性（0.5-1.0较自然）</span>
</span></span><span style="display:flex;"><span>            top_p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.95</span>,       <span style="color:#75715e"># 核采样参数</span>
</span></span><span style="display:flex;"><span>            max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>,  <span style="color:#75715e"># 控制语音时长（约15秒）</span>
</span></span><span style="display:flex;"><span>            use_audio_in_video<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 7. 保存结果</span>
</span></span><span style="display:flex;"><span>    os<span style="color:#f92672">.</span>makedirs(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>dirname(output_audio_path), exist_ok<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    sf<span style="color:#f92672">.</span>write(output_audio_path, audio<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>cpu()<span style="color:#f92672">.</span>numpy(), samplerate<span style="color:#f92672">=</span><span style="color:#ae81ff">24000</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;✅ 生成完成：</span><span style="color:#e6db74">{</span>output_audio_path<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;📄 生成文本：</span><span style="color:#e6db74">{</span>processor<span style="color:#f92672">.</span>batch_decode(text_ids, skip_special_tokens<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)[<span style="color:#ae81ff">0</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 示例输入（可替换为任意文本）</span>
</span></span><span style="display:flex;"><span>    input_text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;你好，这是Qwen2.5-Omni的文本转语音示例。祝你使用愉快！&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 调用函数（指定输出路径和语音类型）</span>
</span></span><span style="display:flex;"><span>    text_to_speech(
</span></span><span style="display:flex;"><span>        input_text,
</span></span><span style="display:flex;"><span>        output_audio_path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;output/hello_qwen.wav&#34;</span>,
</span></span><span style="display:flex;"><span>        speaker<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Chelsie&#34;</span>  <span style="color:#75715e"># 可选&#34;Ethan&#34;</span>
</span></span><span style="display:flex;"><span>    )
</span></span></code></pre></div><p>运行脚本</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>uv run main.py
</span></span></code></pre></div>]]></content:encoded></item></channel></rss>