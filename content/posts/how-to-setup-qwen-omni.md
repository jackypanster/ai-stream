+++
title = 'Qwen2.5-Omni-7B æ–‡æœ¬è½¬è¯­éŸ³éƒ¨ç½²æŒ‡å—'
date = 2025-05-19T10:54:50+08:00
draft = false
tags = ["qwen", "omni"]
categories = ["LLM"]
+++

æœ¬è„šæœ¬åŸºäº Qwen2.5-Omni-7B å¤šæ¨¡æ€æ¨¡å‹å®ç°æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰åŠŸèƒ½ï¼Œæ”¯æŒç”Ÿæˆè‡ªç„¶æµç•…çš„ä¸­æ–‡ / è‹±æ–‡è¯­éŸ³ï¼Œå¹¶æä¾›ä¸¤ç§è¯­éŸ³ç±»å‹ï¼ˆå¥³æ€§ â€œChelsieâ€ã€ç”·æ€§ â€œEthanâ€ï¼‰ã€‚è„šæœ¬å¯å°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºéŸ³é¢‘æ–‡ä»¶ï¼ˆ.wavæ ¼å¼ï¼‰ï¼Œé€‚ç”¨äºè¯­éŸ³åŠ©æ‰‹ã€å†…å®¹åˆ›ä½œã€æ— éšœç¢æœåŠ¡ç­‰åœºæ™¯ã€‚

å®‰è£…ä¾èµ–åº“
```bash
uv init
uv add git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview
uv add accelerate
uv add qwen-omni-utils[decord]
uv add soundfile
uv add torchvision
uv sync
```

å®Œæ•´è„šæœ¬ä»£ç ï¼ˆmain_text2audio.pyï¼‰
```python
import os
import soundfile as sf
import torch
from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor
from qwen_omni_utils import process_mm_info
from transformers import AutoConfig

def text_to_speech(
    text_input: str,
    output_audio_path: str = "output/test_audio.wav",
    speaker: str = "Chelsie",
    model_path: str = "/home/llm/model/qwen/Omni/"  # æ”¹ä¸ºæœ¬åœ°è·¯å¾„æˆ–è¿œç¨‹è·¯å¾„
):
    """
    æ–‡æœ¬è½¬è¯­éŸ³æ ¸å¿ƒå‡½æ•°
    :param text_input: è¾“å…¥æ–‡æœ¬ï¼ˆæ”¯æŒä¸­æ–‡/è‹±æ–‡ï¼‰
    :param output_audio_path: éŸ³é¢‘è¾“å‡ºè·¯å¾„ï¼ˆå«æ–‡ä»¶åï¼‰
    :param speaker: è¯­éŸ³ç±»å‹ï¼ˆ"Chelsie"å¥³æ€§/"Ethan"ç”·æ€§ï¼‰
    :param model_path: æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°/è¿œç¨‹ï¼‰
    """
    # 1. åŠ è½½æ¨¡å‹é…ç½®ï¼ˆä¿®å¤ROPEå‚æ•°å…¼å®¹æ€§ï¼‰
    config = AutoConfig.from_pretrained(model_path, local_files_only=True)
    if hasattr(config, "rope_scaling") and "mrope_section" in config.rope_scaling:
        config.rope_scaling.pop("mrope_section")
    
    # 2. åŠ è½½æ¨¡å‹ï¼ˆæ”¯æŒGPUè‡ªåŠ¨åˆ†é…ï¼‰
    model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
        model_path,
        config=config,
        torch_dtype="auto",
        device_map="auto",
        local_files_only=(model_path != "Qwen/Qwen2.5-Omni-7B")
    )
    processor = Qwen2_5OmniProcessor.from_pretrained(model_path)
    
    # 3. ç³»ç»Ÿæç¤ºï¼ˆå¿…é¡»åŒ…å«è¯­éŸ³ç”Ÿæˆèƒ½åŠ›å£°æ˜ï¼‰
    system_prompt = [
        {
            "role": "system",
            "content": [
                {"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}
            ]
        }
    ]
    
    # 4. æ„å»ºå¯¹è¯ï¼ˆçº¯æ–‡æœ¬è¾“å…¥ï¼‰
    conversation = system_prompt + [
        {"role": "user", "content": [{"type": "text", "text": text_input}]}
    ]
    
    # 5. å¤„ç†è¾“å…¥æ•°æ®
    text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
    audios, images, videos = process_mm_info(conversation, use_audio_in_video=False)
    
    # 6. ç”Ÿæˆè¯­éŸ³
    inputs = processor(
        text=text, audio=audios, images=images, videos=videos,
        return_tensors="pt", padding=True, use_audio_in_video=False
    ).to(model.device, model.dtype)
    
    with torch.no_grad():
        text_ids, audio = model.generate(
            **inputs,
            speaker=speaker,
            do_sample=True,  # å¯ç”¨é‡‡æ ·æ¨¡å¼ä»¥ä½¿ç”¨temperature/top_p
            temperature=0.8,  # æ§åˆ¶éšæœºæ€§ï¼ˆ0.5-1.0è¾ƒè‡ªç„¶ï¼‰
            top_p=0.95,       # æ ¸é‡‡æ ·å‚æ•°
            max_new_tokens=1024,  # æ§åˆ¶è¯­éŸ³æ—¶é•¿ï¼ˆçº¦15ç§’ï¼‰
            use_audio_in_video=False
        )
    
    # 7. ä¿å­˜ç»“æœ
    os.makedirs(os.path.dirname(output_audio_path), exist_ok=True)
    sf.write(output_audio_path, audio.reshape(-1).cpu().numpy(), samplerate=24000)
    print(f"âœ… ç”Ÿæˆå®Œæˆï¼š{output_audio_path}")
    print(f"ğŸ“„ ç”Ÿæˆæ–‡æœ¬ï¼š{processor.batch_decode(text_ids, skip_special_tokens=True)[0]}")

if __name__ == "__main__":
    # ç¤ºä¾‹è¾“å…¥ï¼ˆå¯æ›¿æ¢ä¸ºä»»æ„æ–‡æœ¬ï¼‰
    input_text = "ä½ å¥½ï¼Œè¿™æ˜¯Qwen2.5-Omniçš„æ–‡æœ¬è½¬è¯­éŸ³ç¤ºä¾‹ã€‚ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼"
    
    # è°ƒç”¨å‡½æ•°ï¼ˆæŒ‡å®šè¾“å‡ºè·¯å¾„å’Œè¯­éŸ³ç±»å‹ï¼‰
    text_to_speech(
        input_text,
        output_audio_path="output/hello_qwen.wav",
        speaker="Chelsie"  # å¯é€‰"Ethan"
    )
```

è¿è¡Œè„šæœ¬
```bash
uv run main.py
```