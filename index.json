[{"content":"Qwen3-30B 技术优化实践（二）：思考模式控制与性能提升 本文是《从32K到131K：Qwen3-30B大模型上下文扩展实践》的续篇，聚焦于模型性能调优特别是思考模式（reasoning mode）控制的技术细节与实践经验。\n在前文中，我们详细介绍了如何使用YaRN技术将Qwen3-30B的上下文长度从32K扩展到131K。今天，我们将深入探讨另一个关键优化维度：思考模式控制及其对性能的影响。通过一系列实验和调优，我们发现禁用思考模式可以显著提升模型响应速度和内存效率，特别适合编程和直接输出类任务场景。\n🔍 思考模式（Reasoning Mode）解析 什么是思考模式？ 思考模式（Reasoning Mode，也称为Thinking Mode）是Qwen3系列模型的一个特性，让模型能够生成中间思考步骤，这些步骤被包含在\u0026lt;think\u0026gt;...\u0026lt;/think\u0026gt;标签内。理论上，这种\u0026quot;思考过程\u0026quot;有助于模型进行更复杂的推理，但同时也引入了额外的计算和内存开销。\n在默认配置下，Qwen3模型会启用思考模式，产生类似以下的输出：\n\u0026lt;think\u0026gt; 首先，我需要分析用户的问题：如何实现一个简单的文件读写功能。 我应该使用Python的内置文件操作功能。 基本步骤应该是： 1. 打开文件（可以使用with语句自动管理资源） 2. 读取或写入内容 3. 确保文件正确关闭 \u0026lt;/think\u0026gt; 以下是一个简单的Python文件读写示例： ```python # 写入文件 with open(\u0026#39;example.txt\u0026#39;, \u0026#39;w\u0026#39;) as file: file.write(\u0026#39;Hello, World!\u0026#39;) # 读取文件 with open(\u0026#39;example.txt\u0026#39;, \u0026#39;r\u0026#39;) as file: content = file.read() print(content) ### 思考模式实现机制 vLLM部署Qwen3模型时，思考模式通过两种方式实现控制： 1. **服务器级控制**：通过部署参数`--enable-reasoning`和`--reasoning-parser deepseek_r1`启用 2. **API级控制**：通过API调用中的`chat_template_kwargs`参数或`enable_thinking`参数动态控制 我们的发现是，**仅删除服务器级别的参数并不足够完全禁用思考模式**，模型在某些情况下仍会产生思考过程。更彻底的解决方案是使用自定义聊天模板。 ## 💡 禁用思考模式的技术实现 ### 自定义聊天模板方案 经过研究Qwen官方文档和实验，我们发现使用自定义聊天模板是完全禁用思考模式的最可靠方法。我们创建了一个名为`qwen3_nonthinking.jinja`的模板文件： ```jinja {% if messages %} {% set loop_messages = messages %} {% else %} {% set loop_messages = [{\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;\u0026#39;}] %} {% endif %} {% for message in loop_messages %} {% if message[\u0026#39;role\u0026#39;] == \u0026#39;user\u0026#39; %} \u0026lt;|im_start|\u0026gt;user {{ message[\u0026#39;content\u0026#39;] }}\u0026lt;|im_end|\u0026gt; {% elif message[\u0026#39;role\u0026#39;] == \u0026#39;assistant\u0026#39; %} \u0026lt;|im_start|\u0026gt;assistant {{ message[\u0026#39;content\u0026#39;] }}\u0026lt;|im_end|\u0026gt; {% elif message[\u0026#39;role\u0026#39;] == \u0026#39;system\u0026#39; %} \u0026lt;|im_start|\u0026gt;system {{ message[\u0026#39;content\u0026#39;] }}\u0026lt;|im_end|\u0026gt; {% endif %} {% endfor %} \u0026lt;|im_start|\u0026gt;assistant {% if add_generation_prompt is defined and add_generation_prompt %}{{ generation_prompt }}{% endif %} 这个模板的关键点是移除了所有与思考模式相关的标签和处理逻辑，确保模型无法生成\u0026lt;think\u0026gt;...\u0026lt;/think\u0026gt;块，即使API请求中尝试启用思考模式。\n部署脚本修改 为了使用这个模板，我们修改了部署脚本，添加了以下关键参数：\n# 重要：1. 挂载工作目录使模板文件可访问 -v /home/llm/workspace/deploy-qwen:/workspace/deploy-qwen \\ # 重要：2. 使用自定义模板彻底禁用思考模式 --chat-template /workspace/deploy-qwen/qwen3_nonthinking.jinja 同时，我们在脚本中添加了详细注释，便于在不同场景下快速切换模式。\n📊 性能提升测量与分析 实测性能数据 我们通过实际部署测试，观察到禁用思考模式带来的性能提升：\n指标 启用思考模式 禁用思考模式 提升比例 生成速度 ~12-14 tokens/s ~17-19 tokens/s +15-20% GPU KV缓存使用率 ~12-15% ~8-9% -30-40% 内存占用 较高 较低 -20-25% 输出一致性 出现推理过程 直接输出结果 更加简洁 一个典型的性能日志片段显示：\nINFO 06-03 23:06:14 [metrics.py:486] Avg prompt throughput: 2315.5 tokens/s, Avg generation throughput: 12.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.7%, CPU KV cache usage: 0.0%. INFO 06-03 23:06:19 [metrics.py:486] Avg prompt throughput: 506.3 tokens/s, Avg generation throughput: 17.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.7%, CPU KV cache usage: 0.0%. 性能提升原理分析 禁用思考模式带来性能提升的主要原因包括：\n计算负载减少：不再生成中间思考步骤，减少了总体需要生成的token数量\n注意力计算简化：推理过程通常需要模型在更大的上下文窗口中进行注意力计算，禁用后注意力机制更聚焦\n内存使用优化：无需为思考过程分配额外的KV缓存空间，特别是在131K超长上下文模式下，这一优势更为显著\n内部状态跟踪简化：模型不再需要维护和管理额外的思考状态，减少了内部状态转换的复杂度\n🔧 适用场景与参数调优 最适合禁用思考模式的场景 代码生成任务：直接输出代码而非详细解释过程 简洁问答：需要简短直接答案的场景 API集成：作为后端服务集成到其他系统时 高并发服务：需要处理大量请求时 内存受限环境：硬件资源相对有限时 编程任务最佳参数组合 基于我们的测试，禁用思考模式后，编程任务推荐以下参数设置：\n{ \u0026#34;temperature\u0026#34;: 0.2, \u0026#34;top_p\u0026#34;: 0.6, \u0026#34;top_k\u0026#34;: 50, \u0026#34;presence_penalty\u0026#34;: 0.0, \u0026#34;frequency_penalty\u0026#34;: 0.0 } 这组参数提供了高确定性和一致性，使编码输出更可靠。\n🔄 模式切换方法 我们在部署脚本中提供了详细的切换指南：\n保持禁用思考模式（默认配置） 保留--chat-template参数 删除--enable-reasoning和--reasoning-parser参数 启用思考模式 删除--chat-template参数 添加以下参数： --enable-reasoning \\ --reasoning-parser deepseek_r1 应用更改 docker stop coder \u0026amp;\u0026amp; docker rm coder \u0026amp;\u0026amp; ./deploy-32k.sh # 或 ./deploy-131k.sh 🧩 与YaRN扩展的协同优化 禁用思考模式与YaRN上下文扩展技术结合使用时，能带来更全面的性能和能力提升：\n内存效率倍增：在超长上下文场景下，禁用思考模式能显著降低YaRN扩展带来的额外内存压力\n扩展潜力提高：理论上，通过禁用思考模式，YaRN因子可以进一步提高（例如从4.0到4.5或更高），实现更长上下文\n响应速度提升：特别是在处理大型代码库或长文档时，禁用思考模式提供了更快的token生成速度\n🚀 未来优化方向 基于我们的经验，推荐以下优化方向进一步提升性能：\n启发式路由：构建智能路由层，根据输入类型自动选择启用或禁用思考模式\n场景自适应：开发能根据输入动态调整思考模式的混合策略\nPrompt工程优化：研究特定prompt模式，在禁用思考模式的同时保持高质量推理能力\n量化与思考模式协同优化：探索将4位或8位量化与思考模式禁用结合，进一步提升性能\n🏁 结论 通过深入研究和实践，我们证明了对Qwen3-30B模型思考模式的控制是一种效果显著的性能优化技术。禁用思考模式能带来15-20%的速度提升和更高的内存效率，特别适合编程任务和需要直接输出的场景。\n这种技术不需要模型微调或复杂的GPU优化，仅通过模板和配置修改就能实现，是一种低成本、高收益的优化方案。结合YaRN上下文扩展，我们能够构建一个兼具高性能和强大能力的大模型服务。\n作者说明：本文所有测试均基于Qwen3-30B-A3B模型在4×NVIDIA GPU上使用vLLM v0.8.5进行，具体硬件环境为4×GPU(每卡22GB VRAM)，512GB RAM，56核CPU，2TB SSD。实际性能可能因硬件配置、模型版本和工作负载特性而有所不同。\n","permalink":"https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-part2/","summary":"\u003ch1 id=\"qwen3-30b-技术优化实践二思考模式控制与性能提升\"\u003eQwen3-30B 技术优化实践（二）：思考模式控制与性能提升\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e本文是\u003ca href=\"blog-post.md\"\u003e《从32K到131K：Qwen3-30B大模型上下文扩展实践》\u003c/a\u003e的续篇，聚焦于模型性能调优特别是思考模式（reasoning mode）控制的技术细节与实践经验。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e在前文中，我们详细介绍了如何使用YaRN技术将Qwen3-30B的上下文长度从32K扩展到131K。今天，我们将深入探讨另一个关键优化维度：\u003cstrong\u003e思考模式控制\u003c/strong\u003e及其对性能的影响。通过一系列实验和调优，我们发现禁用思考模式可以显著提升模型响应速度和内存效率，特别适合编程和直接输出类任务场景。\u003c/p\u003e\n\u003ch2 id=\"-思考模式reasoning-mode解析\"\u003e🔍 思考模式（Reasoning Mode）解析\u003c/h2\u003e\n\u003ch3 id=\"什么是思考模式\"\u003e什么是思考模式？\u003c/h3\u003e\n\u003cp\u003e思考模式（Reasoning Mode，也称为Thinking Mode）是Qwen3系列模型的一个特性，让模型能够生成中间思考步骤，这些步骤被包含在\u003ccode\u003e\u0026lt;think\u0026gt;...\u0026lt;/think\u0026gt;\u003c/code\u003e标签内。理论上，这种\u0026quot;思考过程\u0026quot;有助于模型进行更复杂的推理，但同时也引入了额外的计算和内存开销。\u003c/p\u003e\n\u003cp\u003e在默认配置下，Qwen3模型会启用思考模式，产生类似以下的输出：\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u0026lt;think\u0026gt;\n首先，我需要分析用户的问题：如何实现一个简单的文件读写功能。\n我应该使用Python的内置文件操作功能。\n基本步骤应该是：\n1. 打开文件（可以使用with语句自动管理资源）\n2. 读取或写入内容\n3. 确保文件正确关闭\n\u0026lt;/think\u0026gt;\n\n以下是一个简单的Python文件读写示例：\n\n```python\n# 写入文件\nwith open(\u0026#39;example.txt\u0026#39;, \u0026#39;w\u0026#39;) as file:\n    file.write(\u0026#39;Hello, World!\u0026#39;)\n\n# 读取文件\nwith open(\u0026#39;example.txt\u0026#39;, \u0026#39;r\u0026#39;) as file:\n    content = file.read()\n    print(content)\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\n### 思考模式实现机制\n\nvLLM部署Qwen3模型时，思考模式通过两种方式实现控制：\n\n1. **服务器级控制**：通过部署参数`--enable-reasoning`和`--reasoning-parser deepseek_r1`启用\n   \n2. **API级控制**：通过API调用中的`chat_template_kwargs`参数或`enable_thinking`参数动态控制\n\n我们的发现是，**仅删除服务器级别的参数并不足够完全禁用思考模式**，模型在某些情况下仍会产生思考过程。更彻底的解决方案是使用自定义聊天模板。\n\n## 💡 禁用思考模式的技术实现\n\n### 自定义聊天模板方案\n\n经过研究Qwen官方文档和实验，我们发现使用自定义聊天模板是完全禁用思考模式的最可靠方法。我们创建了一个名为`qwen3_nonthinking.jinja`的模板文件：\n\n```jinja\n{% if messages %}\n{% set loop_messages = messages %}\n{% else %}\n{% set loop_messages = [{\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;\u0026#39;}] %}\n{% endif %}\n\n{% for message in loop_messages %}\n{% if message[\u0026#39;role\u0026#39;] == \u0026#39;user\u0026#39; %}\n\u0026lt;|im_start|\u0026gt;user\n{{ message[\u0026#39;content\u0026#39;] }}\u0026lt;|im_end|\u0026gt;\n{% elif message[\u0026#39;role\u0026#39;] == \u0026#39;assistant\u0026#39; %}\n\u0026lt;|im_start|\u0026gt;assistant\n{{ message[\u0026#39;content\u0026#39;] }}\u0026lt;|im_end|\u0026gt;\n{% elif message[\u0026#39;role\u0026#39;] == \u0026#39;system\u0026#39; %}\n\u0026lt;|im_start|\u0026gt;system\n{{ message[\u0026#39;content\u0026#39;] }}\u0026lt;|im_end|\u0026gt;\n{% endif %}\n{% endfor %}\n\u0026lt;|im_start|\u0026gt;assistant\n{% if add_generation_prompt is defined and add_generation_prompt %}{{ generation_prompt }}{% endif %}\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e这个模板的关键点是\u003cstrong\u003e移除了所有与思考模式相关的标签和处理逻辑\u003c/strong\u003e，确保模型无法生成\u003ccode\u003e\u0026lt;think\u0026gt;...\u0026lt;/think\u0026gt;\u003c/code\u003e块，即使API请求中尝试启用思考模式。\u003c/p\u003e","title":"Qwen3-30B 技术优化实践（二）：思考模式控制与15-20%性能提升"},{"content":"高性能部署Qwen3-30B：vLLM优化实践指南 📋 概述 本文详细介绍如何使用vLLM高效部署Qwen3-30B-A3B模型，实现32K上下文窗口和OpenAI兼容API，适用于生产环境。通过精细调整部署参数，我们能够在有限的GPU资源下最大化模型性能。\n🖥️ 系统要求 硬件配置 4块NVIDIA GPU (每块22GB显存，总计88GB) 512GB系统内存 2TB SSD存储 56核CPU 软件环境 Ubuntu 24.04 NVIDIA驱动 550.144.03 CUDA 12.4 Docker + NVIDIA Container Toolkit 🧠 模型与架构 Qwen3-30B-A3B是阿里云发布的通用大语言模型，具有以下特点：\n30B参数量 原生支持32K上下文长度 支持思考模式(Chain-of-Thought) 优异的多语言与代码能力 我们使用vLLM作为推理引擎，主要基于以下考量：\n高效内存管理：通过PagedAttention技术优化KV缓存 张量并行：自动跨多GPU分布模型权重 OpenAI兼容API：直接替代OpenAI API，无需修改现有应用 动态批处理：自动批处理多请求，提高吞吐量 🐳 部署脚本 以下是我们用于部署的Docker命令，经过精心调优以平衡性能与资源利用：\ndocker run -d \\ --runtime=nvidia \\ --gpus=all \\ --name coder \\ -v /home/llm/model/qwen/qwen3-30b-a3b:/qwen/qwen3-30b-a3b \\ -p 8000:8000 \\ --cpuset-cpus 0-55 \\ --ulimit memlock=-1 \\ --ulimit stack=67108864 \\ --restart always \\ --ipc=host \\ vllm/vllm-openai:v0.8.5 \\ --model /qwen/qwen3-30b-a3b \\ --served-model-name coder \\ --tensor-parallel-size 4 \\ --dtype half \\ --max-model-len 32768 \\ --max-num-batched-tokens 4096 \\ --gpu-memory-utilization 0.93 \\ --block-size 32 \\ --enable-chunked-prefill \\ --swap-space 16 \\ --tokenizer-pool-size 56 \\ --disable-custom-all-reduce 🔧 参数详解与优化策略 Docker容器配置 参数 值 作用 --runtime=nvidia 启用NVIDIA容器运行时 --gpus=all 将所有GPU暴露给容器 --cpuset-cpus 0-55 限制容器使用0-55号CPU核心 --ulimit memlock=-1 移除内存锁定限制，提高性能 --ipc=host 使用主机IPC命名空间，对共享内存很重要 vLLM引擎配置 1. 张量并行策略 --tensor-parallel-size 4 我们使用4路张量并行，将模型分布在4块GPU上。这是基于实验得出的最佳配置 - 在我们的硬件上，每块22GB显存的GPU无法单独加载完整的30B模型。\n2. 内存优化 --dtype half --gpu-memory-utilization 0.93 --block-size 32 --swap-space 16 half精度(FP16)相比bfloat16能进一步节省内存，且在我们的场景中精度损失可接受 GPU内存利用率93%留出一定缓冲空间防止OOM错误 KV缓存块大小设为32，平衡内存使用与计算效率 16GB的CPU-GPU交换空间支持处理超长序列 3. 上下文长度与批处理 --max-model-len 32768 --max-num-batched-tokens 4096 --enable-chunked-prefill 我们将上下文长度从默认的16K增加到32K，以支持更长输入和输出。为了平衡资源使用，相应地将批处理令牌数从8192减少到4096，这是一个经过测试的合理折中方案。\n启用分块预填充(chunked-prefill)对于处理长上下文尤为重要，它将长序列分解为更小的块进行处理，减少显存峰值使用。\n4. 其他性能调优 --tokenizer-pool-size 56 --disable-custom-all-reduce 令牌化工作池大小与CPU核心数匹配，优化并行处理能力 禁用自定义all-reduce操作，解决某些硬件配置上的兼容性问题 📊 性能分析 部署后，我们可以通过docker logs -f coder查看服务状态，关键性能指标如下：\nINFO 06-03 02:01:19 [worker.py:287] the current vLLM instance can use total_gpu_memory (21.66GiB) x gpu_memory_utilization (0.93) = 20.15GiB INFO 06-03 02:01:19 [worker.py:287] model weights take 14.25GiB; non_torch_memory takes 0.20GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 4.30GiB. INFO 06-03 02:01:20 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 5.73x 这表明：\n每个GPU使用约20.15GB内存 模型权重占用14.25GB 对于32K令牌请求，系统可以并发处理5.73倍的请求 在我们的生产环境中，这个配置能够处理每分钟约15-20个并发对话，满足中小型应用需求。\n📝 API使用示例 服务启动后，可以通过OpenAI兼容的API在本地端口8000访问：\ncurl http://localhost:8000/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;coder\u0026#34;, \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;请解释一下量子计算的基本原理\u0026#34;} ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: 2000 }\u0026#39; 使用Python客户端：\nfrom openai import OpenAI client = OpenAI( base_url=\u0026#34;http://localhost:8000/v1\u0026#34;, api_key=\u0026#34;not-needed\u0026#34; # vLLM不要求API密钥 ) response = client.chat.completions.create( model=\u0026#34;coder\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;写一个Python函数计算斐波那契数列\u0026#34;} ], temperature=0.7, max_tokens=1000 ) print(response.choices[0].message.content) 🚀 扩展到更长上下文 Qwen3-30B原生支持32K上下文，但如需扩展到更长上下文(如131K令牌)，可以使用YaRN技术，通过在vLLM参数中添加：\n--rope-scaling \u0026#39;{\u0026#34;rope_type\u0026#34;:\u0026#34;yarn\u0026#34;,\u0026#34;factor\u0026#34;:4.0,\u0026#34;original_max_position_embeddings\u0026#34;:32768}\u0026#39; \\ --max-model-len 131072 注意这会增加内存使用，可能需要进一步调整其他参数以平衡资源。\n🔍 常见问题排查 OOM错误：减小gpu-memory-utilization或max-num-batched-tokens 推理速度慢：检查GPU利用率，考虑增加batch大小或减小max-model-len CUDA图捕获失败：添加--enforce-eager参数禁用CUDA图优化 📈 未来优化方向 探索使用FlashAttention-2加速注意力计算 尝试AWQ/GPTQ量化技术降低内存使用 配置LLM Router实现多模型负载均衡 🔚 总结 通过精细调优vLLM部署参数，我们成功在有限硬件资源下部署了Qwen3-30B模型，实现了32K上下文窗口的高性能推理服务。这套配置在生产环境中表现稳定，为各类应用提供强大的AI能力支持。\n","permalink":"https://jackypanster.github.io/ai-stream/posts/deploy-qwen3/","summary":"\u003ch1 id=\"高性能部署qwen3-30bvllm优化实践指南\"\u003e高性能部署Qwen3-30B：vLLM优化实践指南\u003c/h1\u003e\n\u003ch2 id=\"-概述\"\u003e📋 概述\u003c/h2\u003e\n\u003cp\u003e本文详细介绍如何使用vLLM高效部署Qwen3-30B-A3B模型，实现32K上下文窗口和OpenAI兼容API，适用于生产环境。通过精细调整部署参数，我们能够在有限的GPU资源下最大化模型性能。\u003c/p\u003e\n\u003ch2 id=\"-系统要求\"\u003e🖥️ 系统要求\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e硬件配置\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e4块NVIDIA GPU (每块22GB显存，总计88GB)\u003c/li\u003e\n\u003cli\u003e512GB系统内存\u003c/li\u003e\n\u003cli\u003e2TB SSD存储\u003c/li\u003e\n\u003cli\u003e56核CPU\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e软件环境\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUbuntu 24.04\u003c/li\u003e\n\u003cli\u003eNVIDIA驱动 550.144.03\u003c/li\u003e\n\u003cli\u003eCUDA 12.4\u003c/li\u003e\n\u003cli\u003eDocker + NVIDIA Container Toolkit\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"-模型与架构\"\u003e🧠 模型与架构\u003c/h2\u003e\n\u003cp\u003eQwen3-30B-A3B是阿里云发布的通用大语言模型，具有以下特点：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e30B参数量\u003c/li\u003e\n\u003cli\u003e原生支持32K上下文长度\u003c/li\u003e\n\u003cli\u003e支持思考模式(Chain-of-Thought)\u003c/li\u003e\n\u003cli\u003e优异的多语言与代码能力\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e我们使用vLLM作为推理引擎，主要基于以下考量：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e高效内存管理\u003c/strong\u003e：通过PagedAttention技术优化KV缓存\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e张量并行\u003c/strong\u003e：自动跨多GPU分布模型权重\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOpenAI兼容API\u003c/strong\u003e：直接替代OpenAI API，无需修改现有应用\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e动态批处理\u003c/strong\u003e：自动批处理多请求，提高吞吐量\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"-部署脚本\"\u003e🐳 部署脚本\u003c/h2\u003e\n\u003cp\u003e以下是我们用于部署的Docker命令，经过精心调优以平衡性能与资源利用：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edocker run -d \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --runtime\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003envidia \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --gpus\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eall \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --name coder \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  -v /home/llm/model/qwen/qwen3-30b-a3b:/qwen/qwen3-30b-a3b \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  -p 8000:8000 \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --cpuset-cpus 0-55 \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --ulimit memlock\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e-1 \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --ulimit stack\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e67108864\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --restart always \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --ipc\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003ehost \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  vllm/vllm-openai:v0.8.5 \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --model /qwen/qwen3-30b-a3b \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --served-model-name coder \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --tensor-parallel-size \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --dtype half \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --max-model-len \u003cspan style=\"color:#ae81ff\"\u003e32768\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --max-num-batched-tokens \u003cspan style=\"color:#ae81ff\"\u003e4096\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --gpu-memory-utilization 0.93 \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --block-size \u003cspan style=\"color:#ae81ff\"\u003e32\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --enable-chunked-prefill \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --swap-space \u003cspan style=\"color:#ae81ff\"\u003e16\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --tokenizer-pool-size \u003cspan style=\"color:#ae81ff\"\u003e56\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --disable-custom-all-reduce\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"-参数详解与优化策略\"\u003e🔧 参数详解与优化策略\u003c/h2\u003e\n\u003ch3 id=\"docker容器配置\"\u003eDocker容器配置\u003c/h3\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e参数\u003c/th\u003e\n          \u003cth\u003e值\u003c/th\u003e\n          \u003cth\u003e作用\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003e--runtime=nvidia\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n          \u003ctd\u003e启用NVIDIA容器运行时\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003e--gpus=all\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n          \u003ctd\u003e将所有GPU暴露给容器\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003e--cpuset-cpus\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003e0-55\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e限制容器使用0-55号CPU核心\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003e--ulimit memlock=-1\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n          \u003ctd\u003e移除内存锁定限制，提高性能\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003e--ipc=host\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n          \u003ctd\u003e使用主机IPC命名空间，对共享内存很重要\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"vllm引擎配置\"\u003evLLM引擎配置\u003c/h3\u003e\n\u003ch4 id=\"1-张量并行策略\"\u003e1. 张量并行策略\u003c/h4\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e--tensor-parallel-size 4\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e我们使用4路张量并行，将模型分布在4块GPU上。这是基于实验得出的最佳配置 - 在我们的硬件上，每块22GB显存的GPU无法单独加载完整的30B模型。\u003c/p\u003e","title":"高性能部署Qwen3-30B：vLLM优化实践指南"},{"content":"AI辅助编程实战：从需求到实现的完整对话记录 引言 本文真实还原了我与AI助手协作实现Docker镜像大小监控功能的全过程。通过这次实践，可以看到AI不仅能辅助开发者梳理需求、设计方案，还能在代码实现、文档编写等环节提供高效帮助。整个流程自然流畅，极大提升了开发体验。\n1. 需求分析与方案设计 初始需求描述 用户： 我们需要在Jenkins共享库中增加Docker镜像大小监控功能，当镜像超过4GB时发出警告。请帮忙分析一下实现思路。 AI的初步分析与建议 AI： 让我先看看项目的结构…… 这是一个Jenkins共享库项目，核心代码都在`vars/`目录。针对镜像大小监控，我建议直接在`buildDocker`方法里加逻辑： 1. 镜像构建完成后用`du`命令获取大小 2. 与4GB阈值比较 3. 超过阈值就输出警告 4. 相关指标写日志，方便后续分析 这个方案您觉得合适吗？有无补充？ 需求确认与细化 用户： 这个思路可以，不过还需要： 1. 阈值要可配置，默认4GB 2. 日志里要有镜像名、大小、时间戳等详细信息 3. 输出优化建议 另外，异常处理也要考虑得更优雅些。 2. 代码实现 初始代码实现 AI： 明白，我来实现。会在`buildDocker`方法里： 1. 用`getEnvOrDefault`支持阈值配置 2. 详细记录指标 3. 加强错误处理 4. 输出优化建议 实现逻辑您看还有什么要调整的吗？ 代码审查与优化 用户： 整体不错，不过警告要改成抛异常直接中断构建。另外，能不能顺便加上基础镜像大小检测？ 异常处理优化 AI： 明白您的需求，代码我会这样优化： 1. 超标直接抛异常中断流水线 2. 增加基础镜像大小检测 3. 明确区分业务异常（比如镜像过大）和技术异常（比如命令失败） 基础镜像检测这块，我建议： - 解析Dockerfile的FROM - 拉取并检测基础镜像大小 - 超限就给出alpine等轻量镜像建议 这样处理您觉得OK吗？ 3. 测试与验证 测试方案讨论 用户： 怎么验证这个功能？ AI： 建议这样测试： 1. 镜像大小正常 2. 恰好等于阈值 3. 超过阈值 4. Dockerfile解析异常 也可以临时把阈值调小，快速验证告警逻辑。 4. 文档与沟通 变更日志生成 AI： 我已为您生成详细CHANGELOG，包括： - 功能说明 - 技术实现细节 - 配置参数说明 - 后续优化计划 如需调整，随时告诉我。 团队通知 用户： 要发个通知给团队，说明我们要限制镜像大小。 AI： 建议分阶段推进： 1. 先监控和通知 2. 再进入警告期 3. 最后强制限制 这样大家有缓冲期，可以逐步优化Dockerfile。需要帮忙起草通知吗？ 5. 经验总结 技术收获 AI辅助效率\n代码实现速度提升显著 自动生成文档节省时间 代码质量\n异常处理更完善 配置更灵活 团队协作\n文档更规范 知识传递更高效 建议与展望 建立AI辅助编码规范 完善提示工程实践 探索更多自动化场景 结语 通过这次实践，AI能陪伴开发者一起思考、决策和落地实现。期待未来AI在更多软件开发场景中发挥更大作用。\n","permalink":"https://jackypanster.github.io/ai-stream/posts/coding-with-ai/","summary":"\u003ch1 id=\"ai辅助编程实战从需求到实现的完整对话记录\"\u003eAI辅助编程实战：从需求到实现的完整对话记录\u003c/h1\u003e\n\u003ch2 id=\"引言\"\u003e引言\u003c/h2\u003e\n\u003cp\u003e本文真实还原了我与AI助手协作实现Docker镜像大小监控功能的全过程。通过这次实践，可以看到AI不仅能辅助开发者梳理需求、设计方案，还能在代码实现、文档编写等环节提供高效帮助。整个流程自然流畅，极大提升了开发体验。\u003c/p\u003e\n\u003ch2 id=\"1-需求分析与方案设计\"\u003e1. 需求分析与方案设计\u003c/h2\u003e\n\u003ch3 id=\"初始需求描述\"\u003e初始需求描述\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e用户：\n我们需要在Jenkins共享库中增加Docker镜像大小监控功能，当镜像超过4GB时发出警告。请帮忙分析一下实现思路。\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"ai的初步分析与建议\"\u003eAI的初步分析与建议\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eAI：\n让我先看看项目的结构……\n\n这是一个Jenkins共享库项目，核心代码都在`vars/`目录。针对镜像大小监控，我建议直接在`buildDocker`方法里加逻辑：\n\n1. 镜像构建完成后用`du`命令获取大小\n2. 与4GB阈值比较\n3. 超过阈值就输出警告\n4. 相关指标写日志，方便后续分析\n\n这个方案您觉得合适吗？有无补充？\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"需求确认与细化\"\u003e需求确认与细化\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e用户：\n这个思路可以，不过还需要：\n1. 阈值要可配置，默认4GB\n2. 日志里要有镜像名、大小、时间戳等详细信息\n3. 输出优化建议\n\n另外，异常处理也要考虑得更优雅些。\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"2-代码实现\"\u003e2. 代码实现\u003c/h2\u003e\n\u003ch3 id=\"初始代码实现\"\u003e初始代码实现\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eAI：\n明白，我来实现。会在`buildDocker`方法里：\n\n1. 用`getEnvOrDefault`支持阈值配置\n2. 详细记录指标\n3. 加强错误处理\n4. 输出优化建议\n\n实现逻辑您看还有什么要调整的吗？\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"代码审查与优化\"\u003e代码审查与优化\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e用户：\n整体不错，不过警告要改成抛异常直接中断构建。另外，能不能顺便加上基础镜像大小检测？\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"异常处理优化\"\u003e异常处理优化\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eAI：\n明白您的需求，代码我会这样优化：\n\n1. 超标直接抛异常中断流水线\n2. 增加基础镜像大小检测\n3. 明确区分业务异常（比如镜像过大）和技术异常（比如命令失败）\n\n基础镜像检测这块，我建议：\n- 解析Dockerfile的FROM\n- 拉取并检测基础镜像大小\n- 超限就给出alpine等轻量镜像建议\n\n这样处理您觉得OK吗？\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"3-测试与验证\"\u003e3. 测试与验证\u003c/h2\u003e\n\u003ch3 id=\"测试方案讨论\"\u003e测试方案讨论\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e用户：\n怎么验证这个功能？\n\nAI：\n建议这样测试：\n1. 镜像大小正常\n2. 恰好等于阈值\n3. 超过阈值\n4. Dockerfile解析异常\n\n也可以临时把阈值调小，快速验证告警逻辑。\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"4-文档与沟通\"\u003e4. 文档与沟通\u003c/h2\u003e\n\u003ch3 id=\"变更日志生成\"\u003e变更日志生成\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eAI：\n我已为您生成详细CHANGELOG，包括：\n- 功能说明\n- 技术实现细节\n- 配置参数说明\n- 后续优化计划\n\n如需调整，随时告诉我。\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"团队通知\"\u003e团队通知\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e用户：\n要发个通知给团队，说明我们要限制镜像大小。\n\nAI：\n建议分阶段推进：\n1. 先监控和通知\n2. 再进入警告期\n3. 最后强制限制\n\n这样大家有缓冲期，可以逐步优化Dockerfile。需要帮忙起草通知吗？\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"5-经验总结\"\u003e5. 经验总结\u003c/h2\u003e\n\u003ch3 id=\"技术收获\"\u003e技术收获\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAI辅助效率\u003c/strong\u003e\u003c/p\u003e","title":"AI辅助编程实战：从需求到实现的完整对话记录"},{"content":"基于FunAudioLLM/SenseVoiceSmall搭建高效语音转录服务的实践之路 项目概述 实现一个语音转录文本（ASR）的服务，目标是能够高效地将用户上传的音频文件转换为文字。出于中文语音的考虑，选择了来自 FunAudioLLM 的 SenseVoiceSmall 模型，它以其多语种支持、高效率以及集成的语音理解能力（如情感识别、事件检测）吸引了我。本文将详细记录从环境配置、核心功能实现到踩坑解决的全过程，并分享一些关于模型选型的思考。\n完整代码已开源在 GitHub 仓库：https://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall\n项目需求文档（prd.md）关键信息如下：\n模型: FunAudioLLM/SenseVoice (具体为 SenseVoiceSmall) 本地模型路径: /home/llm/model/iic/SenseVoiceSmall (从 ModelScope 下载) API框架: FastAPI Python环境管理: uv 环境配置 为了保持开发环境的纯净和高效，采用了 uv 来管理 Python 依赖。\n创建虚拟环境 (如果尚未创建):\nuv venv .venv source .venv/bin/activate 安装核心依赖: 初始的 requirements.txt 包含了 fastapi, uvicorn, python-multipart 等基础库。后续根据模型加载和处理的需求，逐步添加了 torch, torchaudio, numpy, transformers, sentencepiece, 以及最终解决模型加载问题的核心库 funasr。\nuv pip install -r requirements.txt 核心功能实现概览 项目结构 项目的主要结构包括：\napp/main.py: FastAPI 应用入口，定义 API 路由和应用生命周期事件（如模型加载）。 app/models/sensevoice_loader.py: 负责加载 SenseVoiceSmall 模型，采用单例模式。 app/services/asr_service.py: 封装语音处理和模型推理的核心逻辑。 app/schemas.py: 定义 API 的请求和响应数据模型 (Pydantic models)。 API 端点 关键的 API 端点设计为：\nPOST /asr_pure Content-Type: multipart/form-data Body: file (音频文件) 返回转录后的文本及处理时间。\n踩坑与解决之路：模型加载的曲折历程 在项目推进过程中，模型加载部分是遇到问题最多的地方，也是收获最多的地方。\n坑1：Hugging Face AutoClass 的 \u0026ldquo;Unrecognized model\u0026rdquo; 最初，尝试使用 Hugging Face transformers 库通用的 AutoProcessor.from_pretrained() 和 AutoModelForSpeechSeq2Seq.from_pretrained() 来加载本地的 SenseVoiceSmall 模型文件。\n# app/models/sensevoice_loader.py (早期尝试) # from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor # ... # self.processor = AutoProcessor.from_pretrained(MODEL_PATH) # self.model = AutoModelForSpeechSeq2Seq.from_pretrained(MODEL_PATH) 然而，服务启动时立即报错：\nValueError: Unrecognized model in /home/llm/model/iic/SenseVoiceSmall. Should have a model_type key in its config.json... 这个错误表明 transformers 的自动发现机制无法识别模型类型，通常是因为模型目录下的 config.json 文件缺少 model_type 字段，或者该模型需要特定的加载类。\n坑2：转向 funasr 与 trust_remote_code 的初步探索 查阅 FunAudioLLM/SenseVoice 的官方文档后发现，推荐使用 funasr 库的 AutoModel 来加载 SenseVoice 系列模型。于是调整了代码：\n添加 funasr 到 requirements.txt。 修改 SenseVoiceLoader: # app/models/sensevoice_loader.py (引入 funasr) from funasr import AutoModel # ... self.model = AutoModel( model=FUNASR_MODEL_NAME_OR_PATH, # 即本地路径 trust_remote_code=True, device=self.device ) 同时，asr_service.py 中的推理逻辑也相应调整为调用 funasr 模型对象的 .generate() 方法。 本以为这样能解决问题，但启动时又遇到了新的日志：\nLoading remote code failed: model, No module named \u0026#39;model\u0026#39; 尽管这条日志出现，但后续的 API 调用测试居然成功了！这让我非常困惑。\n坑3：remote_code 参数与 model.py 文件的“幻影” 深入研究 funasr 和 SenseVoice 的文档，注意到对于包含自定义代码（如 model.py）的模型，除了 trust_remote_code=True，有时还需要明确指定 remote_code 参数。\n我检查了 Hugging Face 仓库 FunAudioLLM/SenseVoiceSmall (https://huggingface.co/FunAudioLLM/SenseVoiceSmall/tree/main)，发现其文件列表中确实包含一个 model.py。因此，我尝试在 AutoModel 调用中加入 remote_code=\u0026quot;model.py\u0026quot;。\n# app/models/sensevoice_loader.py (尝试指定 remote_code) self.model = AutoModel( model=FUNASR_MODEL_NAME_OR_PATH, trust_remote_code=True, remote_code=\u0026#34;model.py\u0026#34;, # \u0026lt;--- 新增 device=self.device ) 结果，No module named 'model' 的错误依旧。\n解决方案：澄清 ModelScope 与 Hugging Face 的模型文件差异 本地模型 /home/llm/model/iic/SenseVoiceSmall 是从 ModelScope (https://www.modelscope.cn/models/iic/SenseVoiceSmall/files) 下载的，而非直接 clone Hugging Face 的仓库。通过 ls -al /home/llm/model/iic/SenseVoiceSmall/ 查看本地文件，发现确实没有 model.py 文件！\n这解释了为什么指定 remote_code=\u0026quot;model.py\u0026quot; 依然报错。ModelScope 提供的模型包可能与 Hugging Face 仓库中的文件结构不完全一致，特别是对于这种依赖 funasr 特定加载方式的模型。\n最终的正确配置：移除 remote_code 参数，但保留 trust_remote_code=True。\n# app/models/sensevoice_loader.py (最终正确配置) self.model = AutoModel( model=FUNASR_MODEL_NAME_OR_PATH, trust_remote_code=True, # 保留，funasr 可能仍需此权限处理 ModelScope 模型 # remote_code=\u0026#34;model.py\u0026#34;, # 移除，因为本地 ModelScope 版本无此文件 device=self.device ) 这样修改后，服务启动时仍然会打印 Loading remote code failed: model, No module named 'model'，但 API 调用完全正常！\n原因分析：funasr 在 trust_remote_code=True 时，会优先尝试加载自定义代码。如果本地模型路径（如从 ModelScope 下载的）没有 model.py，这个尝试会失败并打印日志。但随后，funasr 能够识别出这是一个有效的 ModelScope 模型路径，并转用其内部的标准加载流程成功加载模型。因此，该日志在这种情况下是良性的。\n模型对比与选型思考 在解决问题的过程中，也探讨了 FunAudioLLM/SenseVoiceSmall 与其他主流 ASR 模型的对比：\nOpenAI Whisper 系列 (如 whisper-large-v3):\n优势: 极高的准确率，强大的多语言能力，庞大的社区。 劣势: 推理速度相对较慢（尤其大模型），不直接提供情感/事件检测。 Wav2Vec2 系列:\n优势: 自监督学习典范，大量特定语言微调模型。 劣势: 基础模型功能相对单一。 SenseVoiceSmall 的核心优势 高效推理：其模型卡声称采用非自回归端到端框架，比 Whisper-Large 快15倍。这对于需要低延迟的应用至关重要。\n多任务集成：内置 ASR、LID（语种识别）、SER（情感识别）、AED（事件检测）。如果应用场景需要这些附加信息，SenseVoiceSmall 提供了一站式解决方案。\n特定语言优化：在中文、粤语等语言上表现突出。\n结论 没有绝对的“最好”，只有“最适合”。\n若追求极致准确性和最广语言覆盖，且对延迟不敏感，Whisper 仍是首选。 若对推理效率、集成的多任务语音理解（特别是情感/事件）或中文等特定场景有高要求，SenseVoiceSmall 是一个极具竞争力的选择。 目前选择的 SenseVoiceSmall，尤其是在确认了其 ModelScope 版本能够顺畅运行后，对于我的项目目标来说是一个合适的起点。\n当前状态与展望 目前，基于 FunAudioLLM/SenseVoiceSmall 和 FastAPI 的语音转录服务已成功搭建并能正确处理请求。\n$ curl -X POST \u0026#34;http://\u0026lt;your_server_ip\u0026gt;:8888/asr_pure\u0026#34; -F \u0026#34;file=@test_audio.wav\u0026#34; {\u0026#34;text\u0026#34;:\u0026#34;太好了，那接下来咱们可以试试其他功能了。比如说你想测试一下语音合成的效果怎么样，或者是看看有没有什么新的语音处理功能出来啦。😔\u0026#34;,\u0026#34;status\u0026#34;:\u0026#34;success\u0026#34;,\u0026#34;processing_time_ms\u0026#34;:503.39...} 后续可优化的方向 性能优化：进一步测试并发处理能力，考虑多 worker 配置。 错误处理与日志：完善更细致的错误捕获和日志记录。 功能扩展：如果需要，可以利用 SenseVoiceSmall 的情感识别和事件检测能力。 VAD 集成：对于长音频，考虑在 funasr.AutoModel 加载时集成 VAD (Voice Activity Detection) 功能，以实现自动分段处理，提升长音频处理的稳定性和效率。 异步处理与队列：对于高并发场景，引入消息队列和异步任务处理。 ","permalink":"https://jackypanster.github.io/ai-stream/posts/howto-use-sensevoicesmall/","summary":"\u003ch1 id=\"基于funaudiollmsensevoicesmall搭建高效语音转录服务的实践之路\"\u003e基于FunAudioLLM/SenseVoiceSmall搭建高效语音转录服务的实践之路\u003c/h1\u003e\n\u003ch2 id=\"项目概述\"\u003e项目概述\u003c/h2\u003e\n\u003cp\u003e实现一个语音转录文本（ASR）的服务，目标是能够高效地将用户上传的音频文件转换为文字。出于中文语音的考虑，选择了来自 \u003ccode\u003eFunAudioLLM\u003c/code\u003e 的 \u003ccode\u003eSenseVoiceSmall\u003c/code\u003e 模型，它以其多语种支持、高效率以及集成的语音理解能力（如情感识别、事件检测）吸引了我。本文将详细记录从环境配置、核心功能实现到踩坑解决的全过程，并分享一些关于模型选型的思考。\u003c/p\u003e\n\u003cp\u003e完整代码已开源在 GitHub 仓库：\u003ca href=\"https://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall\"\u003ehttps://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e项目需求文档（\u003ccode\u003eprd.md\u003c/code\u003e）关键信息如下：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e模型\u003c/strong\u003e: FunAudioLLM/SenseVoice (具体为 \u003ccode\u003eSenseVoiceSmall\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e本地模型路径\u003c/strong\u003e: \u003ccode\u003e/home/llm/model/iic/SenseVoiceSmall\u003c/code\u003e (从 ModelScope 下载)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAPI框架\u003c/strong\u003e: FastAPI\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePython环境管理\u003c/strong\u003e: \u003ccode\u003euv\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"环境配置\"\u003e环境配置\u003c/h2\u003e\n\u003cp\u003e为了保持开发环境的纯净和高效，采用了 \u003ccode\u003euv\u003c/code\u003e 来管理 Python 依赖。\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e创建虚拟环境\u003c/strong\u003e (如果尚未创建):\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv venv .venv\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esource .venv/bin/activate\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e安装核心依赖\u003c/strong\u003e:\n初始的 \u003ccode\u003erequirements.txt\u003c/code\u003e 包含了 \u003ccode\u003efastapi\u003c/code\u003e, \u003ccode\u003euvicorn\u003c/code\u003e, \u003ccode\u003epython-multipart\u003c/code\u003e 等基础库。后续根据模型加载和处理的需求，逐步添加了 \u003ccode\u003etorch\u003c/code\u003e, \u003ccode\u003etorchaudio\u003c/code\u003e, \u003ccode\u003enumpy\u003c/code\u003e, \u003ccode\u003etransformers\u003c/code\u003e, \u003ccode\u003esentencepiece\u003c/code\u003e, 以及最终解决模型加载问题的核心库 \u003ccode\u003efunasr\u003c/code\u003e。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv pip install -r requirements.txt\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"核心功能实现概览\"\u003e核心功能实现概览\u003c/h2\u003e\n\u003ch3 id=\"项目结构\"\u003e项目结构\u003c/h3\u003e\n\u003cp\u003e项目的主要结构包括：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eapp/main.py\u003c/code\u003e: FastAPI 应用入口，定义 API 路由和应用生命周期事件（如模型加载）。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eapp/models/sensevoice_loader.py\u003c/code\u003e: 负责加载 \u003ccode\u003eSenseVoiceSmall\u003c/code\u003e 模型，采用单例模式。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eapp/services/asr_service.py\u003c/code\u003e: 封装语音处理和模型推理的核心逻辑。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eapp/schemas.py\u003c/code\u003e: 定义 API 的请求和响应数据模型 (Pydantic models)。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"api-端点\"\u003eAPI 端点\u003c/h3\u003e\n\u003cp\u003e关键的 API 端点设计为：\u003c/p\u003e","title":"基于FunAudioLLM/SenseVoiceSmall搭建高效语音转录服务的实践之路"},{"content":"如何使用Qwen2.5-Omni实现文本转语音(TTS)和语音转文本(ASR) 项目概述 本项目基于Qwen2.5-Omni-7B模型，实现了两个核心功能：\n文本转语音（TTS）：将输入文本转换为自然流畅的语音 语音转文本（ASR）：将语音文件转换为文本，支持标准ASR和纯ASR两种模式 项目地址：https://github.com/jackypanster/qwen-omni\n环境配置 推荐使用conda管理Python环境，确保依赖安装的稳定性：\n# 创建并激活环境 conda create -n qwen-tts python=3.10 conda activate qwen-tts # 安装PyTorch（GPU版本） conda install pytorch=2.5.1 pytorch-cuda=12.1 -c pytorch -c nvidia conda install torchvision torchaudio -c pytorch # 安装其他依赖 conda install streamlit python-soundfile -c conda-forge pip install git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview pip install qwen-omni-utils 核心功能实现 1. 文本转语音（TTS） def text_to_speech(text_input, output_audio_path=\u0026#34;output/output.wav\u0026#34;, speaker=\u0026#34;Chelsie\u0026#34;): # 加载模型和处理器 model = Qwen2_5OmniForConditionalGeneration.from_pretrained( model_path, config=config, torch_dtype=\u0026#34;auto\u0026#34;, device_map=\u0026#34;auto\u0026#34; ) processor = Qwen2_5OmniProcessor.from_pretrained(model_path) # 构造对话 conversation = [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;You are Qwen...\u0026#34;}]}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: text_input}]} ] # 生成语音 with torch.no_grad(): text_ids, audio = model.generate( **inputs, speaker=speaker, do_sample=True, temperature=0.8, top_p=0.95, max_new_tokens=1024 ) 2. 语音转文本（ASR） def audio_to_text(audio_path: str) -\u0026gt; str: # 标准ASR模式 conversation = [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;你是Qwen...\u0026#34;}]}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;audio\u0026#34;, \u0026#34;audio\u0026#34;: audio_path}]} ] # 生成文本 with torch.no_grad(): text_ids = model.generate( **inputs, do_sample=False, max_new_tokens=1024, return_audio=False ) Web界面实现 使用Streamlit构建了简洁的Web界面：\n# 文本输入 text_input = st.text_area(\u0026#34;请输入要合成的文本：\u0026#34;, height=120, max_chars=200) # 发音人选择 speaker = st.selectbox(\u0026#34;请选择发音人：\u0026#34;, [\u0026#34;Chelsie\u0026#34;, \u0026#34;Ethan\u0026#34;], index=0) # 生成按钮 if st.button(\u0026#34;生成语音\u0026#34;): # 生成语音并播放 audio_path = os.path.join(OUTPUT_DIR, f\u0026#34;tts_{uuid.uuid4().hex}.wav\u0026#34;) text_to_speech(text_input, output_audio_path=audio_path, speaker=speaker) st.audio(audio_path, format=\u0026#34;audio/wav\u0026#34;) RESTful API实现 使用FastAPI构建了RESTful API接口：\n@app.post(\u0026#34;/tts\u0026#34;) async def tts(request: TTSRequest): audio_filename = f\u0026#34;tts_{uuid.uuid4().hex}.wav\u0026#34; audio_path = os.path.join(OUTPUT_DIR, audio_filename) text_to_speech(request.text, audio_path, request.speaker) return {\u0026#34;audio_url\u0026#34;: f\u0026#34;/output/{audio_filename}\u0026#34;} @app.post(\u0026#34;/asr\u0026#34;) async def asr(file: UploadFile = File(...)): # 处理上传的音频文件 audio_path = os.path.join(OUTPUT_DIR, f\u0026#34;asr_{uuid.uuid4().hex}.wav\u0026#34;) with open(audio_path, \u0026#34;wb\u0026#34;) as buffer: shutil.copyfileobj(file.file, buffer) text = audio_to_text(audio_path) return {\u0026#34;text\u0026#34;: text} 使用说明 启动Web界面： streamlit run app_text2audio.py 启动API服务： uvicorn fastapi_app:app --host 0.0.0.0 --port 8000 注意事项 模型文件较大，建议提前下载并配置好模型路径 使用conda安装依赖可以避免大多数环境问题 音频文件会保存在output目录下 API接口支持文件上传和文本转写 后续优化方向 支持更多发音人选项 优化模型加载速度 添加批量处理功能 支持更多音频格式 添加历史记录功能 参考资源 Qwen2.5-Omni-7B官方文档 Streamlit文档 FastAPI文档 ","permalink":"https://jackypanster.github.io/ai-stream/posts/how-to-use-qwen-omni-tts-asr/","summary":"\u003ch1 id=\"如何使用qwen25-omni实现文本转语音tts和语音转文本asr\"\u003e如何使用Qwen2.5-Omni实现文本转语音(TTS)和语音转文本(ASR)\u003c/h1\u003e\n\u003ch2 id=\"项目概述\"\u003e项目概述\u003c/h2\u003e\n\u003cp\u003e本项目基于Qwen2.5-Omni-7B模型，实现了两个核心功能：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e文本转语音（TTS）：将输入文本转换为自然流畅的语音\u003c/li\u003e\n\u003cli\u003e语音转文本（ASR）：将语音文件转换为文本，支持标准ASR和纯ASR两种模式\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e项目地址：\u003ca href=\"https://github.com/jackypanster/qwen-omni\"\u003ehttps://github.com/jackypanster/qwen-omni\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"环境配置\"\u003e环境配置\u003c/h2\u003e\n\u003cp\u003e推荐使用conda管理Python环境，确保依赖安装的稳定性：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 创建并激活环境\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econda create -n qwen-tts python\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e3.10\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econda activate qwen-tts\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 安装PyTorch（GPU版本）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econda install pytorch\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e2.5.1 pytorch-cuda\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e12.1 -c pytorch -c nvidia\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econda install torchvision torchaudio -c pytorch\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 安装其他依赖\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econda install streamlit python-soundfile -c conda-forge\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip install git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip install qwen-omni-utils\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"核心功能实现\"\u003e核心功能实现\u003c/h2\u003e\n\u003ch3 id=\"1-文本转语音tts\"\u003e1. 文本转语音（TTS）\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etext_to_speech\u003c/span\u003e(text_input, output_audio_path\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;output/output.wav\u0026#34;\u003c/span\u003e, speaker\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Chelsie\u0026#34;\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 加载模型和处理器\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    model \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Qwen2_5OmniForConditionalGeneration\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efrom_pretrained(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        model_path, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003econfig, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        torch_dtype\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;auto\u0026#34;\u003c/span\u003e, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        device_map\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;auto\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    processor \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Qwen2_5OmniProcessor\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efrom_pretrained(model_path)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 构造对话\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    conversation \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;system\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e: [{\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;You are Qwen...\u0026#34;\u003c/span\u003e}]},\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;user\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e: [{\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e: text_input}]}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 生成语音\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ewith\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eno_grad():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        text_ids, audio \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egenerate(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003einputs,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            speaker\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003espeaker,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            do_sample\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            temperature\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.8\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            top_p\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.95\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            max_new_tokens\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1024\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        )\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"2-语音转文本asr\"\u003e2. 语音转文本（ASR）\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eaudio_to_text\u003c/span\u003e(audio_path: str) \u003cspan style=\"color:#f92672\"\u003e-\u0026gt;\u003c/span\u003e str:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 标准ASR模式\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    conversation \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;system\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e: [{\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;你是Qwen...\u0026#34;\u003c/span\u003e}]},\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;user\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e: [{\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;audio\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;audio\u0026#34;\u003c/span\u003e: audio_path}]}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 生成文本\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ewith\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eno_grad():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        text_ids \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egenerate(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003einputs,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            do_sample\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            max_new_tokens\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1024\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            return_audio\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        )\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"web界面实现\"\u003eWeb界面实现\u003c/h2\u003e\n\u003cp\u003e使用Streamlit构建了简洁的Web界面：\u003c/p\u003e","title":"如何使用Qwen2.5-Omni实现文本转语音(TTS)和语音转文本(ASR)"},{"content":"\n概述 本脚本基于 Qwen2.5-Omni-7B 多模态模型实现文本转语音（TTS）功能，支持生成自然流畅的中文 / 英文语音，并提供两种语音类型（女性 “Chelsie”、男性 “Ethan”）。脚本可将输入文本转换为音频文件（.wav格式），适用于语音助手、内容创作、无障碍服务等场景。\n主要特性 🎙️ 支持自然流畅的 中文/英文 语音合成 👥 提供两种语音类型选择： 女性声线：\u0026ldquo;Chelsie\u0026rdquo; 男性声线：\u0026ldquo;Ethan\u0026rdquo; 💾 输出格式：标准 .wav 音频文件 🚀 高性能推理，适用于生产环境 应用场景 智能语音助手开发 内容创作与播客制作 无障碍服务 教育类应用 多媒体内容生成 开始使用 💡 在开始之前，请确保您的系统满足以下要求：\nPython 3.8+ CUDA 11.7+ (如需GPU加速) 至少16GB可用内存 安装依赖库\nuv init uv add git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview uv add accelerate uv add qwen-omni-utils[decord] uv add soundfile uv add torchvision uv sync 完整脚本代码（main_text2audio.py）\nimport os import soundfile as sf import torch from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor from qwen_omni_utils import process_mm_info from transformers import AutoConfig def text_to_speech( text_input: str, output_audio_path: str = \u0026#34;output/test_audio.wav\u0026#34;, speaker: str = \u0026#34;Chelsie\u0026#34;, model_path: str = \u0026#34;/home/llm/model/qwen/Omni/\u0026#34; # 改为本地路径或远程路径 ): \u0026#34;\u0026#34;\u0026#34; 文本转语音核心函数 :param text_input: 输入文本（支持中文/英文） :param output_audio_path: 音频输出路径（含文件名） :param speaker: 语音类型（\u0026#34;Chelsie\u0026#34;女性/\u0026#34;Ethan\u0026#34;男性） :param model_path: 模型路径（本地/远程） \u0026#34;\u0026#34;\u0026#34; # 1. 加载模型配置（修复ROPE参数兼容性） config = AutoConfig.from_pretrained(model_path, local_files_only=True) if hasattr(config, \u0026#34;rope_scaling\u0026#34;) and \u0026#34;mrope_section\u0026#34; in config.rope_scaling: config.rope_scaling.pop(\u0026#34;mrope_section\u0026#34;) # 2. 加载模型（支持GPU自动分配） model = Qwen2_5OmniForConditionalGeneration.from_pretrained( model_path, config=config, torch_dtype=\u0026#34;auto\u0026#34;, device_map=\u0026#34;auto\u0026#34;, local_files_only=(model_path != \u0026#34;Qwen/Qwen2.5-Omni-7B\u0026#34;) ) processor = Qwen2_5OmniProcessor.from_pretrained(model_path) # 3. 系统提示（必须包含语音生成能力声明） system_prompt = [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\u0026#34;} ] } ] # 4. 构建对话（纯文本输入） conversation = system_prompt + [ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: text_input}]} ] # 5. 处理输入数据 text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) audios, images, videos = process_mm_info(conversation, use_audio_in_video=False) # 6. 生成语音 inputs = processor( text=text, audio=audios, images=images, videos=videos, return_tensors=\u0026#34;pt\u0026#34;, padding=True, use_audio_in_video=False ).to(model.device, model.dtype) with torch.no_grad(): text_ids, audio = model.generate( **inputs, speaker=speaker, do_sample=True, # 启用采样模式以使用temperature/top_p temperature=0.8, # 控制随机性（0.5-1.0较自然） top_p=0.95, # 核采样参数 max_new_tokens=1024, # 控制语音时长（约15秒） use_audio_in_video=False ) # 7. 保存结果 os.makedirs(os.path.dirname(output_audio_path), exist_ok=True) sf.write(output_audio_path, audio.reshape(-1).cpu().numpy(), samplerate=24000) print(f\u0026#34;✅ 生成完成：{output_audio_path}\u0026#34;) print(f\u0026#34;📄 生成文本：{processor.batch_decode(text_ids, skip_special_tokens=True)[0]}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: # 示例输入（可替换为任意文本） input_text = \u0026#34;你好，这是Qwen2.5-Omni的文本转语音示例。祝你使用愉快！\u0026#34; # 调用函数（指定输出路径和语音类型） text_to_speech( input_text, output_audio_path=\u0026#34;output/hello_qwen.wav\u0026#34;, speaker=\u0026#34;Chelsie\u0026#34; # 可选\u0026#34;Ethan\u0026#34; ) 运行脚本\nuv run main.py ","permalink":"https://jackypanster.github.io/ai-stream/posts/how-to-setup-qwen-omni/","summary":"\u003cp\u003e\u003cimg alt=\"Qwen2.5-Omni-7B TTS\" loading=\"lazy\" src=\"https://via.placeholder.com/800x400.png/007bff/ffffff?text=Qwen2.5-Omni-7B+TTS\"\u003e\u003c/p\u003e\n\u003ch2 id=\"概述\"\u003e概述\u003c/h2\u003e\n\u003cp\u003e本脚本基于 Qwen2.5-Omni-7B 多模态模型实现文本转语音（TTS）功能，支持生成自然流畅的中文 / 英文语音，并提供两种语音类型（女性 “Chelsie”、男性 “Ethan”）。脚本可将输入文本转换为音频文件（.wav格式），适用于语音助手、内容创作、无障碍服务等场景。\u003c/p\u003e\n\u003ch2 id=\"主要特性\"\u003e主要特性\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e🎙️ 支持自然流畅的 \u003cstrong\u003e中文/英文\u003c/strong\u003e 语音合成\u003c/li\u003e\n\u003cli\u003e👥 提供两种语音类型选择：\n\u003cul\u003e\n\u003cli\u003e女性声线：\u0026ldquo;Chelsie\u0026rdquo;\u003c/li\u003e\n\u003cli\u003e男性声线：\u0026ldquo;Ethan\u0026rdquo;\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e💾 输出格式：标准 \u003ccode\u003e.wav\u003c/code\u003e 音频文件\u003c/li\u003e\n\u003cli\u003e🚀 高性能推理，适用于生产环境\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"应用场景\"\u003e应用场景\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e智能语音助手开发\u003c/li\u003e\n\u003cli\u003e内容创作与播客制作\u003c/li\u003e\n\u003cli\u003e无障碍服务\u003c/li\u003e\n\u003cli\u003e教育类应用\u003c/li\u003e\n\u003cli\u003e多媒体内容生成\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"开始使用\"\u003e开始使用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e💡 在开始之前，请确保您的系统满足以下要求：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePython 3.8+\u003c/li\u003e\n\u003cli\u003eCUDA 11.7+ (如需GPU加速)\u003c/li\u003e\n\u003cli\u003e至少16GB可用内存\u003c/li\u003e\n\u003c/ul\u003e\u003c/blockquote\u003e\n\u003cp\u003e安装依赖库\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv init\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv add git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv add accelerate\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv add qwen-omni-utils\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003edecord\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv add soundfile\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv add torchvision\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv sync\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e完整脚本代码（main_text2audio.py）\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e os\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e soundfile \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e sf\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e torch\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e transformers \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e qwen_omni_utils \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e process_mm_info\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e transformers \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e AutoConfig\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etext_to_speech\u003c/span\u003e(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    text_input: str,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    output_audio_path: str \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;output/test_audio.wav\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    speaker: str \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Chelsie\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    model_path: str \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;/home/llm/model/qwen/Omni/\u0026#34;\u003c/span\u003e  \u003cspan style=\"color:#75715e\"\u003e# 改为本地路径或远程路径\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    文本转语音核心函数\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    :param text_input: 输入文本（支持中文/英文）\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    :param output_audio_path: 音频输出路径（含文件名）\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    :param speaker: 语音类型（\u0026#34;Chelsie\u0026#34;女性/\u0026#34;Ethan\u0026#34;男性）\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    :param model_path: 模型路径（本地/远程）\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 1. 加载模型配置（修复ROPE参数兼容性）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    config \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e AutoConfig\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efrom_pretrained(model_path, local_files_only\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e hasattr(config, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rope_scaling\u0026#34;\u003c/span\u003e) \u003cspan style=\"color:#f92672\"\u003eand\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;mrope_section\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e config\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erope_scaling:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erope_scaling\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epop(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;mrope_section\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 2. 加载模型（支持GPU自动分配）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    model \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Qwen2_5OmniForConditionalGeneration\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efrom_pretrained(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        model_path,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003econfig,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        torch_dtype\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;auto\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        device_map\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;auto\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        local_files_only\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e(model_path \u003cspan style=\"color:#f92672\"\u003e!=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Qwen/Qwen2.5-Omni-7B\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    processor \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Qwen2_5OmniProcessor\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efrom_pretrained(model_path)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 3. 系统提示（必须包含语音生成能力声明）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    system_prompt \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;system\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e: [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\u0026#34;\u003c/span\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 4. 构建对话（纯文本输入）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    conversation \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e system_prompt \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;user\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e: [{\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e: text_input}]}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 5. 处理输入数据\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    text \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e processor\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eapply_chat_template(conversation, add_generation_prompt\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e, tokenize\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    audios, images, videos \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e process_mm_info(conversation, use_audio_in_video\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 6. 生成语音\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    inputs \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e processor(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        text\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003etext, audio\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eaudios, images\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eimages, videos\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003evideos,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        return_tensors\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;pt\u0026#34;\u003c/span\u003e, padding\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e, use_audio_in_video\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    )\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto(model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edevice, model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edtype)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ewith\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eno_grad():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        text_ids, audio \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egenerate(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003einputs,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            speaker\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003espeaker,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            do_sample\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e,  \u003cspan style=\"color:#75715e\"\u003e# 启用采样模式以使用temperature/top_p\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            temperature\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.8\u003c/span\u003e,  \u003cspan style=\"color:#75715e\"\u003e# 控制随机性（0.5-1.0较自然）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            top_p\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.95\u003c/span\u003e,       \u003cspan style=\"color:#75715e\"\u003e# 核采样参数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            max_new_tokens\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1024\u003c/span\u003e,  \u003cspan style=\"color:#75715e\"\u003e# 控制语音时长（约15秒）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            use_audio_in_video\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 7. 保存结果\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    os\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emakedirs(os\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epath\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edirname(output_audio_path), exist_ok\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    sf\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ewrite(output_audio_path, audio\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ereshape(\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecpu()\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enumpy(), samplerate\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e24000\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    print(\u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;✅ 生成完成：\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003eoutput_audio_path\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    print(\u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;📄 生成文本：\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003eprocessor\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebatch_decode(text_ids, skip_special_tokens\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e)[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e __name__ \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 示例输入（可替换为任意文本）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    input_text \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;你好，这是Qwen2.5-Omni的文本转语音示例。祝你使用愉快！\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 调用函数（指定输出路径和语音类型）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    text_to_speech(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        input_text,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        output_audio_path\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;output/hello_qwen.wav\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        speaker\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Chelsie\u0026#34;\u003c/span\u003e  \u003cspan style=\"color:#75715e\"\u003e# 可选\u0026#34;Ethan\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    )\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e运行脚本\u003c/p\u003e","title":"Qwen2.5-Omni-7B 文本转语音部署指南"}]