[{"content":"AI辅助编程实战：从需求到实现的完整对话记录 引言 本文真实还原了我与AI助手协作实现Docker镜像大小监控功能的全过程。通过这次实践，可以看到AI不仅能辅助开发者梳理需求、设计方案，还能在代码实现、文档编写等环节提供高效帮助。整个流程自然流畅，极大提升了开发体验。\n1. 需求分析与方案设计 初始需求描述 用户： 我们需要在Jenkins共享库中增加Docker镜像大小监控功能，当镜像超过4GB时发出警告。请帮忙分析一下实现思路。 AI的初步分析与建议 AI： 让我先看看项目的结构…… 这是一个Jenkins共享库项目，核心代码都在`vars/`目录。针对镜像大小监控，我建议直接在`buildDocker`方法里加逻辑： 1. 镜像构建完成后用`du`命令获取大小 2. 与4GB阈值比较 3. 超过阈值就输出警告 4. 相关指标写日志，方便后续分析 这个方案您觉得合适吗？有无补充？ 需求确认与细化 用户： 这个思路可以，不过还需要： 1. 阈值要可配置，默认4GB 2. 日志里要有镜像名、大小、时间戳等详细信息 3. 输出优化建议 另外，异常处理也要考虑得更优雅些。 2. 代码实现 初始代码实现 AI： 明白，我来实现。会在`buildDocker`方法里： 1. 用`getEnvOrDefault`支持阈值配置 2. 详细记录指标 3. 加强错误处理 4. 输出优化建议 实现逻辑您看还有什么要调整的吗？ 代码审查与优化 用户： 整体不错，不过警告要改成抛异常直接中断构建。另外，能不能顺便加上基础镜像大小检测？ 异常处理优化 AI： 明白您的需求，代码我会这样优化： 1. 超标直接抛异常中断流水线 2. 增加基础镜像大小检测 3. 明确区分业务异常（比如镜像过大）和技术异常（比如命令失败） 基础镜像检测这块，我建议： - 解析Dockerfile的FROM - 拉取并检测基础镜像大小 - 超限就给出alpine等轻量镜像建议 这样处理您觉得OK吗？ 3. 测试与验证 测试方案讨论 用户： 怎么验证这个功能？ AI： 建议这样测试： 1. 镜像大小正常 2. 恰好等于阈值 3. 超过阈值 4. Dockerfile解析异常 也可以临时把阈值调小，快速验证告警逻辑。 4. 文档与沟通 变更日志生成 AI： 我已为您生成详细CHANGELOG，包括： - 功能说明 - 技术实现细节 - 配置参数说明 - 后续优化计划 如需调整，随时告诉我。 团队通知 用户： 要发个通知给团队，说明我们要限制镜像大小。 AI： 建议分阶段推进： 1. 先监控和通知 2. 再进入警告期 3. 最后强制限制 这样大家有缓冲期，可以逐步优化Dockerfile。需要帮忙起草通知吗？ 5. 经验总结 技术收获 AI辅助效率\n代码实现速度提升显著 自动生成文档节省时间 代码质量\n异常处理更完善 配置更灵活 团队协作\n文档更规范 知识传递更高效 建议与展望 建立AI辅助编码规范 完善提示工程实践 探索更多自动化场景 结语 通过这次实践，AI能陪伴开发者一起思考、决策和落地实现。期待未来AI在更多软件开发场景中发挥更大作用。\n","permalink":"https://jackypanster.github.io/ai-stream/posts/coding-with-ai/","summary":"\u003ch1 id=\"ai辅助编程实战从需求到实现的完整对话记录\"\u003eAI辅助编程实战：从需求到实现的完整对话记录\u003c/h1\u003e\n\u003ch2 id=\"引言\"\u003e引言\u003c/h2\u003e\n\u003cp\u003e本文真实还原了我与AI助手协作实现Docker镜像大小监控功能的全过程。通过这次实践，可以看到AI不仅能辅助开发者梳理需求、设计方案，还能在代码实现、文档编写等环节提供高效帮助。整个流程自然流畅，极大提升了开发体验。\u003c/p\u003e\n\u003ch2 id=\"1-需求分析与方案设计\"\u003e1. 需求分析与方案设计\u003c/h2\u003e\n\u003ch3 id=\"初始需求描述\"\u003e初始需求描述\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e用户：\n我们需要在Jenkins共享库中增加Docker镜像大小监控功能，当镜像超过4GB时发出警告。请帮忙分析一下实现思路。\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"ai的初步分析与建议\"\u003eAI的初步分析与建议\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eAI：\n让我先看看项目的结构……\n\n这是一个Jenkins共享库项目，核心代码都在`vars/`目录。针对镜像大小监控，我建议直接在`buildDocker`方法里加逻辑：\n\n1. 镜像构建完成后用`du`命令获取大小\n2. 与4GB阈值比较\n3. 超过阈值就输出警告\n4. 相关指标写日志，方便后续分析\n\n这个方案您觉得合适吗？有无补充？\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"需求确认与细化\"\u003e需求确认与细化\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e用户：\n这个思路可以，不过还需要：\n1. 阈值要可配置，默认4GB\n2. 日志里要有镜像名、大小、时间戳等详细信息\n3. 输出优化建议\n\n另外，异常处理也要考虑得更优雅些。\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"2-代码实现\"\u003e2. 代码实现\u003c/h2\u003e\n\u003ch3 id=\"初始代码实现\"\u003e初始代码实现\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eAI：\n明白，我来实现。会在`buildDocker`方法里：\n\n1. 用`getEnvOrDefault`支持阈值配置\n2. 详细记录指标\n3. 加强错误处理\n4. 输出优化建议\n\n实现逻辑您看还有什么要调整的吗？\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"代码审查与优化\"\u003e代码审查与优化\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e用户：\n整体不错，不过警告要改成抛异常直接中断构建。另外，能不能顺便加上基础镜像大小检测？\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"异常处理优化\"\u003e异常处理优化\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eAI：\n明白您的需求，代码我会这样优化：\n\n1. 超标直接抛异常中断流水线\n2. 增加基础镜像大小检测\n3. 明确区分业务异常（比如镜像过大）和技术异常（比如命令失败）\n\n基础镜像检测这块，我建议：\n- 解析Dockerfile的FROM\n- 拉取并检测基础镜像大小\n- 超限就给出alpine等轻量镜像建议\n\n这样处理您觉得OK吗？\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"3-测试与验证\"\u003e3. 测试与验证\u003c/h2\u003e\n\u003ch3 id=\"测试方案讨论\"\u003e测试方案讨论\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e用户：\n怎么验证这个功能？\n\nAI：\n建议这样测试：\n1. 镜像大小正常\n2. 恰好等于阈值\n3. 超过阈值\n4. Dockerfile解析异常\n\n也可以临时把阈值调小，快速验证告警逻辑。\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"4-文档与沟通\"\u003e4. 文档与沟通\u003c/h2\u003e\n\u003ch3 id=\"变更日志生成\"\u003e变更日志生成\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eAI：\n我已为您生成详细CHANGELOG，包括：\n- 功能说明\n- 技术实现细节\n- 配置参数说明\n- 后续优化计划\n\n如需调整，随时告诉我。\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"团队通知\"\u003e团队通知\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e用户：\n要发个通知给团队，说明我们要限制镜像大小。\n\nAI：\n建议分阶段推进：\n1. 先监控和通知\n2. 再进入警告期\n3. 最后强制限制\n\n这样大家有缓冲期，可以逐步优化Dockerfile。需要帮忙起草通知吗？\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"5-经验总结\"\u003e5. 经验总结\u003c/h2\u003e\n\u003ch3 id=\"技术收获\"\u003e技术收获\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAI辅助效率\u003c/strong\u003e\u003c/p\u003e","title":"AI辅助编程实战：从需求到实现的完整对话记录"},{"content":"基于FunAudioLLM/SenseVoiceSmall搭建高效语音转录服务的实践之路 项目概述 实现一个语音转录文本（ASR）的服务，目标是能够高效地将用户上传的音频文件转换为文字。出于中文语音的考虑，选择了来自 FunAudioLLM 的 SenseVoiceSmall 模型，它以其多语种支持、高效率以及集成的语音理解能力（如情感识别、事件检测）吸引了我。本文将详细记录从环境配置、核心功能实现到踩坑解决的全过程，并分享一些关于模型选型的思考。\n完整代码已开源在 GitHub 仓库：https://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall\n项目需求文档（prd.md）关键信息如下：\n模型: FunAudioLLM/SenseVoice (具体为 SenseVoiceSmall) 本地模型路径: /home/llm/model/iic/SenseVoiceSmall (从 ModelScope 下载) API框架: FastAPI Python环境管理: uv 环境配置 为了保持开发环境的纯净和高效，采用了 uv 来管理 Python 依赖。\n创建虚拟环境 (如果尚未创建):\nuv venv .venv source .venv/bin/activate 安装核心依赖: 初始的 requirements.txt 包含了 fastapi, uvicorn, python-multipart 等基础库。后续根据模型加载和处理的需求，逐步添加了 torch, torchaudio, numpy, transformers, sentencepiece, 以及最终解决模型加载问题的核心库 funasr。\nuv pip install -r requirements.txt 核心功能实现概览 项目结构 项目的主要结构包括：\napp/main.py: FastAPI 应用入口，定义 API 路由和应用生命周期事件（如模型加载）。 app/models/sensevoice_loader.py: 负责加载 SenseVoiceSmall 模型，采用单例模式。 app/services/asr_service.py: 封装语音处理和模型推理的核心逻辑。 app/schemas.py: 定义 API 的请求和响应数据模型 (Pydantic models)。 API 端点 关键的 API 端点设计为：\nPOST /asr_pure Content-Type: multipart/form-data Body: file (音频文件) 返回转录后的文本及处理时间。\n踩坑与解决之路：模型加载的曲折历程 在项目推进过程中，模型加载部分是遇到问题最多的地方，也是收获最多的地方。\n坑1：Hugging Face AutoClass 的 \u0026ldquo;Unrecognized model\u0026rdquo; 最初，尝试使用 Hugging Face transformers 库通用的 AutoProcessor.from_pretrained() 和 AutoModelForSpeechSeq2Seq.from_pretrained() 来加载本地的 SenseVoiceSmall 模型文件。\n# app/models/sensevoice_loader.py (早期尝试) # from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor # ... # self.processor = AutoProcessor.from_pretrained(MODEL_PATH) # self.model = AutoModelForSpeechSeq2Seq.from_pretrained(MODEL_PATH) 然而，服务启动时立即报错：\nValueError: Unrecognized model in /home/llm/model/iic/SenseVoiceSmall. Should have a model_type key in its config.json... 这个错误表明 transformers 的自动发现机制无法识别模型类型，通常是因为模型目录下的 config.json 文件缺少 model_type 字段，或者该模型需要特定的加载类。\n坑2：转向 funasr 与 trust_remote_code 的初步探索 查阅 FunAudioLLM/SenseVoice 的官方文档后发现，推荐使用 funasr 库的 AutoModel 来加载 SenseVoice 系列模型。于是调整了代码：\n添加 funasr 到 requirements.txt。 修改 SenseVoiceLoader: # app/models/sensevoice_loader.py (引入 funasr) from funasr import AutoModel # ... self.model = AutoModel( model=FUNASR_MODEL_NAME_OR_PATH, # 即本地路径 trust_remote_code=True, device=self.device ) 同时，asr_service.py 中的推理逻辑也相应调整为调用 funasr 模型对象的 .generate() 方法。 本以为这样能解决问题，但启动时又遇到了新的日志：\nLoading remote code failed: model, No module named \u0026#39;model\u0026#39; 尽管这条日志出现，但后续的 API 调用测试居然成功了！这让我非常困惑。\n坑3：remote_code 参数与 model.py 文件的“幻影” 深入研究 funasr 和 SenseVoice 的文档，注意到对于包含自定义代码（如 model.py）的模型，除了 trust_remote_code=True，有时还需要明确指定 remote_code 参数。\n我检查了 Hugging Face 仓库 FunAudioLLM/SenseVoiceSmall (https://huggingface.co/FunAudioLLM/SenseVoiceSmall/tree/main)，发现其文件列表中确实包含一个 model.py。因此，我尝试在 AutoModel 调用中加入 remote_code=\u0026quot;model.py\u0026quot;。\n# app/models/sensevoice_loader.py (尝试指定 remote_code) self.model = AutoModel( model=FUNASR_MODEL_NAME_OR_PATH, trust_remote_code=True, remote_code=\u0026#34;model.py\u0026#34;, # \u0026lt;--- 新增 device=self.device ) 结果，No module named 'model' 的错误依旧。\n解决方案：澄清 ModelScope 与 Hugging Face 的模型文件差异 本地模型 /home/llm/model/iic/SenseVoiceSmall 是从 ModelScope (https://www.modelscope.cn/models/iic/SenseVoiceSmall/files) 下载的，而非直接 clone Hugging Face 的仓库。通过 ls -al /home/llm/model/iic/SenseVoiceSmall/ 查看本地文件，发现确实没有 model.py 文件！\n这解释了为什么指定 remote_code=\u0026quot;model.py\u0026quot; 依然报错。ModelScope 提供的模型包可能与 Hugging Face 仓库中的文件结构不完全一致，特别是对于这种依赖 funasr 特定加载方式的模型。\n最终的正确配置：移除 remote_code 参数，但保留 trust_remote_code=True。\n# app/models/sensevoice_loader.py (最终正确配置) self.model = AutoModel( model=FUNASR_MODEL_NAME_OR_PATH, trust_remote_code=True, # 保留，funasr 可能仍需此权限处理 ModelScope 模型 # remote_code=\u0026#34;model.py\u0026#34;, # 移除，因为本地 ModelScope 版本无此文件 device=self.device ) 这样修改后，服务启动时仍然会打印 Loading remote code failed: model, No module named 'model'，但 API 调用完全正常！\n原因分析：funasr 在 trust_remote_code=True 时，会优先尝试加载自定义代码。如果本地模型路径（如从 ModelScope 下载的）没有 model.py，这个尝试会失败并打印日志。但随后，funasr 能够识别出这是一个有效的 ModelScope 模型路径，并转用其内部的标准加载流程成功加载模型。因此，该日志在这种情况下是良性的。\n模型对比与选型思考 在解决问题的过程中，也探讨了 FunAudioLLM/SenseVoiceSmall 与其他主流 ASR 模型的对比：\nOpenAI Whisper 系列 (如 whisper-large-v3):\n优势: 极高的准确率，强大的多语言能力，庞大的社区。 劣势: 推理速度相对较慢（尤其大模型），不直接提供情感/事件检测。 Wav2Vec2 系列:\n优势: 自监督学习典范，大量特定语言微调模型。 劣势: 基础模型功能相对单一。 SenseVoiceSmall 的核心优势 高效推理：其模型卡声称采用非自回归端到端框架，比 Whisper-Large 快15倍。这对于需要低延迟的应用至关重要。\n多任务集成：内置 ASR、LID（语种识别）、SER（情感识别）、AED（事件检测）。如果应用场景需要这些附加信息，SenseVoiceSmall 提供了一站式解决方案。\n特定语言优化：在中文、粤语等语言上表现突出。\n结论 没有绝对的“最好”，只有“最适合”。\n若追求极致准确性和最广语言覆盖，且对延迟不敏感，Whisper 仍是首选。 若对推理效率、集成的多任务语音理解（特别是情感/事件）或中文等特定场景有高要求，SenseVoiceSmall 是一个极具竞争力的选择。 目前选择的 SenseVoiceSmall，尤其是在确认了其 ModelScope 版本能够顺畅运行后，对于我的项目目标来说是一个合适的起点。\n当前状态与展望 目前，基于 FunAudioLLM/SenseVoiceSmall 和 FastAPI 的语音转录服务已成功搭建并能正确处理请求。\n$ curl -X POST \u0026#34;http://\u0026lt;your_server_ip\u0026gt;:8888/asr_pure\u0026#34; -F \u0026#34;file=@test_audio.wav\u0026#34; {\u0026#34;text\u0026#34;:\u0026#34;太好了，那接下来咱们可以试试其他功能了。比如说你想测试一下语音合成的效果怎么样，或者是看看有没有什么新的语音处理功能出来啦。😔\u0026#34;,\u0026#34;status\u0026#34;:\u0026#34;success\u0026#34;,\u0026#34;processing_time_ms\u0026#34;:503.39...} 后续可优化的方向 性能优化：进一步测试并发处理能力，考虑多 worker 配置。 错误处理与日志：完善更细致的错误捕获和日志记录。 功能扩展：如果需要，可以利用 SenseVoiceSmall 的情感识别和事件检测能力。 VAD 集成：对于长音频，考虑在 funasr.AutoModel 加载时集成 VAD (Voice Activity Detection) 功能，以实现自动分段处理，提升长音频处理的稳定性和效率。 异步处理与队列：对于高并发场景，引入消息队列和异步任务处理。 ","permalink":"https://jackypanster.github.io/ai-stream/posts/howto-use-sensevoicesmall/","summary":"\u003ch1 id=\"基于funaudiollmsensevoicesmall搭建高效语音转录服务的实践之路\"\u003e基于FunAudioLLM/SenseVoiceSmall搭建高效语音转录服务的实践之路\u003c/h1\u003e\n\u003ch2 id=\"项目概述\"\u003e项目概述\u003c/h2\u003e\n\u003cp\u003e实现一个语音转录文本（ASR）的服务，目标是能够高效地将用户上传的音频文件转换为文字。出于中文语音的考虑，选择了来自 \u003ccode\u003eFunAudioLLM\u003c/code\u003e 的 \u003ccode\u003eSenseVoiceSmall\u003c/code\u003e 模型，它以其多语种支持、高效率以及集成的语音理解能力（如情感识别、事件检测）吸引了我。本文将详细记录从环境配置、核心功能实现到踩坑解决的全过程，并分享一些关于模型选型的思考。\u003c/p\u003e\n\u003cp\u003e完整代码已开源在 GitHub 仓库：\u003ca href=\"https://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall\"\u003ehttps://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e项目需求文档（\u003ccode\u003eprd.md\u003c/code\u003e）关键信息如下：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e模型\u003c/strong\u003e: FunAudioLLM/SenseVoice (具体为 \u003ccode\u003eSenseVoiceSmall\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e本地模型路径\u003c/strong\u003e: \u003ccode\u003e/home/llm/model/iic/SenseVoiceSmall\u003c/code\u003e (从 ModelScope 下载)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAPI框架\u003c/strong\u003e: FastAPI\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePython环境管理\u003c/strong\u003e: \u003ccode\u003euv\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"环境配置\"\u003e环境配置\u003c/h2\u003e\n\u003cp\u003e为了保持开发环境的纯净和高效，采用了 \u003ccode\u003euv\u003c/code\u003e 来管理 Python 依赖。\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e创建虚拟环境\u003c/strong\u003e (如果尚未创建):\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv venv .venv\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esource .venv/bin/activate\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e安装核心依赖\u003c/strong\u003e:\n初始的 \u003ccode\u003erequirements.txt\u003c/code\u003e 包含了 \u003ccode\u003efastapi\u003c/code\u003e, \u003ccode\u003euvicorn\u003c/code\u003e, \u003ccode\u003epython-multipart\u003c/code\u003e 等基础库。后续根据模型加载和处理的需求，逐步添加了 \u003ccode\u003etorch\u003c/code\u003e, \u003ccode\u003etorchaudio\u003c/code\u003e, \u003ccode\u003enumpy\u003c/code\u003e, \u003ccode\u003etransformers\u003c/code\u003e, \u003ccode\u003esentencepiece\u003c/code\u003e, 以及最终解决模型加载问题的核心库 \u003ccode\u003efunasr\u003c/code\u003e。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv pip install -r requirements.txt\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"核心功能实现概览\"\u003e核心功能实现概览\u003c/h2\u003e\n\u003ch3 id=\"项目结构\"\u003e项目结构\u003c/h3\u003e\n\u003cp\u003e项目的主要结构包括：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eapp/main.py\u003c/code\u003e: FastAPI 应用入口，定义 API 路由和应用生命周期事件（如模型加载）。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eapp/models/sensevoice_loader.py\u003c/code\u003e: 负责加载 \u003ccode\u003eSenseVoiceSmall\u003c/code\u003e 模型，采用单例模式。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eapp/services/asr_service.py\u003c/code\u003e: 封装语音处理和模型推理的核心逻辑。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eapp/schemas.py\u003c/code\u003e: 定义 API 的请求和响应数据模型 (Pydantic models)。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"api-端点\"\u003eAPI 端点\u003c/h3\u003e\n\u003cp\u003e关键的 API 端点设计为：\u003c/p\u003e","title":"基于FunAudioLLM/SenseVoiceSmall搭建高效语音转录服务的实践之路"},{"content":"如何使用Qwen2.5-Omni实现文本转语音(TTS)和语音转文本(ASR) 项目概述 本项目基于Qwen2.5-Omni-7B模型，实现了两个核心功能：\n文本转语音（TTS）：将输入文本转换为自然流畅的语音 语音转文本（ASR）：将语音文件转换为文本，支持标准ASR和纯ASR两种模式 项目地址：https://github.com/jackypanster/qwen-omni\n环境配置 推荐使用conda管理Python环境，确保依赖安装的稳定性：\n# 创建并激活环境 conda create -n qwen-tts python=3.10 conda activate qwen-tts # 安装PyTorch（GPU版本） conda install pytorch=2.5.1 pytorch-cuda=12.1 -c pytorch -c nvidia conda install torchvision torchaudio -c pytorch # 安装其他依赖 conda install streamlit python-soundfile -c conda-forge pip install git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview pip install qwen-omni-utils 核心功能实现 1. 文本转语音（TTS） def text_to_speech(text_input, output_audio_path=\u0026#34;output/output.wav\u0026#34;, speaker=\u0026#34;Chelsie\u0026#34;): # 加载模型和处理器 model = Qwen2_5OmniForConditionalGeneration.from_pretrained( model_path, config=config, torch_dtype=\u0026#34;auto\u0026#34;, device_map=\u0026#34;auto\u0026#34; ) processor = Qwen2_5OmniProcessor.from_pretrained(model_path) # 构造对话 conversation = [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;You are Qwen...\u0026#34;}]}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: text_input}]} ] # 生成语音 with torch.no_grad(): text_ids, audio = model.generate( **inputs, speaker=speaker, do_sample=True, temperature=0.8, top_p=0.95, max_new_tokens=1024 ) 2. 语音转文本（ASR） def audio_to_text(audio_path: str) -\u0026gt; str: # 标准ASR模式 conversation = [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;你是Qwen...\u0026#34;}]}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;audio\u0026#34;, \u0026#34;audio\u0026#34;: audio_path}]} ] # 生成文本 with torch.no_grad(): text_ids = model.generate( **inputs, do_sample=False, max_new_tokens=1024, return_audio=False ) Web界面实现 使用Streamlit构建了简洁的Web界面：\n# 文本输入 text_input = st.text_area(\u0026#34;请输入要合成的文本：\u0026#34;, height=120, max_chars=200) # 发音人选择 speaker = st.selectbox(\u0026#34;请选择发音人：\u0026#34;, [\u0026#34;Chelsie\u0026#34;, \u0026#34;Ethan\u0026#34;], index=0) # 生成按钮 if st.button(\u0026#34;生成语音\u0026#34;): # 生成语音并播放 audio_path = os.path.join(OUTPUT_DIR, f\u0026#34;tts_{uuid.uuid4().hex}.wav\u0026#34;) text_to_speech(text_input, output_audio_path=audio_path, speaker=speaker) st.audio(audio_path, format=\u0026#34;audio/wav\u0026#34;) RESTful API实现 使用FastAPI构建了RESTful API接口：\n@app.post(\u0026#34;/tts\u0026#34;) async def tts(request: TTSRequest): audio_filename = f\u0026#34;tts_{uuid.uuid4().hex}.wav\u0026#34; audio_path = os.path.join(OUTPUT_DIR, audio_filename) text_to_speech(request.text, audio_path, request.speaker) return {\u0026#34;audio_url\u0026#34;: f\u0026#34;/output/{audio_filename}\u0026#34;} @app.post(\u0026#34;/asr\u0026#34;) async def asr(file: UploadFile = File(...)): # 处理上传的音频文件 audio_path = os.path.join(OUTPUT_DIR, f\u0026#34;asr_{uuid.uuid4().hex}.wav\u0026#34;) with open(audio_path, \u0026#34;wb\u0026#34;) as buffer: shutil.copyfileobj(file.file, buffer) text = audio_to_text(audio_path) return {\u0026#34;text\u0026#34;: text} 使用说明 启动Web界面： streamlit run app_text2audio.py 启动API服务： uvicorn fastapi_app:app --host 0.0.0.0 --port 8000 注意事项 模型文件较大，建议提前下载并配置好模型路径 使用conda安装依赖可以避免大多数环境问题 音频文件会保存在output目录下 API接口支持文件上传和文本转写 后续优化方向 支持更多发音人选项 优化模型加载速度 添加批量处理功能 支持更多音频格式 添加历史记录功能 参考资源 Qwen2.5-Omni-7B官方文档 Streamlit文档 FastAPI文档 ","permalink":"https://jackypanster.github.io/ai-stream/posts/how-to-use-qwen-omni-tts-asr/","summary":"\u003ch1 id=\"如何使用qwen25-omni实现文本转语音tts和语音转文本asr\"\u003e如何使用Qwen2.5-Omni实现文本转语音(TTS)和语音转文本(ASR)\u003c/h1\u003e\n\u003ch2 id=\"项目概述\"\u003e项目概述\u003c/h2\u003e\n\u003cp\u003e本项目基于Qwen2.5-Omni-7B模型，实现了两个核心功能：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e文本转语音（TTS）：将输入文本转换为自然流畅的语音\u003c/li\u003e\n\u003cli\u003e语音转文本（ASR）：将语音文件转换为文本，支持标准ASR和纯ASR两种模式\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e项目地址：\u003ca href=\"https://github.com/jackypanster/qwen-omni\"\u003ehttps://github.com/jackypanster/qwen-omni\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"环境配置\"\u003e环境配置\u003c/h2\u003e\n\u003cp\u003e推荐使用conda管理Python环境，确保依赖安装的稳定性：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 创建并激活环境\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econda create -n qwen-tts python\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e3.10\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econda activate qwen-tts\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 安装PyTorch（GPU版本）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econda install pytorch\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e2.5.1 pytorch-cuda\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e12.1 -c pytorch -c nvidia\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econda install torchvision torchaudio -c pytorch\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 安装其他依赖\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econda install streamlit python-soundfile -c conda-forge\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip install git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip install qwen-omni-utils\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"核心功能实现\"\u003e核心功能实现\u003c/h2\u003e\n\u003ch3 id=\"1-文本转语音tts\"\u003e1. 文本转语音（TTS）\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etext_to_speech\u003c/span\u003e(text_input, output_audio_path\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;output/output.wav\u0026#34;\u003c/span\u003e, speaker\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Chelsie\u0026#34;\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 加载模型和处理器\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    model \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Qwen2_5OmniForConditionalGeneration\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efrom_pretrained(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        model_path, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003econfig, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        torch_dtype\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;auto\u0026#34;\u003c/span\u003e, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        device_map\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;auto\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    processor \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Qwen2_5OmniProcessor\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efrom_pretrained(model_path)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 构造对话\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    conversation \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;system\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e: [{\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;You are Qwen...\u0026#34;\u003c/span\u003e}]},\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;user\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e: [{\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e: text_input}]}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 生成语音\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ewith\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eno_grad():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        text_ids, audio \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egenerate(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003einputs,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            speaker\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003espeaker,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            do_sample\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            temperature\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.8\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            top_p\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.95\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            max_new_tokens\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1024\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        )\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"2-语音转文本asr\"\u003e2. 语音转文本（ASR）\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eaudio_to_text\u003c/span\u003e(audio_path: str) \u003cspan style=\"color:#f92672\"\u003e-\u0026gt;\u003c/span\u003e str:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 标准ASR模式\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    conversation \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;system\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e: [{\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;你是Qwen...\u0026#34;\u003c/span\u003e}]},\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;user\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e: [{\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;audio\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;audio\u0026#34;\u003c/span\u003e: audio_path}]}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 生成文本\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ewith\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eno_grad():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        text_ids \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egenerate(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003einputs,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            do_sample\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            max_new_tokens\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1024\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            return_audio\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        )\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"web界面实现\"\u003eWeb界面实现\u003c/h2\u003e\n\u003cp\u003e使用Streamlit构建了简洁的Web界面：\u003c/p\u003e","title":"如何使用Qwen2.5-Omni实现文本转语音(TTS)和语音转文本(ASR)"},{"content":"\n概述 本脚本基于 Qwen2.5-Omni-7B 多模态模型实现文本转语音（TTS）功能，支持生成自然流畅的中文 / 英文语音，并提供两种语音类型（女性 “Chelsie”、男性 “Ethan”）。脚本可将输入文本转换为音频文件（.wav格式），适用于语音助手、内容创作、无障碍服务等场景。\n主要特性 🎙️ 支持自然流畅的 中文/英文 语音合成 👥 提供两种语音类型选择： 女性声线：\u0026ldquo;Chelsie\u0026rdquo; 男性声线：\u0026ldquo;Ethan\u0026rdquo; 💾 输出格式：标准 .wav 音频文件 🚀 高性能推理，适用于生产环境 应用场景 智能语音助手开发 内容创作与播客制作 无障碍服务 教育类应用 多媒体内容生成 开始使用 💡 在开始之前，请确保您的系统满足以下要求：\nPython 3.8+ CUDA 11.7+ (如需GPU加速) 至少16GB可用内存 安装依赖库\nuv init uv add git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview uv add accelerate uv add qwen-omni-utils[decord] uv add soundfile uv add torchvision uv sync 完整脚本代码（main_text2audio.py）\nimport os import soundfile as sf import torch from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor from qwen_omni_utils import process_mm_info from transformers import AutoConfig def text_to_speech( text_input: str, output_audio_path: str = \u0026#34;output/test_audio.wav\u0026#34;, speaker: str = \u0026#34;Chelsie\u0026#34;, model_path: str = \u0026#34;/home/llm/model/qwen/Omni/\u0026#34; # 改为本地路径或远程路径 ): \u0026#34;\u0026#34;\u0026#34; 文本转语音核心函数 :param text_input: 输入文本（支持中文/英文） :param output_audio_path: 音频输出路径（含文件名） :param speaker: 语音类型（\u0026#34;Chelsie\u0026#34;女性/\u0026#34;Ethan\u0026#34;男性） :param model_path: 模型路径（本地/远程） \u0026#34;\u0026#34;\u0026#34; # 1. 加载模型配置（修复ROPE参数兼容性） config = AutoConfig.from_pretrained(model_path, local_files_only=True) if hasattr(config, \u0026#34;rope_scaling\u0026#34;) and \u0026#34;mrope_section\u0026#34; in config.rope_scaling: config.rope_scaling.pop(\u0026#34;mrope_section\u0026#34;) # 2. 加载模型（支持GPU自动分配） model = Qwen2_5OmniForConditionalGeneration.from_pretrained( model_path, config=config, torch_dtype=\u0026#34;auto\u0026#34;, device_map=\u0026#34;auto\u0026#34;, local_files_only=(model_path != \u0026#34;Qwen/Qwen2.5-Omni-7B\u0026#34;) ) processor = Qwen2_5OmniProcessor.from_pretrained(model_path) # 3. 系统提示（必须包含语音生成能力声明） system_prompt = [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\u0026#34;} ] } ] # 4. 构建对话（纯文本输入） conversation = system_prompt + [ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: text_input}]} ] # 5. 处理输入数据 text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) audios, images, videos = process_mm_info(conversation, use_audio_in_video=False) # 6. 生成语音 inputs = processor( text=text, audio=audios, images=images, videos=videos, return_tensors=\u0026#34;pt\u0026#34;, padding=True, use_audio_in_video=False ).to(model.device, model.dtype) with torch.no_grad(): text_ids, audio = model.generate( **inputs, speaker=speaker, do_sample=True, # 启用采样模式以使用temperature/top_p temperature=0.8, # 控制随机性（0.5-1.0较自然） top_p=0.95, # 核采样参数 max_new_tokens=1024, # 控制语音时长（约15秒） use_audio_in_video=False ) # 7. 保存结果 os.makedirs(os.path.dirname(output_audio_path), exist_ok=True) sf.write(output_audio_path, audio.reshape(-1).cpu().numpy(), samplerate=24000) print(f\u0026#34;✅ 生成完成：{output_audio_path}\u0026#34;) print(f\u0026#34;📄 生成文本：{processor.batch_decode(text_ids, skip_special_tokens=True)[0]}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: # 示例输入（可替换为任意文本） input_text = \u0026#34;你好，这是Qwen2.5-Omni的文本转语音示例。祝你使用愉快！\u0026#34; # 调用函数（指定输出路径和语音类型） text_to_speech( input_text, output_audio_path=\u0026#34;output/hello_qwen.wav\u0026#34;, speaker=\u0026#34;Chelsie\u0026#34; # 可选\u0026#34;Ethan\u0026#34; ) 运行脚本\nuv run main.py ","permalink":"https://jackypanster.github.io/ai-stream/posts/how-to-setup-qwen-omni/","summary":"\u003cp\u003e\u003cimg alt=\"Qwen2.5-Omni-7B TTS\" loading=\"lazy\" src=\"https://via.placeholder.com/800x400.png/007bff/ffffff?text=Qwen2.5-Omni-7B+TTS\"\u003e\u003c/p\u003e\n\u003ch2 id=\"概述\"\u003e概述\u003c/h2\u003e\n\u003cp\u003e本脚本基于 Qwen2.5-Omni-7B 多模态模型实现文本转语音（TTS）功能，支持生成自然流畅的中文 / 英文语音，并提供两种语音类型（女性 “Chelsie”、男性 “Ethan”）。脚本可将输入文本转换为音频文件（.wav格式），适用于语音助手、内容创作、无障碍服务等场景。\u003c/p\u003e\n\u003ch2 id=\"主要特性\"\u003e主要特性\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e🎙️ 支持自然流畅的 \u003cstrong\u003e中文/英文\u003c/strong\u003e 语音合成\u003c/li\u003e\n\u003cli\u003e👥 提供两种语音类型选择：\n\u003cul\u003e\n\u003cli\u003e女性声线：\u0026ldquo;Chelsie\u0026rdquo;\u003c/li\u003e\n\u003cli\u003e男性声线：\u0026ldquo;Ethan\u0026rdquo;\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e💾 输出格式：标准 \u003ccode\u003e.wav\u003c/code\u003e 音频文件\u003c/li\u003e\n\u003cli\u003e🚀 高性能推理，适用于生产环境\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"应用场景\"\u003e应用场景\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e智能语音助手开发\u003c/li\u003e\n\u003cli\u003e内容创作与播客制作\u003c/li\u003e\n\u003cli\u003e无障碍服务\u003c/li\u003e\n\u003cli\u003e教育类应用\u003c/li\u003e\n\u003cli\u003e多媒体内容生成\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"开始使用\"\u003e开始使用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e💡 在开始之前，请确保您的系统满足以下要求：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePython 3.8+\u003c/li\u003e\n\u003cli\u003eCUDA 11.7+ (如需GPU加速)\u003c/li\u003e\n\u003cli\u003e至少16GB可用内存\u003c/li\u003e\n\u003c/ul\u003e\u003c/blockquote\u003e\n\u003cp\u003e安装依赖库\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv init\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv add git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv add accelerate\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv add qwen-omni-utils\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003edecord\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv add soundfile\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv add torchvision\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv sync\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e完整脚本代码（main_text2audio.py）\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e os\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e soundfile \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e sf\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e torch\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e transformers \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e qwen_omni_utils \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e process_mm_info\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e transformers \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e AutoConfig\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etext_to_speech\u003c/span\u003e(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    text_input: str,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    output_audio_path: str \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;output/test_audio.wav\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    speaker: str \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Chelsie\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    model_path: str \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;/home/llm/model/qwen/Omni/\u0026#34;\u003c/span\u003e  \u003cspan style=\"color:#75715e\"\u003e# 改为本地路径或远程路径\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    文本转语音核心函数\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    :param text_input: 输入文本（支持中文/英文）\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    :param output_audio_path: 音频输出路径（含文件名）\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    :param speaker: 语音类型（\u0026#34;Chelsie\u0026#34;女性/\u0026#34;Ethan\u0026#34;男性）\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    :param model_path: 模型路径（本地/远程）\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 1. 加载模型配置（修复ROPE参数兼容性）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    config \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e AutoConfig\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efrom_pretrained(model_path, local_files_only\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e hasattr(config, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rope_scaling\u0026#34;\u003c/span\u003e) \u003cspan style=\"color:#f92672\"\u003eand\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;mrope_section\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e config\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erope_scaling:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erope_scaling\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epop(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;mrope_section\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 2. 加载模型（支持GPU自动分配）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    model \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Qwen2_5OmniForConditionalGeneration\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efrom_pretrained(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        model_path,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003econfig,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        torch_dtype\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;auto\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        device_map\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;auto\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        local_files_only\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e(model_path \u003cspan style=\"color:#f92672\"\u003e!=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Qwen/Qwen2.5-Omni-7B\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    processor \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Qwen2_5OmniProcessor\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efrom_pretrained(model_path)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 3. 系统提示（必须包含语音生成能力声明）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    system_prompt \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;system\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e: [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\u0026#34;\u003c/span\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 4. 构建对话（纯文本输入）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    conversation \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e system_prompt \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;user\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e: [{\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e: text_input}]}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 5. 处理输入数据\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    text \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e processor\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eapply_chat_template(conversation, add_generation_prompt\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e, tokenize\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    audios, images, videos \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e process_mm_info(conversation, use_audio_in_video\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 6. 生成语音\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    inputs \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e processor(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        text\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003etext, audio\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eaudios, images\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eimages, videos\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003evideos,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        return_tensors\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;pt\u0026#34;\u003c/span\u003e, padding\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e, use_audio_in_video\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    )\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto(model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edevice, model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edtype)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ewith\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eno_grad():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        text_ids, audio \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egenerate(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003einputs,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            speaker\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003espeaker,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            do_sample\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e,  \u003cspan style=\"color:#75715e\"\u003e# 启用采样模式以使用temperature/top_p\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            temperature\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.8\u003c/span\u003e,  \u003cspan style=\"color:#75715e\"\u003e# 控制随机性（0.5-1.0较自然）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            top_p\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.95\u003c/span\u003e,       \u003cspan style=\"color:#75715e\"\u003e# 核采样参数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            max_new_tokens\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1024\u003c/span\u003e,  \u003cspan style=\"color:#75715e\"\u003e# 控制语音时长（约15秒）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            use_audio_in_video\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 7. 保存结果\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    os\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emakedirs(os\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epath\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edirname(output_audio_path), exist_ok\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    sf\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ewrite(output_audio_path, audio\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ereshape(\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecpu()\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enumpy(), samplerate\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e24000\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    print(\u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;✅ 生成完成：\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003eoutput_audio_path\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    print(\u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;📄 生成文本：\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003eprocessor\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebatch_decode(text_ids, skip_special_tokens\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e)[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e __name__ \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 示例输入（可替换为任意文本）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    input_text \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;你好，这是Qwen2.5-Omni的文本转语音示例。祝你使用愉快！\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 调用函数（指定输出路径和语音类型）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    text_to_speech(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        input_text,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        output_audio_path\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;output/hello_qwen.wav\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        speaker\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Chelsie\u0026#34;\u003c/span\u003e  \u003cspan style=\"color:#75715e\"\u003e# 可选\u0026#34;Ethan\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    )\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e运行脚本\u003c/p\u003e","title":"Qwen2.5-Omni-7B 文本转语音部署指南"}]