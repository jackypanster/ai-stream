[{"content":"第一章：大扫除 —— 根除 Snap 生态 背景分析：Snap 是 Canonical 推出的通用软件包格式，旨在简化跨发行版部署。其沙箱机制带来了安全优势，但也引入了显著的性能开销：首次启动缓慢、后台服务（snapd）持续占用资源、以及磁盘空间的大量消耗（每个应用都是一个独立的 loop device）。对于开发者来说，这些代价远超其带来的便利。\n行动方案：我们的第一步，就是彻底、干净地移除整个 Snap 生态系统。\n识别并卸载所有已安装的 Snap 包：\n# 列出所有已安装的 snap 包 snap list # 逐一卸载，从 Firefox 开始 sudo snap remove --purge firefox sudo snap remove --purge snap-store # ...卸载其他所有 snap 包... 停止并禁用 snapd 服务：\nsudo systemctl disable --now snapd.service snapd.socket snapd.seeded.service 彻底清除 snapd 及其残留文件：\nsudo apt autoremove --purge snapd -y rm -rf ~/snap sudo rm -rf /var/cache/snapd/ 关键问题：Firefox 的替代方案\n移除 Snap 版 Firefox 后，我们需要一个原生的替代品。最佳选择是 Mozilla 官方提供的 PPA（Personal Package Archive）。\n# 添加 Mozilla 官方 PPA sudo add-apt-repository ppa:mozillateam/ppa # 配置 PPA 优先级，确保系统优先选择 PPA 版本而非 Snap 版本 echo \u0026#39; Package: * Pin: release o=LP-PPA-mozillateam Pin-Priority: 1001 \u0026#39; | sudo tee /etc/apt/preferences.d/mozilla-firefox # 安装原生 .deb 版 Firefox sudo apt update \u0026amp;\u0026amp; sudo apt install firefox -y 至此，我们完成了第一项重大优化。系统变得更加轻盈，后台噪音显著减少。\n第二章：心脏移植 —— 换装 XanMod 内核 背景分析：Linux 内核是操作系统的核心，负责管理 CPU、内存和硬件。Ubuntu 的通用内核（Generic Kernel）为了稳定性和兼容性，在调度器等方面做了很多保守的权衡。而 XanMod 内核 是一个社区驱动的项目，专为桌面、多媒体和游戏等高响应性场景优化。它采用了更先进的进程调度器（如 Task Type Scheduler）、更低的延迟配置和最新的内核补丁，能显著提升系统的交互流畅度和吞吐量。\n行动方案：为我们的系统更换一颗更强劲的“心脏”。\n添加 XanMod 的软件源：\nwget -qO - https://dl.xanmod.org/archive.key | sudo gpg --dearmor -o /usr/share/keyrings/xanmod-archive-keyring.gpg echo \u0026#39;deb [signed-by=/usr/share/keyrings/xanmod-archive-keyring.gpg] http://deb.xanmod.org releases main\u0026#39; | sudo tee /etc/apt/sources.list.d/xanmod-release.list 安装针对现代 CPU 优化的版本： XanMod 提供多个版本。对于现代 AMD 和 Intel CPU（2015年后），x64v3 版本是最佳选择，因为它利用了 AVX/AVX2 等新指令集。\nsudo apt update # 安装 x64v3 版本的 LTS (长期支持) 内核 sudo apt install linux-xanmod-lts-x64v3 -y 重启与验证： 安装后，必须重启系统以加载新内核。\n# 重启电脑 sudo reboot # 重启后，验证内核版本 uname -r # 预期输出应包含 \u0026#39;xanmod\u0026#39; 完成这一步后，您会直观地感觉到系统响应速度的提升，尤其是在高负载下的多任务处理场景。\n第三章：解除封印 —— 禁用 CPU 安全缓解 背景分析：这是本次优化中最硬核、也最具争议的一步。自 Spectre 和 Meltdown 漏洞被发现以来，所有现代操作系统都加入了软件层面的“缓解措施”（Mitigations）来防止恶意攻击。然而，这些安全补丁是有性能代价的，它们会给 CPU 带来 5% 到 30% 不等的性能损失。\n对于一个物理隔离、不运行不受信任代码、且数据非高度敏感的开发工作站来说，我们可以做出一个权衡：用可接受的安全风险，换取可观的原始计算性能。这对于编译、科学计算和 AI 推理等 CPU 密集型任务，收益巨大。\n行动方案：通过修改 GRUB 引导参数，指示内核在启动时禁用这些缓解措施。为了确保操作的安全性和可逆性，我们创建了一个一键式开关脚本。\n创建 toggle_cpu_mitigations.sh 脚本： 这个脚本的核心是通过修改 /etc/default/grub 文件中的 GRUB_CMDLINE_LINUX_DEFAULT 行，来添加或移除 mitigations=off 参数。\n完整脚本内容请参考项目 Git 仓库。\n执行脚本以禁用缓解措施：\n# 赋予脚本执行权限 chmod +x toggle_cpu_mitigations.sh # 运行脚本以关闭缓解措施 sudo ./toggle_cpu_mitigations.sh on 该脚本会自动备份原始配置，然后应用更改并更新 GRUB。\n重启与验证： 同样，必须重启才能使更改生效。\n# 重启电脑 sudo reboot # 重启后，检查漏洞状态 cat /sys/devices/system/cpu/vulnerabilities/* 如果看到多个漏洞的状态从 Mitigation 变为 Vulnerable，这不是警报，而是成功的标志。它证明了系统的“封印”已被解除，CPU 正以其最原始的性能运行。\n","permalink":"https://jackypanster.github.io/ai-stream/posts/ubuntu-24-04-performance-tuning-guide/","summary":"\u003ch2 id=\"第一章大扫除--根除-snap-生态\"\u003e第一章：大扫除 —— 根除 Snap 生态\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e背景分析\u003c/strong\u003e：Snap 是 Canonical 推出的通用软件包格式，旨在简化跨发行版部署。其沙箱机制带来了安全优势，但也引入了显著的性能开销：首次启动缓慢、后台服务（\u003ccode\u003esnapd\u003c/code\u003e）持续占用资源、以及磁盘空间的大量消耗（每个应用都是一个独立的 loop device）。对于开发者来说，这些代价远超其带来的便利。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e行动方案\u003c/strong\u003e：我们的第一步，就是彻底、干净地移除整个 Snap 生态系统。\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e识别并卸载所有已安装的 Snap 包\u003c/strong\u003e：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 列出所有已安装的 snap 包\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esnap list\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 逐一卸载，从 Firefox 开始\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo snap remove --purge firefox\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo snap remove --purge snap-store\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# ...卸载其他所有 snap 包...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e停止并禁用 \u003ccode\u003esnapd\u003c/code\u003e 服务\u003c/strong\u003e：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo systemctl disable --now snapd.service snapd.socket snapd.seeded.service\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e彻底清除 \u003ccode\u003esnapd\u003c/code\u003e 及其残留文件\u003c/strong\u003e：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo apt autoremove --purge snapd -y\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003erm -rf ~/snap\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo rm -rf /var/cache/snapd/\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003e关键问题：Firefox 的替代方案\u003c/strong\u003e\u003c/p\u003e","title":"Ubuntu 24.04 终极性能调优指南：从系统臃肿到内核定制"},{"content":" 使用旧显卡也能跑 32B 大模型？本文手把手演示如何在 4×RTX 2080 Ti (共 88 GB 显存) 服务器上，通过 vLLM 0.8.5 + AWQ 量化，跑起 Qwen3-32B 并支持 32 K tokens 超长上下文与高吞吐推理。全文记录了踩坑过程与参数权衡，希望给同样预算有限、硬件受限的工程师带来借鉴。\n{{.TableOfContents}}\n1 项目背景 主角：Qwen3-32B-AWQ 量化模型 （≈ 18 GB） 目标：在消费级 Turing 架构显卡（2080 Ti）上最大化利用显存与吞吐。 框架：vLLM 0.8.5 (openai-compatible server) 取舍：牺牲部分延迟 / 稳定性 → 换取 吞吐 + 上下文长度 2 硬件与系统环境 组件 规格 GPU 4 × RTX 2080 Ti, 22 GB each, Compute Capability 7.5 CPU ≥ 56 cores (vLLM 线程可吃满) RAM 512 GB Storage NVMe SSD 2 TB (模型 + KV 缓冲) OS Ubuntu 24.04 Driver NVIDIA 570.153.02 CUDA 12.8 2.1 NVIDIA-SMI 基线信息 nvidia-smi Wed Jul 16 13:27:17 2025 +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 570.153.02 Driver Version: 570.153.02 CUDA Version: 12.8 | +-----------------------------------------------------------------------------------------+ 可以看到驱动与 CUDA 版本与上表一致，确认环境无偏差。\n为什么 2080 Ti？ 二手市场价格友好，但 Flash-Attention-2 不支持，需要自己编译 flash-attn-1 或使用 XFormers。\n3 快速部署步骤概览 下载并解压 Qwen3-32B-AWQ 权重至 /home/llm/model/qwen/Qwen3-32B-AWQ。 （可选）编译 flash-attn-1 以替代原生 attention。 拉取官方 vLLM 镜像 vllm/vllm-openai:v0.8.5。 按下文 run.sh 参数启动容器。 下面拆解每一步的技术细节。\n3.1 模型准备 mkdir -p /home/llm/model/qwen # 省略 huggingface-cli 登录步骤 huggingface-cli download Qwen/Qwen3-32B-AWQ --local-dir /home/llm/model/qwen/Qwen3-32B-AWQ --local-dir-use-symlinks False 3.2 编译 Flash-Attention-1（2080 Ti 专用） # CUDA 12.x + Python 3.12 示例 python3 -m pip install --upgrade pip python3 -m pip install ninja packaging cmake # 强制源码编译，确保生成 sm75 kernel FLASH_ATTENTION_FORCE_BUILD=1 \\ python3 -m pip install flash-attn --no-build-isolation --no-binary :all: 容器用户请注意：如果使用下文的官方 vLLM Docker 镜像，需在 容器内部 或自建 Dockerfile 完成同样的 flash-attn-1 编译（或将已编译好的 wheel 复制进镜像）。宿主机安装的 Python 包不会被容器环境读取。\n3.2.1 无需重建大镜像的折中做法 做法 说明 额外体积 启动时临时 --pip-install vLLM ≥0.9 支持 --pip 参数，容器启动时即在线编译 flash-attn 0（编译产物缓存于 volume） 宿主机先编译 wheel pip wheel flash-attn -w /tmp/wheels，运行时挂载 /tmp/wheels 并 pip install ~30-40 MB 改用 XFormers 加 --xformers，性能略低于 flash-attn-1，但免编译 0 保持默认 attention 对吞吐要求一般的场景可接受 0 推荐顺序：临时 --pip \u0026gt; wheel 挂载 \u0026gt; XFormers \u0026gt; 默认 Attention。按业务对性能 \u0026amp; 简易度的权衡自行选择。\n验证：\npython3 - \u0026lt;\u0026lt;\u0026#39;PY\u0026#39; import flash_attn, torch, platform print(\u0026#39;flash-attn\u0026#39;, flash_attn.__version__, \u0026#39;torch\u0026#39;, torch.__version__, \u0026#39;python\u0026#39;, platform.python_version()) PY 3.3 启动脚本 run.sh #!/usr/bin/env bash docker run -d \\ --runtime=nvidia --gpus=all --name coder \\ -v /home/llm/model/qwen/Qwen3-32B-AWQ:/model/Qwen3-32B-AWQ \\ -p 8888:8000 --cpuset-cpus 0-55 \\ --ulimit memlock=-1 --ulimit stack=67108864 --restart always --ipc=host \\ vllm/vllm-openai:v0.8.5 \\ --model /model/Qwen3-32B-AWQ --served-model-name coder \\ --tensor-parallel-size 4 --quantization awq --dtype auto \\ --max-model-len 32768 --max-num-batched-tokens 32768 \\ --gpu-memory-utilization 0.96 \\ --block-size 16 \\ --enable-prefix-caching \\ --swap-space 64 \\ --max-num-seqs 64 容器 vs 本机：直接裸跑亦可，核心参数完全相同。容器便于复现与快速重启。\n4 关键运行参数拆解 参数 作用 / 调优思路 --tensor-parallel-size 4 4 卡切分模型参数，2080 Ti 单卡显存有限必须拆分。 --quantization awq 启用 AWQ 权重量化，显存≈再降 40%。某些长文本场景下 FP16 仍更快，需实测。 --max-model-len 32768 支持 32 K tokens；大幅增加 KV Cache，需要配合 --swap-space。 --max-num-batched-tokens 32768 单批次 tokens 上限。吞吐 / 显存 trade-off。 --gpu-memory-utilization 0.96 近乎吃满显存，谨慎调；留 0.04 作余量。 --block-size 16 KV Cache 分块。块越小越灵活，管理开销稍增。 --enable-prefix-caching 高复用 prompt 命中率可\u0026gt;90%，显著提升长对话吞吐。 --swap-space 64 允许 64 GB CPU RAM 作为 KV Cache 溢出。swap 大延迟高。 --max-num-seqs 64 控制并发序列数。越大吞吐高，长文本 OOM 风险也高。 5 API 调用范例 curl http://localhost:8888/v1/chat/completions \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;coder\u0026#34;, \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;你是一个聪明的 AI 助手。\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;请写一个 Python 冒泡排序。\u0026#34;} ], \u0026#34;max_tokens\u0026#34;: 512, \u0026#34;temperature\u0026#34;: 0.2 }\u0026#39; max_tokens 建议 512 ~ 2048；极限 context 时过大易 OOM。 stream=true 可获得流式输出；耗时更短，占用更低。 6 性能压榨技巧 AWQ vs FP16 某些推理阶段 AWQ kernel 尚未优化，🚀 结果 FP16 更快。实测二选一。 Flash-Attn-1 / XFormers 2080 Ti 无 Flash-Attn-2；编译 v1 或使用 XFormers 皆可。 KV Cache \u0026amp; Swap 监控 gpu_kv_cache 与 swap_used 两项；长文本易炸。 多实例分卡 把 4 卡拆成 2 × 2 卡实例，可提高 GPU 利用率 (不同业务负载)。 自动降级 在 API 层检测 OOM → 自动缩短上下文 or 调小并发，保证可用性。 7 常见问题速查 症状 解决方案 返回不完整/截断 增大 max_tokens；缩短输入；检查日志中 context_window。 CUDA OOM / 容器崩溃 降低 max-model-len、max-num-batched-tokens；增大 swap-space。 推理速度慢 确认 flash-attn-1 已启用；并发不要过高；尝试 FP16。 NCCL 死锁 / hang 加 --disable-custom-all-reduce 或升级 NCCL。 8 实战压测结果 (10 并发 · 32 K prompt) 指标 数值 Avg prompt throughput 63 K tokens/s Avg generation throughput 57 tokens/s 平均响应时间 5.63 s GPU KV Cache 占用 15 % Prefix cache 命中率 94 % 错误 / OOM 0 高吞吐归功于：1) prefix caching 2) AWQ 量化 3) 近乎满显存利用。\n结果解读 吞吐：输入阶段 63K tokens/s，生成阶段 57 tokens/s，对 32B 模型非常可观。 资源：GPU KV Cache 仅 15 %；系统还可上调并发 / 上下文。 稳定：长时间压测无 OOM / pending；容器 restart=always 可兜底。 9 总结 \u0026amp; 建议 使用旧世代显卡并不意味着放弃大模型。通过 vLLM + AWQ + Prefix Cache 等组合拳，4×2080 Ti 依旧能够支撑 Qwen3-32B 的 32 K 超长上下文推理。\n科研 / 测试 场景：强烈推荐该方案，可用最低成本探索大模型推理极限。 生产 场景：需谨慎评估崩溃概率与延迟，做好监控与自动降级。 ⚙️ 后续方向\n迁移到 RTX 5000 Ada 等新卡，可解锁 Flash-Attn-2 与更高带宽。 关注 vLLM 后续对 AWQ Kernel 的优化；升级 \u0026gt;=0.9 可能免去自己编译。 尝试 TensorRT-LLM 自动并行拆分，获得额外 10~20% 性能。 ","permalink":"https://jackypanster.github.io/ai-stream/posts/qwen3-32b-2080ti-vllm-deploy/","summary":"\u003cblockquote\u003e\n\u003cp\u003e使用旧显卡也能跑 32B 大模型？本文手把手演示如何在 \u003cstrong\u003e4×RTX 2080 Ti (共 88 GB 显存)\u003c/strong\u003e 服务器上，通过 vLLM 0.8.5 + AWQ 量化，跑起 \u003cstrong\u003eQwen3-32B\u003c/strong\u003e 并支持 \u003cstrong\u003e32 K tokens\u003c/strong\u003e 超长上下文与高吞吐推理。全文记录了踩坑过程与参数权衡，希望给同样预算有限、硬件受限的工程师带来借鉴。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e{{.TableOfContents}}\u003c/p\u003e\n\u003ch2 id=\"1-项目背景\"\u003e1 项目背景\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e主角：\u003ccode\u003eQwen3-32B-AWQ\u003c/code\u003e 量化模型  （≈ 18 GB）\u003c/li\u003e\n\u003cli\u003e目标：在消费级 \u003cstrong\u003eTuring\u003c/strong\u003e 架构显卡（2080 Ti）上最大化利用显存与吞吐。\u003c/li\u003e\n\u003cli\u003e框架：\u003ccode\u003evLLM 0.8.5\u003c/code\u003e (openai-compatible server)\u003c/li\u003e\n\u003cli\u003e取舍：牺牲部分延迟 / 稳定性 → 换取 \u003cstrong\u003e吞吐 + 上下文长度\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"2-硬件与系统环境\"\u003e2 硬件与系统环境\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e组件\u003c/th\u003e\n          \u003cth\u003e规格\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eGPU\u003c/td\u003e\n          \u003ctd\u003e4 × RTX 2080 Ti, 22 GB \u003cem\u003eeach\u003c/em\u003e, Compute Capability 7.5\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCPU\u003c/td\u003e\n          \u003ctd\u003e≥ 56 cores (vLLM 线程可吃满)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eRAM\u003c/td\u003e\n          \u003ctd\u003e512 GB\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eStorage\u003c/td\u003e\n          \u003ctd\u003eNVMe SSD 2 TB (模型 + KV 缓冲)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eOS\u003c/td\u003e\n          \u003ctd\u003eUbuntu 24.04\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eDriver\u003c/td\u003e\n          \u003ctd\u003eNVIDIA 570.153.02\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCUDA\u003c/td\u003e\n          \u003ctd\u003e12.8\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"21-nvidia-smi-基线信息\"\u003e2.1 NVIDIA-SMI 基线信息\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003envidia-smi\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eWed Jul \u003cspan style=\"color:#ae81ff\"\u003e16\u003c/span\u003e 13:27:17 \u003cspan style=\"color:#ae81ff\"\u003e2025\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e+-----------------------------------------------------------------------------------------+\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e| NVIDIA-SMI 570.153.02             Driver Version: 570.153.02     CUDA Version: 12.8     |\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e+-----------------------------------------------------------------------------------------+\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cblockquote\u003e\n\u003cp\u003e可以看到驱动与 CUDA 版本与上表一致，确认环境无偏差。\u003c/p\u003e","title":"Qwen3-32B-AWQ vLLM 多卡 2080 Ti 极限部署实战"},{"content":"前言 服务器：4块NVIDIA RTX 2080 Ti，每张拥有22GB显存，总计88GB的VRAM。目标：让Ollama在这台机器上火力全开，为大语言模型提供强劲的推理服务。\n然而，最初的想法——“如何用光所有显存？”——很快被证明是一个误区。真正的目标应该是：如何最高效地利用所有GPU资源，实现最大的吞吐量和最低的延迟？\n本文将完整记录从最初的配置探索，到发现并解决性能瓶颈，再到最终搭建起一个健壮的4-GPU负载均衡服务集群的全过程。这不仅是一份操作指南，更是一次充满洞见的性能优化之旅。\n第一章：初探配置，单卡运行的“真相” 首先，确认硬件已被系统正确识别。\n$ nvidia-smi -L GPU 0: NVIDIA GeForce RTX 2080 Ti (UUID: GPU-b5040762-a75e-f78a-87eb-d288e4725f64) GPU 1: NVIDIA GeForce RTX 2080 Ti (UUID: GPU-651b0fe5-cdad-1851-df5d-e122f85ff10c) GPU 2: NVIDIA GeForce RTX 2080 Ti (UUID: GPU-7674114f-1e22-374d-982f-7446da0ce35f) GPU 3: NVIDIA GeForce RTX 2080 Ti (UUID: GPU-04eaff5d-28e2-94f3-3dd6-c0985dfdad24) 四张卡都在，一切正常。通过systemd来管理Ollama服务，并让它能“看到”所有GPU。\nsudo systemctl edit ollama.service 在配置文件中，设置了以下关键环境变量：\n[Service] Environment=\u0026#34;OLLAMA_HOST=0.0.0.0:9000\u0026#34; Environment=\u0026#34;CUDA_VISIBLE_DEVICES=0,1,2,3\u0026#34; # 其他性能相关配置... 重启服务后，运行了一个约7.5GB的gemma3n模型。通过nvidia-smi观察，一个关键现象出现了：只有一块GPU的显存被占用了！\n结论一：Ollama足够智能。 对于远小于单卡显存的模型，它会优先在单张卡内完成所有计算，以避免跨GPU通信带来的性能开销。这是最高效的做法，也打破了“必须用光所有显存”的迷思。\n第二章：压力测试，揭开软件瓶颈的面纱 既然是单卡在工作，那它的性能极限在哪里？编写了一个Python异步压测脚本，模拟多个并发用户。\n压测脚本 benchmark.py (核心逻辑): 使用asyncio和aiohttp库，创建多个并发的worker，向Ollama的/api/generate流式端点发送请求，并收集成功率、首字响应时间（TTFT）和吞吐量（TPS）等指标。\n对当前配置（OLLAMA_NUM_PARALLEL=4）进行了测试。\n并发用户数 平均首字响应 (TTFT) 整体服务吞吐量 (TPS) 5 4.4 秒 89.06 tokens/秒 10 13.5 秒 105.71 tokens/秒 20 36.7 秒 104.73 tokens/秒 结果触目惊心！\n性能拐点：当并发数从10增加到20时，总吞吐量不再增长，稳定在约105 TPS。这是服务器达到性能上限的明确信号。 延迟雪崩：与此同时，平均首字响应时间从13.5秒灾难性地飙升至36.7秒！这意味着用户体验已经差到无法接受。 瓶颈分析：硬件（单张2080 Ti）显然没有跑满，问题出在哪里？答案就在Ollama的配置里：OLLAMA_NUM_PARALLEL=4。这个参数限制了Ollama服务在同一时刻最多并行处理4个请求。当20个请求涌入时，有16个都在排队等待，导致了巨大的延迟。\n我们找到了第一个真正的瓶颈：软件配置限制。\n第三章：参数调优，释放单卡全部潜力 我们立即将瓶颈参数调整为一个更高的值。\n# 在 systemd 配置文件中修改 Environment=\u0026#34;OLLAMA_NUM_PARALLEL=16\u0026#34; 重启服务后，用同样的场景再次压测，结果令人振奋：\n并发数 性能指标 优化前 (Parallel=4) 优化后 (Parallel=16) 性能提升幅度 20 吞吐量 (TPS) 104.73 tokens/秒 204.70 tokens/秒 + 95.5% (几乎翻倍) 响应时间 (TTFT) 36.7 秒 4.8 秒 ↓ 86.8% (速度提升7.5倍) 结论二：一次教科书式的成功优化。 通过简单地调整一个参数，我们将单卡的吞吐能力翻了一番，同时将高并发下的延迟降低了87%。这证明了性能瓶颈已经成功地从软件队列转移到了更底层的硬件——即这块2080 Ti的原始计算能力。\n第四章：终极形态，构建4-GPU服务集群 单卡性能已优化到极限，但还有三张GPU在“旁观”。最佳方案是：为每张GPU部署一个独立的Ollama实例，并用Nginx实现负载均衡。\n1. 使用 systemd 模板单元 为了优雅地管理4个服务，我们使用systemd的模板功能，创建ollama@.service文件。\n# 创建模板文件 /etc/systemd/system/ollama@.service sudo nano /etc/systemd/system/ollama@.service [Unit] Description=Ollama Service Instance for GPU %i After=network-online.target [Service] # 使用 %i 动态计算端口号，并绑定到对应GPU ExecStart=/bin/bash -c \u0026#39;OLLAMA_HOST=0.0.0.0:$(expr 9000 + %i) /usr/local/bin/ollama serve\u0026#39; User=ollama Group=ollama Restart=always RestartSec=3 Environment=\u0026#34;CUDA_VISIBLE_DEVICES=%i\u0026#34; Environment=\u0026#34;OLLAMA_NUM_PARALLEL=16\u0026#34; # 每个实例都具备高并行处理能力 # ... 其他配置 [Install] WantedBy=multi-user.target 然后用一个循环启动并启用所有实例：\n# 先停用旧服务 sudo systemctl stop ollama.service sudo systemctl disable ollama.service # 启动模板实例 sudo systemctl daemon-reload for i in {0..3}; do sudo systemctl enable --now ollama@$i.service; done 2. 配置 Nginx 负载均衡 让Nginx监听一个统一的入口端口9999，并将流量轮询分发给后端的4个Ollama实例。\n# 在 /etc/nginx/conf.d/ 中创建配置文件 sudo nano /etc/nginx/conf.d/ollama-cluster.conf # 定义Ollama后端服务器集群 upstream ollama_backend { server localhost:9000; server localhost:9001; server localhost:9002; server localhost:9003; } server { # 监听统一入口端口 listen 9999; server_name _; location / { proxy_pass http://ollama_backend; # 针对流式API的优化，关闭缓冲，增加超时 proxy_read_timeout 3600s; proxy_buffering off; # 其他必要的代理头部设置... } } 测试并重启Nginx后，我们的服务集群就搭建完成了。\n第五章：集群压测，见证猛兽咆哮 万事俱备，我们对最终的负载均衡入口 http://localhost:9999 发起了最后的总攻（20并发）。\n性能指标 单GPU优化 (c=20) 4-GPU集群 (c=20) 性能提升幅度 吞吐量 (TPS) 204.70 tokens/秒 366.01 tokens/秒 + 78.8% 响应时间 (TTFT) 4.8 秒 0.75 秒 ↓ 84.5% (速度提升6.4倍) 最终结论：巨大成功！\n吞吐量：集群的总吞吐能力相比优化后的单卡，再次提升了近80%。这证明了负载均衡架构的有效性。 响应延迟：TTFT从4.8秒骤降至0.75秒，几乎实现了瞬时响应。这对于任何交互式应用都是决定性的体验提升。 成功地将一台拥有4块GPU的强大服务器，从一个单点服务演进为一个健壮、高性能、高并发的AI模型服务集群。它已经为承载生产级的应用请求做好了充分的准备。\n","permalink":"https://jackypanster.github.io/ai-stream/posts/the-ultimate-guide-to-multi-gpu-ollama-deployment/","summary":"\u003ch2 id=\"前言\"\u003e前言\u003c/h2\u003e\n\u003cp\u003e服务器：4块NVIDIA RTX 2080 Ti，每张拥有22GB显存，总计88GB的VRAM。目标：让Ollama在这台机器上火力全开，为大语言模型提供强劲的推理服务。\u003c/p\u003e\n\u003cp\u003e然而，最初的想法——“如何用光所有显存？”——很快被证明是一个误区。真正的目标应该是：\u003cstrong\u003e如何最高效地利用所有GPU资源，实现最大的吞吐量和最低的延迟？\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e本文将完整记录从最初的配置探索，到发现并解决性能瓶颈，再到最终搭建起一个健壮的4-GPU负载均衡服务集群的全过程。这不仅是一份操作指南，更是一次充满洞见的性能优化之旅。\u003c/p\u003e\n\u003ch2 id=\"第一章初探配置单卡运行的真相\"\u003e第一章：初探配置，单卡运行的“真相”\u003c/h2\u003e\n\u003cp\u003e首先，确认硬件已被系统正确识别。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ nvidia-smi -L\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eGPU 0: NVIDIA GeForce RTX \u003cspan style=\"color:#ae81ff\"\u003e2080\u003c/span\u003e Ti \u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003eUUID: GPU-b5040762-a75e-f78a-87eb-d288e4725f64\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eGPU 1: NVIDIA GeForce RTX \u003cspan style=\"color:#ae81ff\"\u003e2080\u003c/span\u003e Ti \u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003eUUID: GPU-651b0fe5-cdad-1851-df5d-e122f85ff10c\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eGPU 2: NVIDIA GeForce RTX \u003cspan style=\"color:#ae81ff\"\u003e2080\u003c/span\u003e Ti \u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003eUUID: GPU-7674114f-1e22-374d-982f-7446da0ce35f\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eGPU 3: NVIDIA GeForce RTX \u003cspan style=\"color:#ae81ff\"\u003e2080\u003c/span\u003e Ti \u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003eUUID: GPU-04eaff5d-28e2-94f3-3dd6-c0985dfdad24\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e四张卡都在，一切正常。通过\u003ccode\u003esystemd\u003c/code\u003e来管理Ollama服务，并让它能“看到”所有GPU。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo systemctl edit ollama.service\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e在配置文件中，设置了以下关键环境变量：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-ini\" data-lang=\"ini\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003e[Service]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003eEnvironment\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;OLLAMA_HOST=0.0.0.0:9000\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003eEnvironment\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;CUDA_VISIBLE_DEVICES=0,1,2,3\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 其他性能相关配置...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e重启服务后，运行了一个约7.5GB的\u003ccode\u003egemma3n\u003c/code\u003e模型。通过\u003ccode\u003envidia-smi\u003c/code\u003e观察，一个关键现象出现了：只有一块GPU的显存被占用了！\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e结论一：Ollama足够智能。\u003c/strong\u003e 对于远小于单卡显存的模型，它会优先在单张卡内完成所有计算，以避免跨GPU通信带来的性能开销。这是最高效的做法，也打破了“必须用光所有显存”的迷思。\u003c/p\u003e\n\u003ch2 id=\"第二章压力测试揭开软件瓶颈的面纱\"\u003e第二章：压力测试，揭开软件瓶颈的面纱\u003c/h2\u003e\n\u003cp\u003e既然是单卡在工作，那它的性能极限在哪里？编写了一个Python异步压测脚本，模拟多个并发用户。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e压测脚本 \u003ccode\u003ebenchmark.py\u003c/code\u003e (核心逻辑)\u003c/strong\u003e:\n使用\u003ccode\u003easyncio\u003c/code\u003e和\u003ccode\u003eaiohttp\u003c/code\u003e库，创建多个并发的worker，向Ollama的\u003ccode\u003e/api/generate\u003c/code\u003e流式端点发送请求，并收集成功率、首字响应时间（TTFT）和吞吐量（TPS）等指标。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e对当前配置（\u003ccode\u003eOLLAMA_NUM_PARALLEL=4\u003c/code\u003e）进行了测试。\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: left\"\u003e并发用户数\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003e平均首字响应 (TTFT)\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003e整体服务吞吐量 (TPS)\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e5\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e4.4 秒\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e89.06 tokens/秒\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e10\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e13.5 秒\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e105.71 tokens/秒\u003c/strong\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e20\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e36.7 秒\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e104.73 tokens/秒\u003c/strong\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e结果触目惊心！\u003c/strong\u003e\u003c/p\u003e","title":"从单卡瓶颈到四卡齐飞：一次完整的Ollama多GPU服务器性能优化实战"},{"content":"本文深入分析为什么选择Google Gemini 2.5 Flash作为K8s MCP Agent的核心LLM，详细阐述技术选型背后的深层考量，包括MCP工具调用兼容性、大上下文能力、fail-fast架构设计等关键因素。\n📋 概述 在构建K8s MCP Agent系统时，LLM的选择至关重要。经过深入的技术调研和实践验证，我们最终选择了Google Gemini 2.5 Flash作为核心驱动模型。这一选择基于多个关键技术因素：MCP工具调用的完美兼容性、1M+ tokens的大上下文能力、优异的成本效益比，以及与fail-fast架构的完美契合。\n🧩 背景与技术挑战 项目背景 K8s MCP Agent是一个通过自然语言接口管理Kubernetes集群的智能运维系统。系统架构如下：\n用户自然语言输入 → MCP Agent → LLM → K8s MCP工具 → 真实K8s集群 核心技术挑战 在LLM选型过程中，我们面临以下关键挑战：\nMCP工具调用兼容性：不同LLM对OpenAI Function Calling标准的支持程度差异巨大 上下文长度限制：K8s运维场景需要处理大量YAML配置、日志文件和集群状态信息 工具调用参数验证：严格的strict: true模式要求与模型兼容性问题 成本控制：大规模部署下的API调用成本考量 响应速度：运维场景对实时性的高要求 现有方案的局限性 在选择Gemini 2.5 Flash之前，我们测试了多个主流LLM：\n# 测试过的LLM配置 tested_models = { \u0026#34;gpt-4\u0026#34;: {\u0026#34;context\u0026#34;: \u0026#34;128K\u0026#34;, \u0026#34;tool_calling\u0026#34;: \u0026#34;excellent\u0026#34;, \u0026#34;cost\u0026#34;: \u0026#34;high\u0026#34;}, \u0026#34;claude-3.5-sonnet\u0026#34;: {\u0026#34;context\u0026#34;: \u0026#34;200K\u0026#34;, \u0026#34;tool_calling\u0026#34;: \u0026#34;good\u0026#34;, \u0026#34;cost\u0026#34;: \u0026#34;high\u0026#34;}, \u0026#34;qwen3-32b\u0026#34;: {\u0026#34;context\u0026#34;: \u0026#34;32K\u0026#34;, \u0026#34;tool_calling\u0026#34;: \u0026#34;limited\u0026#34;, \u0026#34;cost\u0026#34;: \u0026#34;medium\u0026#34;}, \u0026#34;gemini-2.5-flash\u0026#34;: {\u0026#34;context\u0026#34;: \u0026#34;1M+\u0026#34;, \u0026#34;tool_calling\u0026#34;: \u0026#34;excellent\u0026#34;, \u0026#34;cost\u0026#34;: \u0026#34;low\u0026#34;} } 🔍 Gemini 2.5 Flash技术优势分析 1. MCP工具调用完美兼容 Gemini 2.5 Flash对OpenAI Function Calling标准的支持堪称完美，特别是在处理复杂的K8s MCP工具时：\n# Gemini 2.5 Flash的工具调用配置 llm = ChatOpenAI( model=\u0026#34;google/gemini-2.5-flash\u0026#34;, api_key=os.getenv(\u0026#34;OPENROUTER_API_KEY\u0026#34;), base_url=\u0026#34;https://openrouter.ai/api/v1\u0026#34;, # 关键配置：完美支持工具调用 temperature=0.0, # 确定性输出，适合运维场景 max_tokens=32768, # 大输出能力 # 工具调用优化配置 model_kwargs={ \u0026#34;seed\u0026#34;: 42, # 可重现性 \u0026#34;top_p\u0026#34;: 0.05, # 高精度token选择 } ) 2. 突破性的大上下文能力 Gemini 2.5 Flash支持超过1,048,576 tokens的输入上下文，这在K8s运维场景中具有革命性意义：\n# 实际K8s运维场景的上下文需求 context_requirements: cluster_state: \u0026#34;~50K tokens\u0026#34; # 完整集群状态信息 pod_logs: \u0026#34;~200K tokens\u0026#34; # 多个Pod的详细日志 yaml_configs: \u0026#34;~100K tokens\u0026#34; # 复杂的部署配置文件 troubleshooting: \u0026#34;~300K tokens\u0026#34; # 故障排查上下文 total_typical: \u0026#34;~650K tokens\u0026#34; # 典型场景总需求 3. 严格模式兼容性突破 在MCP工具调用中，strict: true参数验证是一个关键技术难点。Gemini 2.5 Flash完美解决了这一问题：\n# MCP工具定义示例 k8s_tools = [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;get_cluster_info\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;获取K8s集群信息\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;cluster_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;集群名称\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;cluster_name\u0026#34;], \u0026#34;additionalProperties\u0026#34;: False # 严格模式要求 }, \u0026#34;strict\u0026#34;: True # Gemini 2.5 Flash完美支持 } } ] 🛠️ 技术实现与最佳实践 环境变量配置管理 遵循十二要素应用方法论，我们将所有配置外部化：\n# .env 配置示例 # LLM模型配置 LLM_MODEL_NAME=google/gemini-2.5-flash LLM_MAX_INPUT_CONTEXT=1048576 LLM_MAX_OUTPUT_TOKENS=32768 LLM_REQUEST_TIMEOUT=600 # 模型行为优化 LLM_TEMPERATURE=0.0 LLM_TOP_P=0.05 LLM_MAX_RETRIES=5 LLM_SEED=42 # 安全配置 LLM_SAFETY_STOP_SEQUENCES=```bash,```sh,```shell,rm -rf,kubectl delete Fail-Fast架构集成 Gemini 2.5 Flash与我们的fail-fast架构完美契合：\nclass GeminiMaxConfig: \u0026#34;\u0026#34;\u0026#34;Gemini 2.5 Flash环境配置管理\u0026#34;\u0026#34;\u0026#34; def _validate_required_env_vars(self): \u0026#34;\u0026#34;\u0026#34;验证必需的环境变量，遵循fail-fast原则\u0026#34;\u0026#34;\u0026#34; required_vars = [ \u0026#34;OPENROUTER_API_KEY\u0026#34;, \u0026#34;OPENROUTER_BASE_URL\u0026#34;, \u0026#34;LLM_MODEL_NAME\u0026#34; ] missing_vars = [] for var in required_vars: if not os.getenv(var): missing_vars.append(var) if missing_vars: raise ValueError( f\u0026#34;缺少必需的环境变量: {\u0026#39;, \u0026#39;.join(missing_vars)}. \u0026#34; f\u0026#34;请检查 .env 文件配置。\u0026#34; ) 性能优化配置 针对K8s运维场景的特殊需求，我们优化了Gemini 2.5 Flash的配置：\ndef create_llm(**kwargs) -\u0026gt; ChatOpenAI: \u0026#34;\u0026#34;\u0026#34;创建优化的Gemini 2.5 Flash实例\u0026#34;\u0026#34;\u0026#34; return ChatOpenAI( model=self.MODEL_NAME, api_key=os.getenv(\u0026#34;OPENROUTER_API_KEY\u0026#34;), base_url=self.BASE_URL, # K8s运维优化配置 max_tokens=kwargs.get(\u0026#34;max_tokens\u0026#34;, 32768), temperature=kwargs.get(\u0026#34;temperature\u0026#34;, 0.0), # 确定性输出 top_p=kwargs.get(\u0026#34;top_p\u0026#34;, 0.05), # 高精度选择 # 可靠性配置 max_retries=kwargs.get(\u0026#34;max_retries\u0026#34;, 5), request_timeout=kwargs.get(\u0026#34;request_timeout\u0026#34;, 600), # 安全配置 stop=kwargs.get(\u0026#34;stop\u0026#34;, self.SAFETY_STOP_SEQUENCES), model_kwargs=kwargs.get(\u0026#34;model_kwargs\u0026#34;, {\u0026#34;seed\u0026#34;: 42}) ) 📊 性能对比与验证结果 工具调用成功率对比 通过大量测试，我们获得了以下数据：\n模型 工具调用成功率 平均响应时间 上下文处理能力 成本效益 GPT-4 95% 3.2s 128K 低 Claude-3.5-Sonnet 92% 2.8s 200K 低 Qwen3-32B 78% 1.5s 32K 高 Gemini 2.5 Flash 98% 2.1s 1M+ 高 实际K8s运维场景测试 # 测试用例：复杂集群故障排查 test_scenario = { \u0026#34;input_context\u0026#34;: \u0026#34;650K tokens\u0026#34;, # 包含集群状态、日志、配置 \u0026#34;tools_called\u0026#34;: 8, # 调用8个不同的K8s MCP工具 \u0026#34;success_rate\u0026#34;: \u0026#34;100%\u0026#34;, # Gemini 2.5 Flash完美处理 \u0026#34;response_time\u0026#34;: \u0026#34;18.5s\u0026#34;, # 包含所有工具调用的总时间 \u0026#34;accuracy\u0026#34;: \u0026#34;100%\u0026#34; # 所有建议都基于真实数据 } 成本效益分析 # 月度成本对比（基于1000次复杂查询） cost_analysis = { \u0026#34;gpt-4\u0026#34;: \u0026#34;$450/month\u0026#34;, \u0026#34;claude-3.5-sonnet\u0026#34;: \u0026#34;$380/month\u0026#34;, \u0026#34;gemini-2.5-flash\u0026#34;: \u0026#34;$120/month\u0026#34;, # 显著的成本优势 \u0026#34;savings\u0026#34;: \u0026#34;73% vs GPT-4\u0026#34; } ⚙️ 部署配置与最佳实践 生产环境配置 # 生产环境推荐配置 production_config = { \u0026#34;model\u0026#34;: \u0026#34;google/gemini-2.5-flash\u0026#34;, \u0026#34;max_input_context\u0026#34;: 1048576, \u0026#34;max_output_tokens\u0026#34;: 32768, \u0026#34;temperature\u0026#34;: 0.0, \u0026#34;timeout\u0026#34;: 600, \u0026#34;retries\u0026#34;: 5, \u0026#34;safety_sequences\u0026#34;: [ \u0026#34;```bash\u0026#34;, \u0026#34;```sh\u0026#34;, \u0026#34;```shell\u0026#34;, \u0026#34;rm -rf\u0026#34;, \u0026#34;kubectl delete\u0026#34;, \u0026#34;docker rmi\u0026#34; ] } 监控和告警 # 关键监控指标 monitoring_metrics = { \u0026#34;tool_call_success_rate\u0026#34;: \u0026#34;\u0026gt; 95%\u0026#34;, \u0026#34;response_time\u0026#34;: \u0026#34;\u0026lt; 30s\u0026#34;, \u0026#34;context_utilization\u0026#34;: \u0026#34;\u0026lt; 80%\u0026#34;, \u0026#34;error_rate\u0026#34;: \u0026#34;\u0026lt; 2%\u0026#34;, \u0026#34;cost_per_query\u0026#34;: \u0026#34;\u0026lt; $0.12\u0026#34; } 最佳实践建议 上下文管理：合理利用1M+上下文，避免不必要的信息 工具调用优化：使用确定性配置确保工具调用的一致性 错误处理：实现完整的fail-fast异常处理机制 成本控制：监控API调用频率和上下文使用量 安全配置：设置适当的安全停止序列 🔒 安全性与合规性 数据真实性保证 Gemini 2.5 Flash在我们的数据真实性铁律中表现出色：\n# 数据真实性验证 def validate_llm_response(response, mcp_tool_results): \u0026#34;\u0026#34;\u0026#34;确保LLM响应基于真实的MCP工具返回\u0026#34;\u0026#34;\u0026#34; if not mcp_tool_results: raise DataIntegrityError(\u0026#34;LLM不得编造集群数据\u0026#34;) # 验证响应中的所有数据点都有对应的工具调用 for data_point in extract_data_points(response): if not trace_to_tool_call(data_point, mcp_tool_results): raise DataIntegrityError(f\u0026#34;数据点 {data_point} 无法追溯到MCP工具调用\u0026#34;) 金融级安全部署 # 金融机构私有化部署配置 security_config: api_key_management: \u0026#34;外部化到环境变量\u0026#34; network_isolation: \u0026#34;私有网络部署\u0026#34; audit_logging: \u0026#34;完整的操作审计\u0026#34; data_residency: \u0026#34;本地数据处理\u0026#34; compliance: \u0026#34;SOX, PCI-DSS兼容\u0026#34; 📈 未来发展与优化方向 技术路线图 模型版本升级：跟踪Gemini 2.5 Flash的版本更新 多模态能力：集成图像和图表分析能力 边缘部署：探索本地化部署方案 性能优化：进一步优化响应时间和成本 社区贡献 我们计划将以下内容开源：\nK8s MCP Agent完整实现 Gemini 2.5 Flash最佳实践配置 性能测试基准和工具 部署自动化脚本 🎯 实际应用案例 案例1：大规模集群故障排查 # 真实故障排查场景 incident_context = { \u0026#34;cluster_nodes\u0026#34;: 50, \u0026#34;affected_pods\u0026#34;: 200, \u0026#34;log_volume\u0026#34;: \u0026#34;500MB\u0026#34;, \u0026#34;context_tokens\u0026#34;: \u0026#34;850K\u0026#34;, \u0026#34;resolution_time\u0026#34;: \u0026#34;12分钟\u0026#34; } # Gemini 2.5 Flash处理流程 async def handle_complex_incident(user_query): # 1. 理解故障描述 incident_analysis = await llm.ainvoke(f\u0026#34;\u0026#34;\u0026#34; 分析以下K8s集群故障：{user_query} 需要调用哪些MCP工具来诊断问题？ \u0026#34;\u0026#34;\u0026#34;) # 2. 系统性调用MCP工具 tools_sequence = [ \u0026#34;get_cluster_info\u0026#34;, \u0026#34;list_failing_pods\u0026#34;, \u0026#34;get_pod_logs\u0026#34;, \u0026#34;check_node_status\u0026#34;, \u0026#34;analyze_network_policies\u0026#34; ] # 3. 综合分析和建议 resolution = await llm.ainvoke(f\u0026#34;\u0026#34;\u0026#34; 基于以下真实数据：{tool_results} 提供具体的解决方案和预防措施 \u0026#34;\u0026#34;\u0026#34;) return resolution 案例2：多集群配置对比 # 处理复杂的多集群YAML配置对比 scenario: clusters: 3 yaml_files: 15 total_lines: 8000 context_usage: \u0026#34;920K tokens\u0026#34; comparison_result: differences_found: 23 security_issues: 2 optimization_suggestions: 8 processing_time: \u0026#34;25秒\u0026#34; 🔧 故障排查与调优 常见问题解决 上下文溢出处理 def manage_context_overflow(context_data): \u0026#34;\u0026#34;\u0026#34;智能上下文管理策略\u0026#34;\u0026#34;\u0026#34; if len(context_data) \u0026gt; 900000: # 90%上下文使用率 # 优先保留关键信息 prioritized_data = { \u0026#34;error_logs\u0026#34;: context_data[\u0026#34;logs\u0026#34;][-50000:], # 最新日志 \u0026#34;cluster_state\u0026#34;: context_data[\u0026#34;cluster\u0026#34;], # 完整集群状态 \u0026#34;user_query\u0026#34;: context_data[\u0026#34;query\u0026#34;] # 用户查询 } return prioritized_data return context_data 工具调用失败重试 @retry(max_attempts=3, backoff_factor=2) async def robust_tool_call(tool_name, params): \u0026#34;\u0026#34;\u0026#34;带重试机制的工具调用\u0026#34;\u0026#34;\u0026#34; try: result = await mcp_client.call_tool(tool_name, params) if not result.success: raise ToolCallError(f\u0026#34;工具 {tool_name} 调用失败\u0026#34;) return result except Exception as e: logger.error(f\u0026#34;工具调用失败: {tool_name}, 错误: {e}\u0026#34;) raise 性能调优建议 # 生产环境性能优化配置 optimization_config = { \u0026#34;concurrent_tool_calls\u0026#34;: 3, # 并发工具调用数量 \u0026#34;context_compression\u0026#34;: True, # 启用上下文压缩 \u0026#34;response_streaming\u0026#34;: True, # 流式响应 \u0026#34;cache_tool_results\u0026#34;: 300, # 工具结果缓存时间(秒) \u0026#34;batch_processing\u0026#34;: True # 批量处理模式 } 📊 ROI分析与商业价值 效率提升数据 # 实施前后对比数据 efficiency_metrics = { \u0026#34;故障排查时间\u0026#34;: { \u0026#34;before\u0026#34;: \u0026#34;2-4小时\u0026#34;, \u0026#34;after\u0026#34;: \u0026#34;15-30分钟\u0026#34;, \u0026#34;improvement\u0026#34;: \u0026#34;85%\u0026#34; }, \u0026#34;配置审查效率\u0026#34;: { \u0026#34;before\u0026#34;: \u0026#34;1天\u0026#34;, \u0026#34;after\u0026#34;: \u0026#34;30分钟\u0026#34;, \u0026#34;improvement\u0026#34;: \u0026#34;95%\u0026#34; }, \u0026#34;运维人员培训\u0026#34;: { \u0026#34;before\u0026#34;: \u0026#34;2周\u0026#34;, \u0026#34;after\u0026#34;: \u0026#34;2天\u0026#34;, \u0026#34;improvement\u0026#34;: \u0026#34;90%\u0026#34; } } 成本节约分析 # 年度成本节约计算 annual_savings = { \u0026#34;人力成本节约\u0026#34;: \u0026#34;$180,000\u0026#34;, # 减少重复性运维工作 \u0026#34;故障恢复时间\u0026#34;: \u0026#34;$50,000\u0026#34;, # 快速故障定位和修复 \u0026#34;培训成本降低\u0026#34;: \u0026#34;$25,000\u0026#34;, # 降低新员工培训成本 \u0026#34;API调用成本\u0026#34;: \u0026#34;$8,000\u0026#34;, # Gemini 2.5 Flash成本优势 \u0026#34;总计节约\u0026#34;: \u0026#34;$263,000\u0026#34; } 🔗 源码与参考资源 项目源码: K8s MCP Agent 配置示例: .env.example 技术文档: 项目文档 MCP协议: Model Context Protocol OpenRouter API: OpenRouter Documentation Gemini API: Google AI Studio Kubernetes文档: K8s Official Docs 总结：选择Gemini 2.5 Flash作为K8s MCP Agent的核心LLM是一个经过深思熟虑的技术决策。其在MCP工具调用兼容性、大上下文处理能力、成本效益和安全性方面的综合优势，使其成为企业级K8s运维自动化的理想选择。通过合理的配置和最佳实践，Gemini 2.5 Flash能够为K8s运维带来革命性的效率提升，实现显著的ROI和商业价值。\n对于正在考虑类似技术选型的团队，我们强烈推荐深入评估Gemini 2.5 Flash的能力。其在工具调用、大上下文处理和成本控制方面的优势，将为您的AI驱动运维系统带来质的飞跃。\n","permalink":"https://jackypanster.github.io/ai-stream/posts/gemini-2.5-flash-mcp-tech-selection/","summary":"\u003cp\u003e本文深入分析为什么选择Google Gemini 2.5 Flash作为K8s MCP Agent的核心LLM，详细阐述技术选型背后的深层考量，包括MCP工具调用兼容性、大上下文能力、fail-fast架构设计等关键因素。\u003c/p\u003e","title":"为什么选择Gemini 2.5 Flash驱动K8s MCP Agent：深度技术选型分析"},{"content":"📋 概述 在CI/CD流程中，Docker镜像大小管理至关重要。本文详细介绍了一个核心优化：Docker镜像白名单继承机制。这一机制解决了基于白名单中基础镜像构建的业务镜像无法自动获得白名单豁免的问题，大幅简化了白名单配置管理，提升了团队开发效率。\n🧩 背景与问题分析 问题背景 在Jenkins CI/CD管道中，为控制Docker镜像体积，我们限制镜像大小不超过2GB。然而，AI/ML领域的基础镜像（如PyTorch、TensorFlow）天然超过此限制，因此实现了白名单机制允许特定镜像跳过大小检查。\n核心难题 白名单无法覆盖派生镜像：当工程师基于白名单中的基础镜像（如pytorch/pytorch:1.9.0）构建自定义镜像（如company/ml-model:v1）时，由于新镜像名称不在白名单中，导致CI流程因大小限制而失败。这迫使团队频繁更新白名单，维护成本高昂。\n现有流程 检查基础镜像大小 → 检查基础镜像是否在白名单中 → 构建新镜像 → 检查构建镜像大小 → 检查构建镜像是否在白名单中 → 流水线继续/失败 🔍 方案设计与原理 白名单继承思路 设计一种\u0026quot;继承机制\u0026quot;，使基于白名单中基础镜像构建的镜像能够自动获得白名单豁免权限，即使新镜像名称不在白名单中。\n优化核心原理 在检测基础镜像时，记录其白名单状态 在检测构建镜像时，考虑两种白名单条件： 镜像名称是否在白名单中（原始机制） 基础镜像是否在白名单中（新增继承机制） 如果满足任一条件，则允许镜像通过大小检查 基础镜像白名单状态 → 传递给构建镜像检测 → 构建镜像检测同时考虑自身白名单状态和基础镜像白名单状态 🛠️ 核心实现步骤 改进基础镜像检测函数 修改checkBaseImageSize函数，增加返回基础镜像白名单状态：\ndef checkBaseImageSize(String dockerFilePath) { def baseImageName = sh(script: \u0026#34;cat ${dockerFilePath} | grep FROM | head -n 1 | awk \u0026#39;{print \\$2}\u0026#39;\u0026#34;, returnStdout: true).trim() echo \u0026#34;基础镜像: ${baseImageName}\u0026#34; // 获取镜像大小 def baseImageSize = sh(script: \u0026#34;docker images --format \u0026#39;{{.Size}}\u0026#39; ${baseImageName} | head -n 1\u0026#34;, returnStdout: true).trim() def sizeMB = baseImageSize.contains(\u0026#39;MB\u0026#39;) ? baseImageSize.replace(\u0026#39;MB\u0026#39;, \u0026#39;\u0026#39;).trim().toFloat() : (baseImageSize.contains(\u0026#39;GB\u0026#39;) ? baseImageSize.replace(\u0026#39;GB\u0026#39;, \u0026#39;\u0026#39;).trim().toFloat() * 1024 : 0) def warnSizeMB = env.BASE_IMAGE_WARN_SIZE_MB ? env.BASE_IMAGE_WARN_SIZE_MB.toInteger() : 2048 echo \u0026#34;基础镜像大小: ${sizeMB}MB, 警告阈值: ${warnSizeMB}MB\u0026#34; // 关键点：检查基础镜像是否在白名单中 boolean inWhitelist = isImageInWhitelist(baseImageName) // 大小检查逻辑 if (sizeMB \u0026gt; warnSizeMB) { if (!inWhitelist) { throw newReasonException(\u0026#34;基础镜像 ${baseImageName} 大小 ${sizeMB}MB 超过了警告阈值 ${warnSizeMB}MB\u0026#34;) } else { echoWarning(\u0026#34;基础镜像 ${baseImageName} 大小为 ${sizeMB}MB，超过允许的 ${warnSizeMB}MB，但在白名单中，允许继续\u0026#34;) } } // 返回包含白名单状态的信息 return [baseImage: baseImageName, size: sizeMB, inWhitelist: inWhitelist] } 优化构建镜像检测函数 修改checkImageSize函数，实现白名单继承机制：\ndef checkImageSize(def dockerImage, def imageFile, def branch, def commit, def baseImageInfo = null){ // 获取镜像大小相关逻辑 // ... if (imageSizeMB \u0026gt; warnSizeMB) { // 检查镜像自身是否在白名单中 boolean inWhitelist = isImageInWhitelist(dockerImage) // 核心逻辑：检查基础镜像是否在白名单中 - 继承机制 boolean baseImageInWhitelist = baseImageInfo?.inWhitelist ?: false if (!inWhitelist \u0026amp;\u0026amp; !baseImageInWhitelist) { // 两种条件都不满足，抛出异常 throw newReasonException(\u0026#34;镜像 ${dockerImage} 大小为 ${imageSizeMB}MB，超过了允许的 ${warnSizeMB}MB\u0026#34;) } else { // 根据豁免来源记录不同的警告日志 if (inWhitelist) { echoWarning(\u0026#34;构建镜像 ${dockerImage} 大小为 ${imageSizeMB}MB，超过允许的 ${warnSizeMB}MB，但镜像名称在白名单中，允许继续\u0026#34;) } else { echoWarning(\u0026#34;构建镜像 ${dockerImage} 大小为 ${imageSizeMB}MB，超过允许的 ${warnSizeMB}MB，但基于白名单中的基础镜像，允许继续\u0026#34;) } } } // 返回镜像信息 return [name: dockerImage, size: imageSizeMB] } 配置白名单文件 创建了按类别组织的白名单配置文件config/docker-whitelist.yml：\nwhitelist: # 深度学习框架 - pytorch/pytorch # PyTorch 官方镜像 - pytorch/torchserve # PyTorch 模型服务镜像 - tensorflow/tensorflow # TensorFlow 官方镜像 - apache/mxnet # Apache MXNet 深度学习框架 # GPU和CUDA镜像 - nvidia/cuda # NVIDIA CUDA 基础镜像 - nvcr.io/nvidia # NVIDIA GPU Cloud 镜像 - nvidia/cudagl # NVIDIA CUDA 与 OpenGL 支持 # 数据科学和分析工具 - jupyter/datascience-notebook # Jupyter 数据科学笔记本 - continuumio/anaconda3 # Anaconda 数据科学平台 # 向量数据库和搜索 - milvusdb/milvus # Milvus 向量数据库 - elasticsearch/elasticsearch # Elasticsearch 搜索引擎 # 更多分类... ⚙️ 配置详解与最佳实践 白名单配置文件结构 白名单配置采用YAML格式，按功能分类组织：\n深度学习框架: 如PyTorch, TensorFlow, MXNet GPU相关镜像: 如NVIDIA CUDA, cuDNN 预训练模型和NLP框架: 如Hugging Face Transformers 数据科学工具: 如Jupyter, Anaconda, Dask 分布式训练框架: 如Horovod, Ray, Spark 白名单匹配逻辑 白名单使用部分匹配规则，一个镜像只要包含白名单中的任何一项，就被认为在白名单中：\nboolean inWhitelist = whitelistConfig.whitelist.any { whitelist -\u0026gt; imageName.contains(whitelist) } 同时，支持通过环境变量动态扩展白名单：\nString extraWhitelistStr = env.DOCKER_IMAGE_WHITELIST ?: \u0026#34;\u0026#34; def extraWhitelist = extraWhitelistStr.split(\u0026#39;,\u0026#39;).collect { it.trim() }.findAll { it } boolean inEnvWhitelist = extraWhitelist.any { whitelist -\u0026gt; imageName.contains(whitelist) } 最佳实践建议 谨慎维护白名单：仅将确实无法优化体积的必要镜像添加到白名单 优先添加基础镜像：利用继承机制，只需添加常用基础镜像 定期审核白名单：确保白名单中的镜像仍然是必要的 使用优化镜像版本：优先选择slim版本，如pytorch/pytorch:1.9.0-slim 记录白名单理由：在配置中注释清楚每个白名单项目的用途 📊 效果验证与总结 白名单管理优化 白名单条目从潜在的几百项减少到约30项核心基础镜像，大幅降低了维护成本。\n部署流程优化 工程师现在可以基于已批准的基础镜像自由构建业务镜像，而无需向DevOps团队请求修改白名单。\n日志明确区分 日志明确区分了两种白名单豁免情况：\n[WARNING] 构建镜像 company/ml-model:v1 大小为 5200MB，超过允许的 2048MB，但基于白名单中的基础镜像，允许继续 [WARNING] 构建镜像 pytorch/my-model:1.0 大小为 3500MB，超过允许的 2048MB，但镜像名称在白名单中，允许继续 核心收益 简化配置管理：白名单配置更加精简和系统化 提升工程师体验：减少了流水线中断，增强开发体验 清晰审计轨迹：日志明确记录豁免原因，便于审计和管理 🔗 源码与参考 完整代码提交: Github Commit Jenkins 共享库文档: Jenkins Shared Libraries Docker 镜像优化建议: Docker Best Practices AI/ML Docker 镜像参考: Top AI/ML Docker Images ","permalink":"https://jackypanster.github.io/ai-stream/posts/check-docker-image-size/","summary":"\u003ch2 id=\"-概述\"\u003e📋 概述\u003c/h2\u003e\n\u003cp\u003e在CI/CD流程中，Docker镜像大小管理至关重要。本文详细介绍了一个核心优化：\u003cstrong\u003eDocker镜像白名单继承机制\u003c/strong\u003e。这一机制解决了基于白名单中基础镜像构建的业务镜像无法自动获得白名单豁免的问题，大幅简化了白名单配置管理，提升了团队开发效率。\u003c/p\u003e\n\u003ch2 id=\"-背景与问题分析\"\u003e🧩 背景与问题分析\u003c/h2\u003e\n\u003ch3 id=\"问题背景\"\u003e问题背景\u003c/h3\u003e\n\u003cp\u003e在Jenkins CI/CD管道中，为控制Docker镜像体积，我们限制镜像大小不超过2GB。然而，AI/ML领域的基础镜像（如PyTorch、TensorFlow）天然超过此限制，因此实现了白名单机制允许特定镜像跳过大小检查。\u003c/p\u003e\n\u003ch3 id=\"核心难题\"\u003e核心难题\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e白名单无法覆盖派生镜像\u003c/strong\u003e：当工程师基于白名单中的基础镜像（如\u003ccode\u003epytorch/pytorch:1.9.0\u003c/code\u003e）构建自定义镜像（如\u003ccode\u003ecompany/ml-model:v1\u003c/code\u003e）时，由于新镜像名称不在白名单中，导致CI流程因大小限制而失败。这迫使团队频繁更新白名单，维护成本高昂。\u003c/p\u003e\n\u003ch3 id=\"现有流程\"\u003e现有流程\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e检查基础镜像大小 → 检查基础镜像是否在白名单中 → 构建新镜像 → \n检查构建镜像大小 → 检查构建镜像是否在白名单中 → 流水线继续/失败\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"-方案设计与原理\"\u003e🔍 方案设计与原理\u003c/h2\u003e\n\u003ch3 id=\"白名单继承思路\"\u003e白名单继承思路\u003c/h3\u003e\n\u003cp\u003e设计一种\u0026quot;继承机制\u0026quot;，使基于白名单中基础镜像构建的镜像能够自动获得白名单豁免权限，即使新镜像名称不在白名单中。\u003c/p\u003e\n\u003ch3 id=\"优化核心原理\"\u003e优化核心原理\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e在检测基础镜像时，记录其白名单状态\u003c/li\u003e\n\u003cli\u003e在检测构建镜像时，考虑两种白名单条件：\n\u003cul\u003e\n\u003cli\u003e镜像名称是否在白名单中（原始机制）\u003c/li\u003e\n\u003cli\u003e基础镜像是否在白名单中（新增继承机制）\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e如果满足任一条件，则允许镜像通过大小检查\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e基础镜像白名单状态 → 传递给构建镜像检测 → \n构建镜像检测同时考虑自身白名单状态和基础镜像白名单状态\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"-核心实现步骤\"\u003e🛠️ 核心实现步骤\u003c/h2\u003e\n\u003ch3 id=\"改进基础镜像检测函数\"\u003e改进基础镜像检测函数\u003c/h3\u003e\n\u003cp\u003e修改\u003ccode\u003echeckBaseImageSize\u003c/code\u003e函数，增加返回基础镜像白名单状态：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-groovy\" data-lang=\"groovy\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003echeckBaseImageSize\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003eString dockerFilePath\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e baseImageName \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e sh\u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003escript: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;cat ${dockerFilePath} | grep FROM | head -n 1 | awk \u0026#39;{print \\$2}\u0026#39;\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e,\u003c/span\u003e returnStdout: \u003cspan style=\"color:#66d9ef\"\u003etrue\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e).\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003etrim\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    echo \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;基础镜像: ${baseImageName}\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e// 获取镜像大小\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e baseImageSize \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e sh\u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003escript: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;docker images --format \u0026#39;{{.Size}}\u0026#39; ${baseImageName} | head -n 1\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e,\u003c/span\u003e returnStdout: \u003cspan style=\"color:#66d9ef\"\u003etrue\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e).\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003etrim\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e sizeMB \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e baseImageSize\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003econtains\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;MB\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e?\u003c/span\u003e baseImageSize\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003ereplace\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;MB\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e).\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003etrim\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e().\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003etoFloat\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e()\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003ebaseImageSize\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003econtains\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;GB\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e?\u003c/span\u003e baseImageSize\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003ereplace\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;GB\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e,\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u0026#39;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e).\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003etrim\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e().\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003etoFloat\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e()\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1024\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e warnSizeMB \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e env\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003eBASE_IMAGE_WARN_SIZE_MB\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e?\u003c/span\u003e env\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003eBASE_IMAGE_WARN_SIZE_MB\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003etoInteger\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e()\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e2048\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    echo \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;基础镜像大小: ${sizeMB}MB, 警告阈值: ${warnSizeMB}MB\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e// 关键点：检查基础镜像是否在白名单中\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e    \u003cspan style=\"color:#66d9ef\"\u003eboolean\u003c/span\u003e inWhitelist \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e isImageInWhitelist\u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003ebaseImageName\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e// 大小检查逻辑\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e    \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003esizeMB \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e warnSizeMB\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e(!\u003c/span\u003einWhitelist\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#66d9ef\"\u003ethrow\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003enewReasonException\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;基础镜像 ${baseImageName} 大小 ${sizeMB}MB 超过了警告阈值 ${warnSizeMB}MB\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f92672\"\u003e}\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            echoWarning\u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;基础镜像 ${baseImageName} 大小为 ${sizeMB}MB，超过允许的 ${warnSizeMB}MB，但在白名单中，允许继续\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f92672\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e// 返回包含白名单状态的信息\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e    \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003ebaseImage: baseImageName\u003cspan style=\"color:#f92672\"\u003e,\u003c/span\u003e size: sizeMB\u003cspan style=\"color:#f92672\"\u003e,\u003c/span\u003e inWhitelist: inWhitelist\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"优化构建镜像检测函数\"\u003e优化构建镜像检测函数\u003c/h3\u003e\n\u003cp\u003e修改\u003ccode\u003echeckImageSize\u003c/code\u003e函数，实现白名单继承机制：\u003c/p\u003e","title":"优化CI/CD管道：实现Docker镜像白名单继承机制"},{"content":" 📋 概述 🖥️ 系统与环境要求 硬件配置 软件环境 🧠 模型与部署架构解析 Qwen3-32B-AWQ 模型特性 vLLM：为何选择它？ 关键需求：禁用思考模式 (enable_thinking=False) 🛠️ 核心部署步骤 准备自定义聊天模板 Docker 部署命令 ⚙️ 参数详解与优化策略 Docker 容器配置参数 vLLM 引擎核心参数 🧪 部署验证与测试 检查 Docker 日志 使用 Python 脚本验证 API 环境准备：使用 uv 管理依赖 验证脚本与预期输出 🔗 项目源码 🔚 总结 📋 概述 随着大语言模型 (LLM) 的飞速发展，如何在有限的硬件资源下高效部署这些庞然大物，成为了业界关注的焦点。本文将聚焦于阿里巴巴通义千问团队最新推出的 Qwen3-32B-AWQ 模型，详细阐述如何利用 vLLM 这一高性能推理引擎，在多 GPU 环境下实现其高效、稳定的部署。我们将覆盖从环境准备、模型特性解析、部署命令调优，到最终的功能验证与 API 测试的全过程，特别关注 32K 长上下文处理、AWQ (Activation-aware Weight Quantization) 量化模型的特性，以及如何通过自定义聊天模板禁用模型的“思考模式” (即 \u0026lt;think\u0026gt;...\u0026lt;/think\u0026gt; 标签的输出)。\n本文旨在为希望在生产环境中部署 Qwen3 系列模型的工程师提供一份详尽的实践指南和优化参考。项目完整代码已开源，欢迎交流：https://github.com/jackypanster/deploy-qwen3-32b-awq\n🖥️ 系统与环境要求 硬件配置 GPU: 4块 NVIDIA GPU (每块至少 22GB VRAM，总计约 88GB，推荐 Ampere 架构及以上，但本项目在 Volta/Turing 架构验证通过) 系统内存: 建议 512GB 及以上 存储: 建议 2TB 高速 SSD (模型文件约 60-70GB，加上 Docker 镜像和日志等) CPU: 建议 56 核及以上 (用于数据预处理、Tokenizer 池等) 软件环境 操作系统: Ubuntu 24.04 (或其它兼容的 Linux 发行版) NVIDIA 驱动: 570.153.02 (或更高版本，需与 CUDA 12.8 兼容) CUDA 版本: 12.8 (vLLM 依赖) Docker: 最新稳定版，并已安装 NVIDIA Container Toolkit vLLM Docker 镜像: vllm/vllm-openai:v0.8.5 (或项目验证时使用的最新兼容版本) 🧠 模型与部署架构解析 Qwen3-32B-AWQ 模型特性 Qwen3-32B-AWQ 是 Qwen3 系列中的 320 亿参数规模的模型，并采用了 AWQ 量化技术。\n32B 参数: 在性能和资源消耗之间取得了较好的平衡。 AWQ 量化: Activation-aware Weight Quantization 是一种先进的量化技术，它能够在显著降低模型显存占用和加速推理的同时，最大限度地保持模型精度。相比于传统的 FP16/BF16 推理，AWQ 模型通常能以 INT4/INT8 混合精度运行，对硬件要求更低。 32K 上下文长度: 原生支持高达 32,768 个 token 的上下文长度，使其能够处理更复杂的长文本任务。 禁用思考模式: 对于某些应用场景，我们不希望模型输出中间的思考过程 (如 Qwen 系列特有的 \u0026lt;think\u0026gt;...\u0026lt;/think\u0026gt; 标签)。本项目通过自定义 Jinja 聊天模板在服务端强制禁用了此功能。 vLLM：为何选择它？ vLLM 是一个专为 LLM 推理设计的高性能引擎，其核心优势包括：\nPagedAttention: 一种新颖的注意力算法，有效管理 KV 缓存，显著减少内存浪费和碎片，从而支持更长的序列和更大的批处理大小。 连续批处理 (Continuous Batching): 请求无需等待批处理中的所有序列完成，可以动态插入新的请求，大幅提高 GPU 利用率和吞吐量。 张量并行: 自动且高效地将模型权重和计算任务分布到多个 GPU 上，简化了多 GPU 部署的复杂性。 OpenAI 兼容 API: 提供与 OpenAI API 一致的接口，使得现有应用可以无缝迁移。 广泛的模型支持和社区活跃: 支持包括 Qwen 在内的众多主流模型，并且社区活跃，迭代迅速。 关键需求：禁用思考模式 (enable_thinking=False) Qwen 模型在某些情况下会输出包含 \u0026lt;think\u0026gt;...\u0026lt;/think\u0026gt; 标签的中间思考过程。在我们的应用场景中，这并非期望行为。为了确保 API 输出的纯净性，我们采用了自定义 Jinja 聊天模板的方式。该模板在服务端处理用户输入时，不会引导模型进入“思考”流程。相比于在客户端每次请求时传递 enable_thinking=False 参数，服务端模板的方式更为彻底和统一。\n🛠️ 核心部署步骤 准备自定义聊天模板 在项目根目录下创建 qwen3_nonthinking.jinja 文件，内容如下：\n{% for message in messages %} {% if message[\u0026#39;role\u0026#39;] == \u0026#39;system\u0026#39; %} {{\u0026#39;\u0026lt;|im_start|\u0026gt;system \u0026#39; + message[\u0026#39;content\u0026#39;] + \u0026#39;\u0026lt;|im_end|\u0026gt; \u0026#39;}} {% elif message[\u0026#39;role\u0026#39;] == \u0026#39;user\u0026#39; %} {{\u0026#39;\u0026lt;|im_start|\u0026gt;user \u0026#39; + message[\u0026#39;content\u0026#39;] + \u0026#39;\u0026lt;|im_end|\u0026gt; \u0026#39;}} {% elif message[\u0026#39;role\u0026#39;] == \u0026#39;assistant\u0026#39; %} {{\u0026#39;\u0026lt;|im_start|\u0026gt;assistant \u0026#39; + message[\u0026#39;content\u0026#39;] + \u0026#39;\u0026lt;|im_end|\u0026gt; \u0026#39;}} {% endif %} {% endfor %} {% if add_generation_prompt %} {{\u0026#39;\u0026lt;|im_start|\u0026gt;assistant \u0026#39;}} {% endif %} 此模板移除了可能触发思考模式的特殊指令。\nDocker 部署命令 假设模型文件已下载到宿主机的 /home/llm/model/qwen/Qwen3-32B-AWQ 目录，从项目工作区根目录执行以下命令：\ndocker run -d \\ --runtime=nvidia \\ --gpus=all \\ --name coder \\ -v /home/llm/model/qwen/Qwen3-32B-AWQ:/model/Qwen3-32B-AWQ \\ -v $(pwd)/qwen3_nonthinking.jinja:/app/qwen3_nonthinking.jinja \\ -p 8000:8000 \\ --cpuset-cpus 0-55 \\ --ulimit memlock=-1 \\ --ulimit stack=67108864 \\ --restart always \\ --ipc=host \\ vllm/vllm-openai:v0.8.5 \\ --model /model/Qwen3-32B-AWQ \\ --served-model-name coder \\ --tensor-parallel-size 4 \\ --dtype half \\ --quantization awq \\ --max-model-len 32768 \\ --max-num-batched-tokens 4096 \\ --gpu-memory-utilization 0.93 \\ --block-size 32 \\ --enable-chunked-prefill \\ --swap-space 16 \\ --tokenizer-pool-size 56 \\ --disable-custom-all-reduce \\ --chat-template /app/qwen3_nonthinking.jinja ⚙️ 参数详解与优化策略 Docker 容器配置参数 -d: 后台运行容器。 --runtime=nvidia --gpus=all: 使用 NVIDIA runtime 并分配所有 GPU。 --name coder: 为容器命名，方便管理。 -v /home/llm/model/qwen/Qwen3-32B-AWQ:/model/Qwen3-32B-AWQ: 挂载本地模型目录到容器内。 -v $(pwd)/qwen3_nonthinking.jinja:/app/qwen3_nonthinking.jinja: 挂载自定义聊天模板。 -p 8000:8000: 映射端口。 --cpuset-cpus 0-55: 绑定 CPU核心，避免资源争抢。 --ulimit memlock=-1 --ulimit stack=67108864: 解除内存锁定限制，设置较大堆栈空间，对性能和稳定性有益。 --restart always: 容器异常退出时自动重启。 --ipc=host: 使用宿主机 IPC 命名空间，对 NCCL 通信（多GPU协同）至关重要，能显著提高性能。 vLLM 引擎核心参数 --model /model/Qwen3-32B-AWQ: 指定容器内模型的路径。 --served-model-name coder: API 服务时使用的模型名称。 --tensor-parallel-size 4: 设置张量并行数为 4，即使用 4 块 GPU 协同推理。根据模型大小和 GPU 显存调整。 --dtype half: AWQ 模型通常以半精度 (FP16) 加载权重以获得最佳性能和显存平衡。尽管 AWQ 内部可能使用更低精度，但 vLLM 加载时通常指定 half 或 auto。 --quantization awq: 明确告知 vLLM 模型是 AWQ 量化类型。 --max-model-len 32768: 设置模型能处理的最大序列长度，与 Qwen3-32B 的能力匹配。 --max-num-batched-tokens 4096: 单个批次中处理的最大 token 数量。此值影响并发能力和显存占用，需根据实际负载调整。 --gpu-memory-utilization 0.93: 设置 GPU 显存使用率。保留一部分（这里是 7%）是为了应对突发显存需求和避免 OOM。对于 AWQ 模型，由于 KV 缓存依然是 FP16，这部分显存占用不可忽视。 --block-size 32: PagedAttention 中 KV 缓存块的大小。通常 16 或 32 是较优选择。 --enable-chunked-prefill: 对于长序列（如 32K 上下文），启用分块预填充可以有效降低峰值显存，提高长序列处理的稳定性。 --swap-space 16: 分配 16GB 的 CPU RAM 作为 GPU KV 缓存的交换空间。当 GPU 显存不足以容纳所有活跃请求的 KV 缓存时，vLLM 会将部分冷数据交换到 CPU RAM。 --tokenizer-pool-size 56: 设置 Tokenizer 工作池的大小，建议与 CPU 核心数接近，以充分利用 CPU 并行处理能力进行文本编码解码。 --disable-custom-all-reduce: 在某些多于 2 个纯 PCIe 连接的 GPU 配置中，vLLM 的自定义 all-reduce 内核可能存在兼容性或性能问题。禁用它可以回退到 NCCL 默认实现，通常更稳定。 --chat-template /app/qwen3_nonthinking.jinja: 指定使用我们自定义的聊天模板文件。 🧪 部署验证与测试 检查 Docker 日志 部署启动后，首先通过 docker logs -f coder 查看 vLLM 服务启动日志。关键信息包括：\nGPU 检测和显存分配情况。 模型分片加载情况。 PagedAttention KV 缓存块计算和可用数量。 API 服务启动成功，监听 0.0.0.0:8000。 使用 Python 脚本验证 API 为了确保模型正常响应并且自定义聊天模板生效（不输出 \u0026lt;think\u0026gt; 标签），我们编写一个简单的 Python 脚本进行测试。\n环境准备：使用 uv 管理依赖 我们推荐使用 uv 这一新兴的快速 Python 包管理工具来创建虚拟环境和安装依赖。\n创建虚拟环境: 在项目根目录运行 uv venv。这将创建一个名为 .venv 的虚拟环境。 安装 openai 包: 运行 uv pip install openai。 验证脚本与预期输出 在项目根目录创建 verify_llm.py：\nimport openai # 根据实际vLLM服务器IP和端口配置 SERVER_IP = \u0026#34;10.49.121.127\u0026#34; # 或者 localhost SERVER_PORT = 8000 client = openai.OpenAI( base_url=f\u0026#34;http://{SERVER_IP}:{SERVER_PORT}/v1\u0026#34;, api_key=\u0026#34;dummy-key\u0026#34; # vLLM 默认不需要 API key ) messages = [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;你好，请介绍一下你自己。\u0026#34;} ] print(\u0026#34;Sending request to the LLM...\\n\u0026#34;) try: completion = client.chat.completions.create( model=\u0026#34;coder\u0026#34;, # 对应 --served-model-name messages=messages, temperature=0.7, max_tokens=150 ) response_content = completion.choices[0].message.content print(\u0026#34;LLM Response:\u0026#34;) print(response_content) if \u0026#34;\u0026lt;think\u0026gt;\u0026#34; in response_content or \u0026#34;\u0026lt;/think\u0026gt;\u0026#34; in response_content: print(\u0026#34;\\nVERIFICATION FAILED: \u0026#39;\u0026lt;think\u0026gt;\u0026#39; tags found in the response.\u0026#34;) else: print(\u0026#34;\\nVERIFICATION SUCCESSFUL: No \u0026#39;\u0026lt;think\u0026gt;\u0026#39; tags found. \u0026#39;enable_thinking=False\u0026#39; is working as expected.\u0026#34;) except openai.APIConnectionError as e: print(f\u0026#34;Failed to connect to the server: {e}\u0026#34;) print(f\u0026#34;Please ensure the vLLM server is running and accessible at http://{SERVER_IP}:{SERVER_PORT}.\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred: {e}\u0026#34;) 使用 uv run python3 verify_llm.py 运行此脚本。预期输出应包含模型的自我介绍，并且明确提示 VERIFICATION SUCCESSFUL: No '\u0026lt;think\u0026gt;' tags found。\n🔗 项目源码 本项目的所有配置文件、脚本和详细文档均已在 GitHub 开源： https://github.com/jackypanster/deploy-qwen3-32b-awq\n🔚 总结 通过本文的详细步骤和参数解析，我们成功地在多 GPU 环境下使用 vLLM 高效部署了 Qwen3-32B-AWQ 模型。关键的优化点包括针对 AWQ 模型的参数配置、32K 长上下文处理、以及通过自定义聊天模板实现“无思考模式”输出。这套部署方案兼顾了性能、资源利用率和特定业务需求，为基于 Qwen3 大模型的应用开发提供了坚实的基础。\n","permalink":"https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-32b-awq-vllm-guide/","summary":"本文详细介绍了如何使用 vLLM 高效部署 Qwen3-32B-AWQ 量化模型，实现 32K 上下文窗口、OpenAI 兼容 API，并禁用思考模式。通过对 Docker 及 vLLM 参数的精细调优，最大化模型在多 GPU 环境下的推理性能。","title":"Qwen3-32B-AWQ 高效部署：基于 vLLM 的深度实践与优化"},{"content":"一、问题背景：警告频发的SSSD配置残留 在Ubuntu服务器维护过程中，长期被AppArmor服务的SSSD相关警告困扰。具体表现为：\n每次重启AppArmor服务时，日志频繁提示Warning: found usr.sbin.sssd in /etc/apparmor.d/force-complain apparmor_status持续显示/usr/sbin/sssd配置存在，但实际已卸载SSSD服务 系统日志中伴随Caching disabled for: 'usr.sbin.sssd' due to force complain警告，影响服务稳定性评估 二、技术分析：深入AppArmor的配置逻辑 1. AppArmor的配置加载机制 主配置目录：/etc/apparmor.d/ 存放系统级配置 本地覆盖目录：/etc/apparmor.d.local/ 优先级高于主目录，用于本地自定义策略 运行时缓存：AppArmor会将配置加载到内核并缓存，即使删除文件也可能残留运行时状态 2. SSSD服务的特殊性 该服务默认随Ubuntu部分版本安装，提供LDAP/NIS等认证功能 卸载时默认保留配置文件（/etc/apparmor.d/usr.sbin.sssd），导致AppArmor持续尝试加载已删除服务的策略 3. 关键报错溯源# 核心错误日志 apparmor.systemd[12435]: /lib/apparmor/apparmor.systemd: 148: [: Illegal number: yes apparmor.systemd[12546]: Warning: found usr.sbin.sssd in /etc/apparmor.d/force-complain, forcing complain mode- 语法错误源于rc.apparmor.functions中数值比较符误用（-eq未转义字符串）\n警告本质是残留配置与运行时状态的冲突 三、分步解决方案：从手动清理到内核级重置 1. 语法修复：修正AppArmor函数库# 定位关键函数 sudo nano /lib/apparmor/rc.apparmor.functions\n修改check_userns函数中的比较符 原代码： if [ \u0026ldquo;$userns_restricted\u0026rdquo; -eq 1 ]; then 修正后： if [ \u0026ldquo;$userns_restricted\u0026rdquo; = \u0026ldquo;1\u0026rdquo; ]; then 2. 服务卸载与配置清理# 彻底卸载SSSD sudo apt remove \u0026ndash;purge sssd sssd-tools\n删除主目录配置 sudo rm -f /etc/apparmor.d/usr.sbin.sssd\n发现并删除local目录残留 sudo find /etc/apparmor.d/ -name \u0026ldquo;sssd\u0026rdquo;\n输出：/etc/apparmor.d.local/usr.sbin.sssd sudo rm -f /etc/apparmor.d.local/usr.sbin.sssd\n3. 运行时状态重置# 停止AppArmor服务 sudo systemctl stop apparmor\n（注意：Ubuntu内核内置AppArmor，无需modprobe卸载模块）\n强制重新加载配置并清除缓存 sudo apparmor_parser -r /etc/apparmor.d/ sudo systemctl restart apparmor\n4. 终极验证# 检查配置是否彻底移除 sudo apparmor_status | grep sssd\n预期输出：无任何结果\n确认服务状态 sudo systemctl status apparmor\n理想状态：active (exited) 且无警告日志\n四、技术总结与最佳实践 1. AppArmor运维关键点 配置优先级：local/目录配置会覆盖主目录，卸载服务后需特别检查 缓存机制：修改配置后需通过-r参数或重启服务清除内核缓存 内置模块特性：Ubuntu官方内核默认内置AppArmor，避免使用modprobe操作模块 2. 服务卸载规范# 标准卸载流程 停止服务：sudo systemctl stop 卸载软件包：sudo apt remove \u0026ndash;purge 搜索残留配置：sudo find /etc/ -name \u0026ldquo;\u0026rdquo; 清理日志与缓存：sudo rm -rf /var/log/ /var/cache/ 3. 常见问题预判 Q：为何删除文件后警告仍存在？\nA：AppArmor内核缓存未更新，需通过systemctl restart apparmor强制刷新 Q：能否直接禁用AppArmor？\nA：不建议。作为系统安全核心组件，禁用会导致权限控制失效，应优先清理配置而非关闭服务 五、结语 本次排障深入操作系统安全模块的底层逻辑，通过「语法修复→服务卸载→配置清理→状态重置」的完整链路，彻底解决了长期存在的配置残留问题。实践表明，处理系统级安全组件时，需兼顾文件系统清理与内核运行时状态管理，同时重视配置目录的优先级规则。希望本文能为运维工程师在处理类似问题时提供可复用的技术范式，在保证系统安全性的前提下实现高效维护。\n参考资料\nAppArmor官方文档 Ubuntu服务管理最佳实践 Linux内核模块与AppArmor集成机制 ","permalink":"https://jackypanster.github.io/ai-stream/posts/apparmor-troubleshooting/","summary":"\u003ch2 id=\"一问题背景警告频发的sssd配置残留\"\u003e一、问题背景：警告频发的SSSD配置残留\u003c/h2\u003e\n\u003cp\u003e在Ubuntu服务器维护过程中，长期被AppArmor服务的SSSD相关警告困扰。具体表现为：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e每次重启AppArmor服务时，日志频繁提示\u003ccode\u003eWarning: found usr.sbin.sssd in /etc/apparmor.d/force-complain\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eapparmor_status\u003c/code\u003e持续显示\u003ccode\u003e/usr/sbin/sssd\u003c/code\u003e配置存在，但实际已卸载SSSD服务\u003c/li\u003e\n\u003cli\u003e系统日志中伴随\u003ccode\u003eCaching disabled for: 'usr.sbin.sssd' due to force complain\u003c/code\u003e警告，影响服务稳定性评估\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"二技术分析深入apparmor的配置逻辑\"\u003e二、技术分析：深入AppArmor的配置逻辑\u003c/h2\u003e\n\u003ch3 id=\"1-apparmor的配置加载机制\"\u003e1. AppArmor的配置加载机制\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e主配置目录\u003c/strong\u003e：\u003ccode\u003e/etc/apparmor.d/\u003c/code\u003e 存放系统级配置\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e本地覆盖目录\u003c/strong\u003e：\u003ccode\u003e/etc/apparmor.d.local/\u003c/code\u003e 优先级高于主目录，用于本地自定义策略\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e运行时缓存\u003c/strong\u003e：AppArmor会将配置加载到内核并缓存，即使删除文件也可能残留运行时状态\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"2-sssd服务的特殊性\"\u003e2. SSSD服务的特殊性\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e该服务默认随Ubuntu部分版本安装，提供LDAP/NIS等认证功能\u003c/li\u003e\n\u003cli\u003e卸载时默认保留配置文件（\u003ccode\u003e/etc/apparmor.d/usr.sbin.sssd\u003c/code\u003e），导致AppArmor持续尝试加载已删除服务的策略\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"3-关键报错溯源-核心错误日志\"\u003e3. 关键报错溯源# 核心错误日志\u003c/h3\u003e\n\u003cp\u003eapparmor.systemd[12435]: /lib/apparmor/apparmor.systemd: 148: [: Illegal number: yes\napparmor.systemd[12546]: Warning: found usr.sbin.sssd in /etc/apparmor.d/force-complain, forcing complain mode- 语法错误源于\u003ccode\u003erc.apparmor.functions\u003c/code\u003e中数值比较符误用（\u003ccode\u003e-eq\u003c/code\u003e未转义字符串）\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e警告本质是残留配置与运行时状态的冲突\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"三分步解决方案从手动清理到内核级重置\"\u003e三、分步解决方案：从手动清理到内核级重置\u003c/h2\u003e\n\u003ch3 id=\"1-语法修复修正apparmor函数库-定位关键函数\"\u003e1. 语法修复：修正AppArmor函数库# 定位关键函数\u003c/h3\u003e\n\u003cp\u003esudo nano /lib/apparmor/rc.apparmor.functions\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e修改check_userns函数中的比较符\n原代码：\nif [ \u0026ldquo;$userns_restricted\u0026rdquo; -eq 1 ]; then\n修正后：\nif [ \u0026ldquo;$userns_restricted\u0026rdquo; = \u0026ldquo;1\u0026rdquo; ]; then\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"2-服务卸载与配置清理-彻底卸载sssd\"\u003e2. 服务卸载与配置清理# 彻底卸载SSSD\u003c/h3\u003e\n\u003cp\u003esudo apt remove \u0026ndash;purge sssd sssd-tools\u003c/p\u003e","title":"AppArmor配置残留问题排查与彻底解决：从报错到系统净化的完整实践"},{"content":"DeepSeek-R1-0528-Qwen3-8B部署优化实践：性能与稳定性的平衡艺术 在AI大模型部署领域，本文详细记录对DeepSeek-R1-0528-Qwen3-8B模型使用vLLM进行部署优化的全过程，重点关注上下文窗口长度与硬件资源利用的平衡调优。\n环境与基础设施 我们的部署环境具备以下配置：\nGPU: 4 x NVIDIA RTX 2080 Ti（每张22GB显存，总计88GB显存） 架构: Turing 计算能力: 7.5 CPU: 56核 内存: 512GB RAM 存储: 2TB SSD 操作系统: Ubuntu 24.04 容器镜像: vllm/vllm-openai:v0.8.5 NVIDIA驱动: 570.153.02（CUDA 12.8） 优化前的部署脚本分析 我们最初的部署脚本如下：\ndocker run \\ -d \\ --gpus all \\ --name coder \\ --shm-size 16g \\ --ulimit memlock=-1 \\ --restart always \\ --ipc=host \\ -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models \\ -p 8000:8000 \\ -e CUDA_MODULE_LOADING=LAZY \\ vllm/vllm-openai:v0.8.5 \\ --model /models \\ --served-model-name coder \\ --tensor-parallel-size 4 \\ --gpu-memory-utilization 0.93 \\ --dtype float16 \\ --max-model-len 65536 \\ --trust-remote-code \\ --load-format safetensors \\ --disable-custom-all-reduce 通过分析，我们发现几个可以优化的关键点：\n共享内存：16GB可能不足以支持高并发请求 交换空间：未配置SSD交换空间支持 批处理能力：未设置--max-num-batched-tokens参数 CUDA图形优化：未使用--enforce-eager提高稳定性 深入优化策略 1. 内存与计算资源分配 对于RTX 2080 Ti这类Turing架构GPU，我们需要特别注意显存分配与并行策略：\n共享内存扩展：将--shm-size从16g增加到64g，充分利用512GB系统内存 显存利用率：维持--gpu-memory-utilization 0.93的激进但可控设置 张量并行化：保持--tensor-parallel-size 4充分利用所有GPU 批处理支持：添加--max-num-batched-tokens 8192提高吞吐量 2. 稳定性与效率平衡 CUDA执行模式：添加--enforce-eager参数，避免CUDA图捕获可能导致的OOM问题 交换空间支持：添加--swap-space 32参数，为处理长上下文提供额外内存保障 all-reduce优化：移除--disable-custom-all-reduce参数（注：日志显示系统自动禁用） 3. 上下文长度设计 虽然我们最终保留了--max-model-len 65536设置，但在生产环境中应当根据具体使用场景和稳定性需求考虑降至32768。对于大多数应用场景，这个长度已经足够，并且能提供更好的性能和稳定性平衡。\n优化后的部署脚本 经过一系列优化，我们的最终部署脚本如下：\ndocker run \\ -d \\ --gpus all \\ --name coder \\ --shm-size 64g \\ --ulimit memlock=-1 \\ --restart always \\ --ipc=host \\ -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models \\ -p 8000:8000 \\ -e CUDA_MODULE_LOADING=LAZY \\ vllm/vllm-openai:v0.8.5 \\ --model /models \\ --served-model-name coder \\ --tensor-parallel-size 4 \\ --gpu-memory-utilization 0.93 \\ --dtype float16 \\ --max-model-len 65536 \\ --trust-remote-code \\ --load-format safetensors \\ --swap-space 32 \\ --enforce-eager \\ --max-num-batched-tokens 8192 \\ --chat-template /models/qwen3_programming.jinja 性能与资源分析 部署后，通过日志分析我们得到以下性能指标：\nMemory profiling takes 5.76 seconds the current vLLM instance can use total_gpu_memory (21.48GiB) x gpu_memory_utilization (0.93) = 19.98GiB model weights take 3.85GiB; non_torch_memory takes 0.20GiB; PyTorch activation peak memory takes 1.45GiB; the rest of the memory reserved for KV Cache is 14.49GiB. 关键性能发现：\nKV缓存空间：14.49GiB，足够支持65536 token的上下文处理 最大并发能力：可同时处理约6.44个最大长度（65536 tokens）的请求 初始化时间：31.86秒，相比未优化配置有所改善 实用部署建议 根据我们的实践经验，提供以下部署建议：\n上下文长度选择\n对于追求稳定性的生产环境：使用--max-model-len 32768 对于需要极限性能的场景：可尝试--max-model-len 65536但需密切监控稳定性 显存利用率调优\n稳定性优先：--gpu-memory-utilization 0.9 性能优先：--gpu-memory-utilization 0.93或更高（需谨慎） 批处理参数优化\n对于多用户场景：增加--max-num-batched-tokens至8192或更高 对于单一复杂任务：可适当降低此参数，专注单任务性能 硬件资源分配\n共享内存与系统内存比例：建议1:8左右（如512GB系统内存配置64GB共享内存） 交换空间设置：根据SSD速度和容量，可设置为显存总量的1/3至1/2 排障与验证 每次修改配置后，通过以下命令验证部署状态：\ncurl http://localhost:8000/v1/models 验证结果显示模型已成功部署，并返回了以下实际输出：\n{ \u0026#34;object\u0026#34;: \u0026#34;list\u0026#34;, \u0026#34;data\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;coder\u0026#34;, \u0026#34;object\u0026#34;: \u0026#34;model\u0026#34;, \u0026#34;created\u0026#34;: 1749289780, \u0026#34;owned_by\u0026#34;: \u0026#34;vllm\u0026#34;, \u0026#34;root\u0026#34;: \u0026#34;/models\u0026#34;, \u0026#34;parent\u0026#34;: null, \u0026#34;max_model_len\u0026#34;: 65536, \u0026#34;permission\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;modelperm-ee339bc1702c402f8ae06ea2f1b05c7c\u0026#34;, \u0026#34;object\u0026#34;: \u0026#34;model_permission\u0026#34;, \u0026#34;created\u0026#34;: 1749289780, \u0026#34;allow_create_engine\u0026#34;: false, \u0026#34;allow_sampling\u0026#34;: true, \u0026#34;allow_logprobs\u0026#34;: true, \u0026#34;allow_search_indices\u0026#34;: false, \u0026#34;allow_view\u0026#34;: true, \u0026#34;allow_fine_tuning\u0026#34;: false, \u0026#34;organization\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;group\u0026#34;: null, \u0026#34;is_blocking\u0026#34;: false } ] } ] } 从返回的JSON响应中，我们可以确认模型部署成功并解读以下关键信息：\nid: \u0026ldquo;coder\u0026rdquo; - 确认我们的模型服务名称已正确设置 max_model_len: 65536 - 验证了我们设置的上下文窗口长度为65536 tokens owned_by: \u0026ldquo;vllm\u0026rdquo; - 表明模型由vLLM服务管理 permission对象中： allow_sampling: true - 支持采样生成（temperature、top_p等参数） allow_logprobs: true - 支持输出token概率 organization: \u0026ldquo;*\u0026rdquo; - 允许所有组织访问模型 这些参数确认了我们的部署配置已经正确应用，且模型服务已准备好接收推理请求。\n专用编程提示词模板 由于DeepSeek-R1-0528-Qwen3-8B模型特别适合编程任务，我们在部署中加入了专门的提示词模板来优化其编程能力。我们已经通过--chat-template参数指定了模板路径，模板内容如下：\n{# Enhanced template for Qwen3 optimized for programming tasks #} {% if messages[0][\u0026#39;role\u0026#39;] == \u0026#39;system\u0026#39; %} {% set loop_messages = messages[1:] %} {% set system_message = messages[0][\u0026#39;content\u0026#39;] %} {% else %} {% set loop_messages = messages %} {% set system_message = \u0026#34;You are a programming assistant specialized in writing clean, efficient, and well-documented code. Provide direct code solutions without unnecessary explanations unless requested. Focus on best practices, optimal algorithms, and proper error handling. When multiple approaches exist, choose the most efficient one by default. Always include necessary imports and dependencies.\u0026#34; %} {% endif %} {# Always include system message for programming optimization #} \u0026lt;|im_start|\u0026gt;system {{ system_message }}\u0026lt;|im_end|\u0026gt; {% for message in loop_messages %} {% if message[\u0026#39;role\u0026#39;] == \u0026#39;user\u0026#39; %} \u0026lt;|im_start|\u0026gt;user {{ message[\u0026#39;content\u0026#39;] }}\u0026lt;|im_end|\u0026gt; {% elif message[\u0026#39;role\u0026#39;] == \u0026#39;assistant\u0026#39; %} \u0026lt;|im_start|\u0026gt;assistant {{ message[\u0026#39;content\u0026#39;] }}\u0026lt;|im_end|\u0026gt; {% elif message[\u0026#39;role\u0026#39;] == \u0026#39;tool\u0026#39; %} \u0026lt;|im_start|\u0026gt;tool {{ message[\u0026#39;content\u0026#39;] }}\u0026lt;|im_end|\u0026gt; {% else %} \u0026lt;|im_start|\u0026gt;{{ message[\u0026#39;role\u0026#39;] }} {{ message[\u0026#39;content\u0026#39;] }}\u0026lt;|im_end|\u0026gt; {% endif %} {% endfor %} {% if add_generation_prompt %} \u0026lt;|im_start|\u0026gt;assistant {% endif %} 此模板具有以下特性：\n专业编程指令：默认系统提示词专门针对编程任务优化，强调代码质量、效率和文档 直接输出：倾向于直接提供代码解决方案，减少不必要的解释（除非特别要求） 标准化格式：使用\u0026lt;|im_start|\u0026gt;和\u0026lt;|im_end|\u0026gt;标记清晰界定不同角色的消息 灵活性：允许覆盖默认系统提示词，以适应特定编程场景 在实际使用中，可以将该模板与vLLM的API调用结合，例如：\nimport requests url = \u0026#34;http://localhost:8000/v1/chat/completions\u0026#34; headers = {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} payload = { \u0026#34;model\u0026#34;: \u0026#34;coder\u0026#34;, \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;写一个Python函数计算斐波那契数列的第n项，要求使用动态规划优化性能\u0026#34;} ], \u0026#34;temperature\u0026#34;: 0.2, \u0026#34;response_format\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;} } response = requests.post(url, headers=headers, json=payload) print(response.json()) 通过这种方式，我们可以充分发挥模型在编程领域的专长，获得更高质量、更符合工程实践的代码输出。\n结论与未来方向 通过精心调整vLLM参数，我们成功实现了DeepSeek-R1-0528-Qwen3-8B模型的高效部署，在有限的RTX 2080 Ti显卡上实现了最大化的性能和上下文长度。\n未来的优化方向可以探索：\n进一步量化研究：探索int8量化对性能和质量的影响 调度策略优化：通过--scheduler-delay-factor和--preemption-mode参数优化多用户场景 自动扩缩容方案：根据负载动态调整GPU分配 希望这份部署优化实践能为更多工程师提供参考，在大模型部署中找到性能与稳定性的最佳平衡点。\n参考资料 vLLM官方文档 Qwen3系列模型说明 DeepSeek R1模型系列介绍 ","permalink":"https://jackypanster.github.io/ai-stream/posts/deploy-deepseek-r1-qwen3-8b-optimization/","summary":"\u003ch1 id=\"deepseek-r1-0528-qwen3-8b部署优化实践性能与稳定性的平衡艺术\"\u003eDeepSeek-R1-0528-Qwen3-8B部署优化实践：性能与稳定性的平衡艺术\u003c/h1\u003e\n\u003cp\u003e在AI大模型部署领域，本文详细记录对DeepSeek-R1-0528-Qwen3-8B模型使用vLLM进行部署优化的全过程，重点关注上下文窗口长度与硬件资源利用的平衡调优。\u003c/p\u003e\n\u003ch2 id=\"环境与基础设施\"\u003e环境与基础设施\u003c/h2\u003e\n\u003cp\u003e我们的部署环境具备以下配置：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGPU\u003c/strong\u003e: 4 x NVIDIA RTX 2080 Ti（每张22GB显存，总计88GB显存）\n\u003cul\u003e\n\u003cli\u003e架构: Turing\u003c/li\u003e\n\u003cli\u003e计算能力: 7.5\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCPU\u003c/strong\u003e: 56核\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e内存\u003c/strong\u003e: 512GB RAM\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e存储\u003c/strong\u003e: 2TB SSD\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e操作系统\u003c/strong\u003e: Ubuntu 24.04\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e容器镜像\u003c/strong\u003e: \u003ccode\u003evllm/vllm-openai:v0.8.5\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNVIDIA驱动\u003c/strong\u003e: 570.153.02（CUDA 12.8）\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"优化前的部署脚本分析\"\u003e优化前的部署脚本分析\u003c/h2\u003e\n\u003cp\u003e我们最初的部署脚本如下：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edocker run \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  -d \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --gpus all \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --name coder \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --shm-size 16g \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --ulimit memlock\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e-1 \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --restart always \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --ipc\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003ehost \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  -p 8000:8000 \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  -e CUDA_MODULE_LOADING\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eLAZY \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  vllm/vllm-openai:v0.8.5 \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --model /models \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --served-model-name coder \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --tensor-parallel-size \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --gpu-memory-utilization 0.93 \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --dtype float16 \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --max-model-len \u003cspan style=\"color:#ae81ff\"\u003e65536\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --trust-remote-code \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --load-format safetensors \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --disable-custom-all-reduce\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e通过分析，我们发现几个可以优化的关键点：\u003c/p\u003e","title":"DeepSeek-R1-0528-Qwen3-8B部署优化实践"},{"content":"Qwen3-30B 技术优化实践（二）：思考模式控制与性能提升 本文是《从32K到131K：Qwen3-30B大模型上下文扩展实践》的续篇，聚焦于模型性能调优特别是思考模式（reasoning mode）控制的技术细节与实践经验。\n在前文中，我们详细介绍了如何使用YaRN技术将Qwen3-30B的上下文长度从32K扩展到131K。今天，我们将深入探讨另一个关键优化维度：思考模式控制及其对性能的影响。通过一系列实验和调优，我们发现禁用思考模式可以显著提升模型响应速度和内存效率，特别适合编程和直接输出类任务场景。\n🔍 思考模式（Reasoning Mode）解析 什么是思考模式？ 思考模式（Reasoning Mode，也称为Thinking Mode）是Qwen3系列模型的一个特性，让模型能够生成中间思考步骤，这些步骤被包含在\u0026lt;think\u0026gt;...\u0026lt;/think\u0026gt;标签内。理论上，这种\u0026quot;思考过程\u0026quot;有助于模型进行更复杂的推理，但同时也引入了额外的计算和内存开销。\n在默认配置下，Qwen3模型会启用思考模式，产生类似以下的输出：\n\u0026lt;think\u0026gt; 首先，我需要分析用户的问题：如何实现一个简单的文件读写功能。 我应该使用Python的内置文件操作功能。 基本步骤应该是： 1. 打开文件（可以使用with语句自动管理资源） 2. 读取或写入内容 3. 确保文件正确关闭 \u0026lt;/think\u0026gt; 以下是一个简单的Python文件读写示例： ```python # 写入文件 with open(\u0026#39;example.txt\u0026#39;, \u0026#39;w\u0026#39;) as file: file.write(\u0026#39;Hello, World!\u0026#39;) # 读取文件 with open(\u0026#39;example.txt\u0026#39;, \u0026#39;r\u0026#39;) as file: content = file.read() print(content) ### 思考模式实现机制 vLLM部署Qwen3模型时，思考模式通过两种方式实现控制： 1. **服务器级控制**：通过部署参数`--enable-reasoning`和`--reasoning-parser deepseek_r1`启用 2. **API级控制**：通过API调用中的`chat_template_kwargs`参数或`enable_thinking`参数动态控制 我们的发现是，**仅删除服务器级别的参数并不足够完全禁用思考模式**，模型在某些情况下仍会产生思考过程。更彻底的解决方案是使用自定义聊天模板。 ## 💡 禁用思考模式的技术实现 ### 自定义聊天模板方案 经过研究Qwen官方文档和实验，我们发现使用自定义聊天模板是完全禁用思考模式的最可靠方法。我们创建了一个名为`qwen3_nonthinking.jinja`的模板文件： ```jinja {% if messages %} {% set loop_messages = messages %} {% else %} {% set loop_messages = [{\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;\u0026#39;}] %} {% endif %} {% for message in loop_messages %} {% if message[\u0026#39;role\u0026#39;] == \u0026#39;user\u0026#39; %} \u0026lt;|im_start|\u0026gt;user {{ message[\u0026#39;content\u0026#39;] }}\u0026lt;|im_end|\u0026gt; {% elif message[\u0026#39;role\u0026#39;] == \u0026#39;assistant\u0026#39; %} \u0026lt;|im_start|\u0026gt;assistant {{ message[\u0026#39;content\u0026#39;] }}\u0026lt;|im_end|\u0026gt; {% elif message[\u0026#39;role\u0026#39;] == \u0026#39;system\u0026#39; %} \u0026lt;|im_start|\u0026gt;system {{ message[\u0026#39;content\u0026#39;] }}\u0026lt;|im_end|\u0026gt; {% endif %} {% endfor %} \u0026lt;|im_start|\u0026gt;assistant {% if add_generation_prompt is defined and add_generation_prompt %}{{ generation_prompt }}{% endif %} 这个模板的关键点是移除了所有与思考模式相关的标签和处理逻辑，确保模型无法生成\u0026lt;think\u0026gt;...\u0026lt;/think\u0026gt;块，即使API请求中尝试启用思考模式。\n部署脚本修改 为了使用这个模板，我们修改了部署脚本，添加了以下关键参数：\n# 重要：1. 挂载工作目录使模板文件可访问 -v /home/llm/workspace/deploy-qwen:/workspace/deploy-qwen \\ # 重要：2. 使用自定义模板彻底禁用思考模式 --chat-template /workspace/deploy-qwen/qwen3_nonthinking.jinja 同时，我们在脚本中添加了详细注释，便于在不同场景下快速切换模式。\n📊 性能提升测量与分析 实测性能数据 我们通过实际部署测试，观察到禁用思考模式带来的性能提升：\n指标 启用思考模式 禁用思考模式 提升比例 生成速度 ~12-14 tokens/s ~17-19 tokens/s +15-20% GPU KV缓存使用率 ~12-15% ~8-9% -30-40% 内存占用 较高 较低 -20-25% 输出一致性 出现推理过程 直接输出结果 更加简洁 一个典型的性能日志片段显示：\nINFO 06-03 23:06:14 [metrics.py:486] Avg prompt throughput: 2315.5 tokens/s, Avg generation throughput: 12.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.7%, CPU KV cache usage: 0.0%. INFO 06-03 23:06:19 [metrics.py:486] Avg prompt throughput: 506.3 tokens/s, Avg generation throughput: 17.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.7%, CPU KV cache usage: 0.0%. 性能提升原理分析 禁用思考模式带来性能提升的主要原因包括：\n计算负载减少：不再生成中间思考步骤，减少了总体需要生成的token数量\n注意力计算简化：推理过程通常需要模型在更大的上下文窗口中进行注意力计算，禁用后注意力机制更聚焦\n内存使用优化：无需为思考过程分配额外的KV缓存空间，特别是在131K超长上下文模式下，这一优势更为显著\n内部状态跟踪简化：模型不再需要维护和管理额外的思考状态，减少了内部状态转换的复杂度\n🔧 适用场景与参数调优 最适合禁用思考模式的场景 代码生成任务：直接输出代码而非详细解释过程 简洁问答：需要简短直接答案的场景 API集成：作为后端服务集成到其他系统时 高并发服务：需要处理大量请求时 内存受限环境：硬件资源相对有限时 编程任务最佳参数组合 基于我们的测试，禁用思考模式后，编程任务推荐以下参数设置：\n{ \u0026#34;temperature\u0026#34;: 0.2, \u0026#34;top_p\u0026#34;: 0.6, \u0026#34;top_k\u0026#34;: 50, \u0026#34;presence_penalty\u0026#34;: 0.0, \u0026#34;frequency_penalty\u0026#34;: 0.0 } 这组参数提供了高确定性和一致性，使编码输出更可靠。\n🔄 模式切换方法 我们在部署脚本中提供了详细的切换指南：\n保持禁用思考模式（默认配置） 保留--chat-template参数 删除--enable-reasoning和--reasoning-parser参数 启用思考模式 删除--chat-template参数 添加以下参数： --enable-reasoning \\ --reasoning-parser deepseek_r1 应用更改 docker stop coder \u0026amp;\u0026amp; docker rm coder \u0026amp;\u0026amp; ./deploy-32k.sh # 或 ./deploy-131k.sh 🧩 与YaRN扩展的协同优化 禁用思考模式与YaRN上下文扩展技术结合使用时，能带来更全面的性能和能力提升：\n内存效率倍增：在超长上下文场景下，禁用思考模式能显著降低YaRN扩展带来的额外内存压力\n扩展潜力提高：理论上，通过禁用思考模式，YaRN因子可以进一步提高（例如从4.0到4.5或更高），实现更长上下文\n响应速度提升：特别是在处理大型代码库或长文档时，禁用思考模式提供了更快的token生成速度\n🚀 未来优化方向 基于我们的经验，推荐以下优化方向进一步提升性能：\n启发式路由：构建智能路由层，根据输入类型自动选择启用或禁用思考模式\n场景自适应：开发能根据输入动态调整思考模式的混合策略\nPrompt工程优化：研究特定prompt模式，在禁用思考模式的同时保持高质量推理能力\n量化与思考模式协同优化：探索将4位或8位量化与思考模式禁用结合，进一步提升性能\n🏁 结论 通过深入研究和实践，我们证明了对Qwen3-30B模型思考模式的控制是一种效果显著的性能优化技术。禁用思考模式能带来15-20%的速度提升和更高的内存效率，特别适合编程任务和需要直接输出的场景。\n这种技术不需要模型微调或复杂的GPU优化，仅通过模板和配置修改就能实现，是一种低成本、高收益的优化方案。结合YaRN上下文扩展，我们能够构建一个兼具高性能和强大能力的大模型服务。\n作者说明：本文所有测试均基于Qwen3-30B-A3B模型在4×NVIDIA GPU上使用vLLM v0.8.5进行，具体硬件环境为4×GPU(每卡22GB VRAM)，512GB RAM，56核CPU，2TB SSD。实际性能可能因硬件配置、模型版本和工作负载特性而有所不同。\n","permalink":"https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-part2/","summary":"\u003ch1 id=\"qwen3-30b-技术优化实践二思考模式控制与性能提升\"\u003eQwen3-30B 技术优化实践（二）：思考模式控制与性能提升\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e本文是\u003ca href=\"blog-post.md\"\u003e《从32K到131K：Qwen3-30B大模型上下文扩展实践》\u003c/a\u003e的续篇，聚焦于模型性能调优特别是思考模式（reasoning mode）控制的技术细节与实践经验。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e在前文中，我们详细介绍了如何使用YaRN技术将Qwen3-30B的上下文长度从32K扩展到131K。今天，我们将深入探讨另一个关键优化维度：\u003cstrong\u003e思考模式控制\u003c/strong\u003e及其对性能的影响。通过一系列实验和调优，我们发现禁用思考模式可以显著提升模型响应速度和内存效率，特别适合编程和直接输出类任务场景。\u003c/p\u003e\n\u003ch2 id=\"-思考模式reasoning-mode解析\"\u003e🔍 思考模式（Reasoning Mode）解析\u003c/h2\u003e\n\u003ch3 id=\"什么是思考模式\"\u003e什么是思考模式？\u003c/h3\u003e\n\u003cp\u003e思考模式（Reasoning Mode，也称为Thinking Mode）是Qwen3系列模型的一个特性，让模型能够生成中间思考步骤，这些步骤被包含在\u003ccode\u003e\u0026lt;think\u0026gt;...\u0026lt;/think\u0026gt;\u003c/code\u003e标签内。理论上，这种\u0026quot;思考过程\u0026quot;有助于模型进行更复杂的推理，但同时也引入了额外的计算和内存开销。\u003c/p\u003e\n\u003cp\u003e在默认配置下，Qwen3模型会启用思考模式，产生类似以下的输出：\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u0026lt;think\u0026gt;\n首先，我需要分析用户的问题：如何实现一个简单的文件读写功能。\n我应该使用Python的内置文件操作功能。\n基本步骤应该是：\n1. 打开文件（可以使用with语句自动管理资源）\n2. 读取或写入内容\n3. 确保文件正确关闭\n\u0026lt;/think\u0026gt;\n\n以下是一个简单的Python文件读写示例：\n\n```python\n# 写入文件\nwith open(\u0026#39;example.txt\u0026#39;, \u0026#39;w\u0026#39;) as file:\n    file.write(\u0026#39;Hello, World!\u0026#39;)\n\n# 读取文件\nwith open(\u0026#39;example.txt\u0026#39;, \u0026#39;r\u0026#39;) as file:\n    content = file.read()\n    print(content)\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\n### 思考模式实现机制\n\nvLLM部署Qwen3模型时，思考模式通过两种方式实现控制：\n\n1. **服务器级控制**：通过部署参数`--enable-reasoning`和`--reasoning-parser deepseek_r1`启用\n   \n2. **API级控制**：通过API调用中的`chat_template_kwargs`参数或`enable_thinking`参数动态控制\n\n我们的发现是，**仅删除服务器级别的参数并不足够完全禁用思考模式**，模型在某些情况下仍会产生思考过程。更彻底的解决方案是使用自定义聊天模板。\n\n## 💡 禁用思考模式的技术实现\n\n### 自定义聊天模板方案\n\n经过研究Qwen官方文档和实验，我们发现使用自定义聊天模板是完全禁用思考模式的最可靠方法。我们创建了一个名为`qwen3_nonthinking.jinja`的模板文件：\n\n```jinja\n{% if messages %}\n{% set loop_messages = messages %}\n{% else %}\n{% set loop_messages = [{\u0026#39;role\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;\u0026#39;}] %}\n{% endif %}\n\n{% for message in loop_messages %}\n{% if message[\u0026#39;role\u0026#39;] == \u0026#39;user\u0026#39; %}\n\u0026lt;|im_start|\u0026gt;user\n{{ message[\u0026#39;content\u0026#39;] }}\u0026lt;|im_end|\u0026gt;\n{% elif message[\u0026#39;role\u0026#39;] == \u0026#39;assistant\u0026#39; %}\n\u0026lt;|im_start|\u0026gt;assistant\n{{ message[\u0026#39;content\u0026#39;] }}\u0026lt;|im_end|\u0026gt;\n{% elif message[\u0026#39;role\u0026#39;] == \u0026#39;system\u0026#39; %}\n\u0026lt;|im_start|\u0026gt;system\n{{ message[\u0026#39;content\u0026#39;] }}\u0026lt;|im_end|\u0026gt;\n{% endif %}\n{% endfor %}\n\u0026lt;|im_start|\u0026gt;assistant\n{% if add_generation_prompt is defined and add_generation_prompt %}{{ generation_prompt }}{% endif %}\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e这个模板的关键点是\u003cstrong\u003e移除了所有与思考模式相关的标签和处理逻辑\u003c/strong\u003e，确保模型无法生成\u003ccode\u003e\u0026lt;think\u0026gt;...\u0026lt;/think\u0026gt;\u003c/code\u003e块，即使API请求中尝试启用思考模式。\u003c/p\u003e","title":"Qwen3-30B 技术优化实践（二）：思考模式控制与15-20%性能提升"},{"content":"高性能部署Qwen3-30B：vLLM优化实践指南 📋 概述 本文详细介绍如何使用vLLM高效部署Qwen3-30B-A3B模型，实现32K上下文窗口和OpenAI兼容API，适用于生产环境。通过精细调整部署参数，我们能够在有限的GPU资源下最大化模型性能。\n🖥️ 系统要求 硬件配置 4块NVIDIA GPU (每块22GB显存，总计88GB) 512GB系统内存 2TB SSD存储 56核CPU 软件环境 Ubuntu 24.04 NVIDIA驱动 550.144.03 CUDA 12.4 Docker + NVIDIA Container Toolkit 🧠 模型与架构 Qwen3-30B-A3B是阿里云发布的通用大语言模型，具有以下特点：\n30B参数量 原生支持32K上下文长度 支持思考模式(Chain-of-Thought) 优异的多语言与代码能力 我们使用vLLM作为推理引擎，主要基于以下考量：\n高效内存管理：通过PagedAttention技术优化KV缓存 张量并行：自动跨多GPU分布模型权重 OpenAI兼容API：直接替代OpenAI API，无需修改现有应用 动态批处理：自动批处理多请求，提高吞吐量 🐳 部署脚本 以下是我们用于部署的Docker命令，经过精心调优以平衡性能与资源利用：\ndocker run -d \\ --runtime=nvidia \\ --gpus=all \\ --name coder \\ -v /home/llm/model/qwen/qwen3-30b-a3b:/qwen/qwen3-30b-a3b \\ -p 8000:8000 \\ --cpuset-cpus 0-55 \\ --ulimit memlock=-1 \\ --ulimit stack=67108864 \\ --restart always \\ --ipc=host \\ vllm/vllm-openai:v0.8.5 \\ --model /qwen/qwen3-30b-a3b \\ --served-model-name coder \\ --tensor-parallel-size 4 \\ --dtype half \\ --max-model-len 32768 \\ --max-num-batched-tokens 4096 \\ --gpu-memory-utilization 0.93 \\ --block-size 32 \\ --enable-chunked-prefill \\ --swap-space 16 \\ --tokenizer-pool-size 56 \\ --disable-custom-all-reduce 🔧 参数详解与优化策略 Docker容器配置 参数 值 作用 --runtime=nvidia 启用NVIDIA容器运行时 --gpus=all 将所有GPU暴露给容器 --cpuset-cpus 0-55 限制容器使用0-55号CPU核心 --ulimit memlock=-1 移除内存锁定限制，提高性能 --ipc=host 使用主机IPC命名空间，对共享内存很重要 vLLM引擎配置 1. 张量并行策略 --tensor-parallel-size 4 我们使用4路张量并行，将模型分布在4块GPU上。这是基于实验得出的最佳配置 - 在我们的硬件上，每块22GB显存的GPU无法单独加载完整的30B模型。\n2. 内存优化 --dtype half --gpu-memory-utilization 0.93 --block-size 32 --swap-space 16 half精度(FP16)相比bfloat16能进一步节省内存，且在我们的场景中精度损失可接受 GPU内存利用率93%留出一定缓冲空间防止OOM错误 KV缓存块大小设为32，平衡内存使用与计算效率 16GB的CPU-GPU交换空间支持处理超长序列 3. 上下文长度与批处理 --max-model-len 32768 --max-num-batched-tokens 4096 --enable-chunked-prefill 我们将上下文长度从默认的16K增加到32K，以支持更长输入和输出。为了平衡资源使用，相应地将批处理令牌数从8192减少到4096，这是一个经过测试的合理折中方案。\n启用分块预填充(chunked-prefill)对于处理长上下文尤为重要，它将长序列分解为更小的块进行处理，减少显存峰值使用。\n4. 其他性能调优 --tokenizer-pool-size 56 --disable-custom-all-reduce 令牌化工作池大小与CPU核心数匹配，优化并行处理能力 禁用自定义all-reduce操作，解决某些硬件配置上的兼容性问题 📊 性能分析 部署后，我们可以通过docker logs -f coder查看服务状态，关键性能指标如下：\nINFO 06-03 02:01:19 [worker.py:287] the current vLLM instance can use total_gpu_memory (21.66GiB) x gpu_memory_utilization (0.93) = 20.15GiB INFO 06-03 02:01:19 [worker.py:287] model weights take 14.25GiB; non_torch_memory takes 0.20GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 4.30GiB. INFO 06-03 02:01:20 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 5.73x 这表明：\n每个GPU使用约20.15GB内存 模型权重占用14.25GB 对于32K令牌请求，系统可以并发处理5.73倍的请求 在我们的生产环境中，这个配置能够处理每分钟约15-20个并发对话，满足中小型应用需求。\n📝 API使用示例 服务启动后，可以通过OpenAI兼容的API在本地端口8000访问：\ncurl http://localhost:8000/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;coder\u0026#34;, \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;请解释一下量子计算的基本原理\u0026#34;} ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: 2000 }\u0026#39; 使用Python客户端：\nfrom openai import OpenAI client = OpenAI( base_url=\u0026#34;http://localhost:8000/v1\u0026#34;, api_key=\u0026#34;not-needed\u0026#34; # vLLM不要求API密钥 ) response = client.chat.completions.create( model=\u0026#34;coder\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;写一个Python函数计算斐波那契数列\u0026#34;} ], temperature=0.7, max_tokens=1000 ) print(response.choices[0].message.content) 🚀 扩展到更长上下文 Qwen3-30B原生支持32K上下文，但如需扩展到更长上下文(如131K令牌)，可以使用YaRN技术，通过在vLLM参数中添加：\n--rope-scaling \u0026#39;{\u0026#34;rope_type\u0026#34;:\u0026#34;yarn\u0026#34;,\u0026#34;factor\u0026#34;:4.0,\u0026#34;original_max_position_embeddings\u0026#34;:32768}\u0026#39; \\ --max-model-len 131072 注意这会增加内存使用，可能需要进一步调整其他参数以平衡资源。\n🔍 常见问题排查 OOM错误：减小gpu-memory-utilization或max-num-batched-tokens 推理速度慢：检查GPU利用率，考虑增加batch大小或减小max-model-len CUDA图捕获失败：添加--enforce-eager参数禁用CUDA图优化 📈 未来优化方向 探索使用FlashAttention-2加速注意力计算 尝试AWQ/GPTQ量化技术降低内存使用 配置LLM Router实现多模型负载均衡 🔚 总结 通过精细调优vLLM部署参数，我们成功在有限硬件资源下部署了Qwen3-30B模型，实现了32K上下文窗口的高性能推理服务。这套配置在生产环境中表现稳定，为各类应用提供强大的AI能力支持。\n","permalink":"https://jackypanster.github.io/ai-stream/posts/deploy-qwen3/","summary":"\u003ch1 id=\"高性能部署qwen3-30bvllm优化实践指南\"\u003e高性能部署Qwen3-30B：vLLM优化实践指南\u003c/h1\u003e\n\u003ch2 id=\"-概述\"\u003e📋 概述\u003c/h2\u003e\n\u003cp\u003e本文详细介绍如何使用vLLM高效部署Qwen3-30B-A3B模型，实现32K上下文窗口和OpenAI兼容API，适用于生产环境。通过精细调整部署参数，我们能够在有限的GPU资源下最大化模型性能。\u003c/p\u003e\n\u003ch2 id=\"-系统要求\"\u003e🖥️ 系统要求\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e硬件配置\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e4块NVIDIA GPU (每块22GB显存，总计88GB)\u003c/li\u003e\n\u003cli\u003e512GB系统内存\u003c/li\u003e\n\u003cli\u003e2TB SSD存储\u003c/li\u003e\n\u003cli\u003e56核CPU\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e软件环境\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUbuntu 24.04\u003c/li\u003e\n\u003cli\u003eNVIDIA驱动 550.144.03\u003c/li\u003e\n\u003cli\u003eCUDA 12.4\u003c/li\u003e\n\u003cli\u003eDocker + NVIDIA Container Toolkit\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"-模型与架构\"\u003e🧠 模型与架构\u003c/h2\u003e\n\u003cp\u003eQwen3-30B-A3B是阿里云发布的通用大语言模型，具有以下特点：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e30B参数量\u003c/li\u003e\n\u003cli\u003e原生支持32K上下文长度\u003c/li\u003e\n\u003cli\u003e支持思考模式(Chain-of-Thought)\u003c/li\u003e\n\u003cli\u003e优异的多语言与代码能力\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e我们使用vLLM作为推理引擎，主要基于以下考量：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e高效内存管理\u003c/strong\u003e：通过PagedAttention技术优化KV缓存\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e张量并行\u003c/strong\u003e：自动跨多GPU分布模型权重\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOpenAI兼容API\u003c/strong\u003e：直接替代OpenAI API，无需修改现有应用\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e动态批处理\u003c/strong\u003e：自动批处理多请求，提高吞吐量\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"-部署脚本\"\u003e🐳 部署脚本\u003c/h2\u003e\n\u003cp\u003e以下是我们用于部署的Docker命令，经过精心调优以平衡性能与资源利用：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edocker run -d \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --runtime\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003envidia \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --gpus\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eall \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --name coder \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  -v /home/llm/model/qwen/qwen3-30b-a3b:/qwen/qwen3-30b-a3b \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  -p 8000:8000 \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --cpuset-cpus 0-55 \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --ulimit memlock\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e-1 \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --ulimit stack\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e67108864\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --restart always \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --ipc\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003ehost \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  vllm/vllm-openai:v0.8.5 \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --model /qwen/qwen3-30b-a3b \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --served-model-name coder \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --tensor-parallel-size \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --dtype half \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --max-model-len \u003cspan style=\"color:#ae81ff\"\u003e32768\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --max-num-batched-tokens \u003cspan style=\"color:#ae81ff\"\u003e4096\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --gpu-memory-utilization 0.93 \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --block-size \u003cspan style=\"color:#ae81ff\"\u003e32\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --enable-chunked-prefill \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --swap-space \u003cspan style=\"color:#ae81ff\"\u003e16\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --tokenizer-pool-size \u003cspan style=\"color:#ae81ff\"\u003e56\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e  --disable-custom-all-reduce\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"-参数详解与优化策略\"\u003e🔧 参数详解与优化策略\u003c/h2\u003e\n\u003ch3 id=\"docker容器配置\"\u003eDocker容器配置\u003c/h3\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e参数\u003c/th\u003e\n          \u003cth\u003e值\u003c/th\u003e\n          \u003cth\u003e作用\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003e--runtime=nvidia\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n          \u003ctd\u003e启用NVIDIA容器运行时\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003e--gpus=all\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n          \u003ctd\u003e将所有GPU暴露给容器\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003e--cpuset-cpus\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003e0-55\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e限制容器使用0-55号CPU核心\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003e--ulimit memlock=-1\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n          \u003ctd\u003e移除内存锁定限制，提高性能\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003e--ipc=host\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n          \u003ctd\u003e使用主机IPC命名空间，对共享内存很重要\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"vllm引擎配置\"\u003evLLM引擎配置\u003c/h3\u003e\n\u003ch4 id=\"1-张量并行策略\"\u003e1. 张量并行策略\u003c/h4\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e--tensor-parallel-size 4\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e我们使用4路张量并行，将模型分布在4块GPU上。这是基于实验得出的最佳配置 - 在我们的硬件上，每块22GB显存的GPU无法单独加载完整的30B模型。\u003c/p\u003e","title":"高性能部署Qwen3-30B：vLLM优化实践指南"},{"content":"AI辅助编程实战：从需求到实现的完整对话记录 引言 本文真实还原了我与AI助手协作实现Docker镜像大小监控功能的全过程。通过这次实践，可以看到AI不仅能辅助开发者梳理需求、设计方案，还能在代码实现、文档编写等环节提供高效帮助。整个流程自然流畅，极大提升了开发体验。\n1. 需求分析与方案设计 初始需求描述 用户： 我们需要在Jenkins共享库中增加Docker镜像大小监控功能，当镜像超过4GB时发出警告。请帮忙分析一下实现思路。 AI的初步分析与建议 AI： 让我先看看项目的结构…… 这是一个Jenkins共享库项目，核心代码都在`vars/`目录。针对镜像大小监控，我建议直接在`buildDocker`方法里加逻辑： 1. 镜像构建完成后用`du`命令获取大小 2. 与4GB阈值比较 3. 超过阈值就输出警告 4. 相关指标写日志，方便后续分析 这个方案您觉得合适吗？有无补充？ 需求确认与细化 用户： 这个思路可以，不过还需要： 1. 阈值要可配置，默认4GB 2. 日志里要有镜像名、大小、时间戳等详细信息 3. 输出优化建议 另外，异常处理也要考虑得更优雅些。 2. 代码实现 初始代码实现 AI： 明白，我来实现。会在`buildDocker`方法里： 1. 用`getEnvOrDefault`支持阈值配置 2. 详细记录指标 3. 加强错误处理 4. 输出优化建议 实现逻辑您看还有什么要调整的吗？ 代码审查与优化 用户： 整体不错，不过警告要改成抛异常直接中断构建。另外，能不能顺便加上基础镜像大小检测？ 异常处理优化 AI： 明白您的需求，代码我会这样优化： 1. 超标直接抛异常中断流水线 2. 增加基础镜像大小检测 3. 明确区分业务异常（比如镜像过大）和技术异常（比如命令失败） 基础镜像检测这块，我建议： - 解析Dockerfile的FROM - 拉取并检测基础镜像大小 - 超限就给出alpine等轻量镜像建议 这样处理您觉得OK吗？ 3. 测试与验证 测试方案讨论 用户： 怎么验证这个功能？ AI： 建议这样测试： 1. 镜像大小正常 2. 恰好等于阈值 3. 超过阈值 4. Dockerfile解析异常 也可以临时把阈值调小，快速验证告警逻辑。 4. 文档与沟通 变更日志生成 AI： 我已为您生成详细CHANGELOG，包括： - 功能说明 - 技术实现细节 - 配置参数说明 - 后续优化计划 如需调整，随时告诉我。 团队通知 用户： 要发个通知给团队，说明我们要限制镜像大小。 AI： 建议分阶段推进： 1. 先监控和通知 2. 再进入警告期 3. 最后强制限制 这样大家有缓冲期，可以逐步优化Dockerfile。需要帮忙起草通知吗？ 5. 经验总结 技术收获 AI辅助效率\n代码实现速度提升显著 自动生成文档节省时间 代码质量\n异常处理更完善 配置更灵活 团队协作\n文档更规范 知识传递更高效 建议与展望 建立AI辅助编码规范 完善提示工程实践 探索更多自动化场景 结语 通过这次实践，AI能陪伴开发者一起思考、决策和落地实现。期待未来AI在更多软件开发场景中发挥更大作用。\n","permalink":"https://jackypanster.github.io/ai-stream/posts/coding-with-ai/","summary":"\u003ch1 id=\"ai辅助编程实战从需求到实现的完整对话记录\"\u003eAI辅助编程实战：从需求到实现的完整对话记录\u003c/h1\u003e\n\u003ch2 id=\"引言\"\u003e引言\u003c/h2\u003e\n\u003cp\u003e本文真实还原了我与AI助手协作实现Docker镜像大小监控功能的全过程。通过这次实践，可以看到AI不仅能辅助开发者梳理需求、设计方案，还能在代码实现、文档编写等环节提供高效帮助。整个流程自然流畅，极大提升了开发体验。\u003c/p\u003e\n\u003ch2 id=\"1-需求分析与方案设计\"\u003e1. 需求分析与方案设计\u003c/h2\u003e\n\u003ch3 id=\"初始需求描述\"\u003e初始需求描述\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e用户：\n我们需要在Jenkins共享库中增加Docker镜像大小监控功能，当镜像超过4GB时发出警告。请帮忙分析一下实现思路。\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"ai的初步分析与建议\"\u003eAI的初步分析与建议\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eAI：\n让我先看看项目的结构……\n\n这是一个Jenkins共享库项目，核心代码都在`vars/`目录。针对镜像大小监控，我建议直接在`buildDocker`方法里加逻辑：\n\n1. 镜像构建完成后用`du`命令获取大小\n2. 与4GB阈值比较\n3. 超过阈值就输出警告\n4. 相关指标写日志，方便后续分析\n\n这个方案您觉得合适吗？有无补充？\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"需求确认与细化\"\u003e需求确认与细化\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e用户：\n这个思路可以，不过还需要：\n1. 阈值要可配置，默认4GB\n2. 日志里要有镜像名、大小、时间戳等详细信息\n3. 输出优化建议\n\n另外，异常处理也要考虑得更优雅些。\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"2-代码实现\"\u003e2. 代码实现\u003c/h2\u003e\n\u003ch3 id=\"初始代码实现\"\u003e初始代码实现\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eAI：\n明白，我来实现。会在`buildDocker`方法里：\n\n1. 用`getEnvOrDefault`支持阈值配置\n2. 详细记录指标\n3. 加强错误处理\n4. 输出优化建议\n\n实现逻辑您看还有什么要调整的吗？\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"代码审查与优化\"\u003e代码审查与优化\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e用户：\n整体不错，不过警告要改成抛异常直接中断构建。另外，能不能顺便加上基础镜像大小检测？\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"异常处理优化\"\u003e异常处理优化\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eAI：\n明白您的需求，代码我会这样优化：\n\n1. 超标直接抛异常中断流水线\n2. 增加基础镜像大小检测\n3. 明确区分业务异常（比如镜像过大）和技术异常（比如命令失败）\n\n基础镜像检测这块，我建议：\n- 解析Dockerfile的FROM\n- 拉取并检测基础镜像大小\n- 超限就给出alpine等轻量镜像建议\n\n这样处理您觉得OK吗？\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"3-测试与验证\"\u003e3. 测试与验证\u003c/h2\u003e\n\u003ch3 id=\"测试方案讨论\"\u003e测试方案讨论\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e用户：\n怎么验证这个功能？\n\nAI：\n建议这样测试：\n1. 镜像大小正常\n2. 恰好等于阈值\n3. 超过阈值\n4. Dockerfile解析异常\n\n也可以临时把阈值调小，快速验证告警逻辑。\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"4-文档与沟通\"\u003e4. 文档与沟通\u003c/h2\u003e\n\u003ch3 id=\"变更日志生成\"\u003e变更日志生成\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eAI：\n我已为您生成详细CHANGELOG，包括：\n- 功能说明\n- 技术实现细节\n- 配置参数说明\n- 后续优化计划\n\n如需调整，随时告诉我。\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"团队通知\"\u003e团队通知\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e用户：\n要发个通知给团队，说明我们要限制镜像大小。\n\nAI：\n建议分阶段推进：\n1. 先监控和通知\n2. 再进入警告期\n3. 最后强制限制\n\n这样大家有缓冲期，可以逐步优化Dockerfile。需要帮忙起草通知吗？\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"5-经验总结\"\u003e5. 经验总结\u003c/h2\u003e\n\u003ch3 id=\"技术收获\"\u003e技术收获\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAI辅助效率\u003c/strong\u003e\u003c/p\u003e","title":"AI辅助编程实战：从需求到实现的完整对话记录"},{"content":"基于FunAudioLLM/SenseVoiceSmall搭建高效语音转录服务的实践之路 项目概述 实现一个语音转录文本（ASR）的服务，目标是能够高效地将用户上传的音频文件转换为文字。出于中文语音的考虑，选择了来自 FunAudioLLM 的 SenseVoiceSmall 模型，它以其多语种支持、高效率以及集成的语音理解能力（如情感识别、事件检测）吸引了我。本文将详细记录从环境配置、核心功能实现到踩坑解决的全过程，并分享一些关于模型选型的思考。\n完整代码已开源在 GitHub 仓库：https://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall\n项目需求文档（prd.md）关键信息如下：\n模型: FunAudioLLM/SenseVoice (具体为 SenseVoiceSmall) 本地模型路径: /home/llm/model/iic/SenseVoiceSmall (从 ModelScope 下载) API框架: FastAPI Python环境管理: uv 环境配置 为了保持开发环境的纯净和高效，采用了 uv 来管理 Python 依赖。\n创建虚拟环境 (如果尚未创建):\nuv venv .venv source .venv/bin/activate 安装核心依赖: 初始的 requirements.txt 包含了 fastapi, uvicorn, python-multipart 等基础库。后续根据模型加载和处理的需求，逐步添加了 torch, torchaudio, numpy, transformers, sentencepiece, 以及最终解决模型加载问题的核心库 funasr。\nuv pip install -r requirements.txt 核心功能实现概览 项目结构 项目的主要结构包括：\napp/main.py: FastAPI 应用入口，定义 API 路由和应用生命周期事件（如模型加载）。 app/models/sensevoice_loader.py: 负责加载 SenseVoiceSmall 模型，采用单例模式。 app/services/asr_service.py: 封装语音处理和模型推理的核心逻辑。 app/schemas.py: 定义 API 的请求和响应数据模型 (Pydantic models)。 API 端点 关键的 API 端点设计为：\nPOST /asr_pure Content-Type: multipart/form-data Body: file (音频文件) 返回转录后的文本及处理时间。\n踩坑与解决之路：模型加载的曲折历程 在项目推进过程中，模型加载部分是遇到问题最多的地方，也是收获最多的地方。\n坑1：Hugging Face AutoClass 的 \u0026ldquo;Unrecognized model\u0026rdquo; 最初，尝试使用 Hugging Face transformers 库通用的 AutoProcessor.from_pretrained() 和 AutoModelForSpeechSeq2Seq.from_pretrained() 来加载本地的 SenseVoiceSmall 模型文件。\n# app/models/sensevoice_loader.py (早期尝试) # from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor # ... # self.processor = AutoProcessor.from_pretrained(MODEL_PATH) # self.model = AutoModelForSpeechSeq2Seq.from_pretrained(MODEL_PATH) 然而，服务启动时立即报错：\nValueError: Unrecognized model in /home/llm/model/iic/SenseVoiceSmall. Should have a model_type key in its config.json... 这个错误表明 transformers 的自动发现机制无法识别模型类型，通常是因为模型目录下的 config.json 文件缺少 model_type 字段，或者该模型需要特定的加载类。\n坑2：转向 funasr 与 trust_remote_code 的初步探索 查阅 FunAudioLLM/SenseVoice 的官方文档后发现，推荐使用 funasr 库的 AutoModel 来加载 SenseVoice 系列模型。于是调整了代码：\n添加 funasr 到 requirements.txt。 修改 SenseVoiceLoader: # app/models/sensevoice_loader.py (引入 funasr) from funasr import AutoModel # ... self.model = AutoModel( model=FUNASR_MODEL_NAME_OR_PATH, # 即本地路径 trust_remote_code=True, device=self.device ) 同时，asr_service.py 中的推理逻辑也相应调整为调用 funasr 模型对象的 .generate() 方法。 本以为这样能解决问题，但启动时又遇到了新的日志：\nLoading remote code failed: model, No module named \u0026#39;model\u0026#39; 尽管这条日志出现，但后续的 API 调用测试居然成功了！这让我非常困惑。\n坑3：remote_code 参数与 model.py 文件的“幻影” 深入研究 funasr 和 SenseVoice 的文档，注意到对于包含自定义代码（如 model.py）的模型，除了 trust_remote_code=True，有时还需要明确指定 remote_code 参数。\n我检查了 Hugging Face 仓库 FunAudioLLM/SenseVoiceSmall (https://huggingface.co/FunAudioLLM/SenseVoiceSmall/tree/main)，发现其文件列表中确实包含一个 model.py。因此，我尝试在 AutoModel 调用中加入 remote_code=\u0026quot;model.py\u0026quot;。\n# app/models/sensevoice_loader.py (尝试指定 remote_code) self.model = AutoModel( model=FUNASR_MODEL_NAME_OR_PATH, trust_remote_code=True, remote_code=\u0026#34;model.py\u0026#34;, # \u0026lt;--- 新增 device=self.device ) 结果，No module named 'model' 的错误依旧。\n解决方案：澄清 ModelScope 与 Hugging Face 的模型文件差异 本地模型 /home/llm/model/iic/SenseVoiceSmall 是从 ModelScope (https://www.modelscope.cn/models/iic/SenseVoiceSmall/files) 下载的，而非直接 clone Hugging Face 的仓库。通过 ls -al /home/llm/model/iic/SenseVoiceSmall/ 查看本地文件，发现确实没有 model.py 文件！\n这解释了为什么指定 remote_code=\u0026quot;model.py\u0026quot; 依然报错。ModelScope 提供的模型包可能与 Hugging Face 仓库中的文件结构不完全一致，特别是对于这种依赖 funasr 特定加载方式的模型。\n最终的正确配置：移除 remote_code 参数，但保留 trust_remote_code=True。\n# app/models/sensevoice_loader.py (最终正确配置) self.model = AutoModel( model=FUNASR_MODEL_NAME_OR_PATH, trust_remote_code=True, # 保留，funasr 可能仍需此权限处理 ModelScope 模型 # remote_code=\u0026#34;model.py\u0026#34;, # 移除，因为本地 ModelScope 版本无此文件 device=self.device ) 这样修改后，服务启动时仍然会打印 Loading remote code failed: model, No module named 'model'，但 API 调用完全正常！\n原因分析：funasr 在 trust_remote_code=True 时，会优先尝试加载自定义代码。如果本地模型路径（如从 ModelScope 下载的）没有 model.py，这个尝试会失败并打印日志。但随后，funasr 能够识别出这是一个有效的 ModelScope 模型路径，并转用其内部的标准加载流程成功加载模型。因此，该日志在这种情况下是良性的。\n模型对比与选型思考 在解决问题的过程中，也探讨了 FunAudioLLM/SenseVoiceSmall 与其他主流 ASR 模型的对比：\nOpenAI Whisper 系列 (如 whisper-large-v3):\n优势: 极高的准确率，强大的多语言能力，庞大的社区。 劣势: 推理速度相对较慢（尤其大模型），不直接提供情感/事件检测。 Wav2Vec2 系列:\n优势: 自监督学习典范，大量特定语言微调模型。 劣势: 基础模型功能相对单一。 SenseVoiceSmall 的核心优势 高效推理：其模型卡声称采用非自回归端到端框架，比 Whisper-Large 快15倍。这对于需要低延迟的应用至关重要。\n多任务集成：内置 ASR、LID（语种识别）、SER（情感识别）、AED（事件检测）。如果应用场景需要这些附加信息，SenseVoiceSmall 提供了一站式解决方案。\n特定语言优化：在中文、粤语等语言上表现突出。\n结论 没有绝对的“最好”，只有“最适合”。\n若追求极致准确性和最广语言覆盖，且对延迟不敏感，Whisper 仍是首选。 若对推理效率、集成的多任务语音理解（特别是情感/事件）或中文等特定场景有高要求，SenseVoiceSmall 是一个极具竞争力的选择。 目前选择的 SenseVoiceSmall，尤其是在确认了其 ModelScope 版本能够顺畅运行后，对于我的项目目标来说是一个合适的起点。\n当前状态与展望 目前，基于 FunAudioLLM/SenseVoiceSmall 和 FastAPI 的语音转录服务已成功搭建并能正确处理请求。\n$ curl -X POST \u0026#34;http://\u0026lt;your_server_ip\u0026gt;:8888/asr_pure\u0026#34; -F \u0026#34;file=@test_audio.wav\u0026#34; {\u0026#34;text\u0026#34;:\u0026#34;太好了，那接下来咱们可以试试其他功能了。比如说你想测试一下语音合成的效果怎么样，或者是看看有没有什么新的语音处理功能出来啦。😔\u0026#34;,\u0026#34;status\u0026#34;:\u0026#34;success\u0026#34;,\u0026#34;processing_time_ms\u0026#34;:503.39...} 后续可优化的方向 性能优化：进一步测试并发处理能力，考虑多 worker 配置。 错误处理与日志：完善更细致的错误捕获和日志记录。 功能扩展：如果需要，可以利用 SenseVoiceSmall 的情感识别和事件检测能力。 VAD 集成：对于长音频，考虑在 funasr.AutoModel 加载时集成 VAD (Voice Activity Detection) 功能，以实现自动分段处理，提升长音频处理的稳定性和效率。 异步处理与队列：对于高并发场景，引入消息队列和异步任务处理。 ","permalink":"https://jackypanster.github.io/ai-stream/posts/howto-use-sensevoicesmall/","summary":"\u003ch1 id=\"基于funaudiollmsensevoicesmall搭建高效语音转录服务的实践之路\"\u003e基于FunAudioLLM/SenseVoiceSmall搭建高效语音转录服务的实践之路\u003c/h1\u003e\n\u003ch2 id=\"项目概述\"\u003e项目概述\u003c/h2\u003e\n\u003cp\u003e实现一个语音转录文本（ASR）的服务，目标是能够高效地将用户上传的音频文件转换为文字。出于中文语音的考虑，选择了来自 \u003ccode\u003eFunAudioLLM\u003c/code\u003e 的 \u003ccode\u003eSenseVoiceSmall\u003c/code\u003e 模型，它以其多语种支持、高效率以及集成的语音理解能力（如情感识别、事件检测）吸引了我。本文将详细记录从环境配置、核心功能实现到踩坑解决的全过程，并分享一些关于模型选型的思考。\u003c/p\u003e\n\u003cp\u003e完整代码已开源在 GitHub 仓库：\u003ca href=\"https://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall\"\u003ehttps://github.com/jackypanster/FunAudioLLM-SenseVoiceSmall\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e项目需求文档（\u003ccode\u003eprd.md\u003c/code\u003e）关键信息如下：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e模型\u003c/strong\u003e: FunAudioLLM/SenseVoice (具体为 \u003ccode\u003eSenseVoiceSmall\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e本地模型路径\u003c/strong\u003e: \u003ccode\u003e/home/llm/model/iic/SenseVoiceSmall\u003c/code\u003e (从 ModelScope 下载)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAPI框架\u003c/strong\u003e: FastAPI\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePython环境管理\u003c/strong\u003e: \u003ccode\u003euv\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"环境配置\"\u003e环境配置\u003c/h2\u003e\n\u003cp\u003e为了保持开发环境的纯净和高效，采用了 \u003ccode\u003euv\u003c/code\u003e 来管理 Python 依赖。\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e创建虚拟环境\u003c/strong\u003e (如果尚未创建):\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv venv .venv\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esource .venv/bin/activate\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e安装核心依赖\u003c/strong\u003e:\n初始的 \u003ccode\u003erequirements.txt\u003c/code\u003e 包含了 \u003ccode\u003efastapi\u003c/code\u003e, \u003ccode\u003euvicorn\u003c/code\u003e, \u003ccode\u003epython-multipart\u003c/code\u003e 等基础库。后续根据模型加载和处理的需求，逐步添加了 \u003ccode\u003etorch\u003c/code\u003e, \u003ccode\u003etorchaudio\u003c/code\u003e, \u003ccode\u003enumpy\u003c/code\u003e, \u003ccode\u003etransformers\u003c/code\u003e, \u003ccode\u003esentencepiece\u003c/code\u003e, 以及最终解决模型加载问题的核心库 \u003ccode\u003efunasr\u003c/code\u003e。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv pip install -r requirements.txt\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"核心功能实现概览\"\u003e核心功能实现概览\u003c/h2\u003e\n\u003ch3 id=\"项目结构\"\u003e项目结构\u003c/h3\u003e\n\u003cp\u003e项目的主要结构包括：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eapp/main.py\u003c/code\u003e: FastAPI 应用入口，定义 API 路由和应用生命周期事件（如模型加载）。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eapp/models/sensevoice_loader.py\u003c/code\u003e: 负责加载 \u003ccode\u003eSenseVoiceSmall\u003c/code\u003e 模型，采用单例模式。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eapp/services/asr_service.py\u003c/code\u003e: 封装语音处理和模型推理的核心逻辑。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eapp/schemas.py\u003c/code\u003e: 定义 API 的请求和响应数据模型 (Pydantic models)。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"api-端点\"\u003eAPI 端点\u003c/h3\u003e\n\u003cp\u003e关键的 API 端点设计为：\u003c/p\u003e","title":"基于FunAudioLLM/SenseVoiceSmall搭建高效语音转录服务的实践之路"},{"content":"如何使用Qwen2.5-Omni实现文本转语音(TTS)和语音转文本(ASR) 项目概述 本项目基于Qwen2.5-Omni-7B模型，实现了两个核心功能：\n文本转语音（TTS）：将输入文本转换为自然流畅的语音 语音转文本（ASR）：将语音文件转换为文本，支持标准ASR和纯ASR两种模式 项目地址：https://github.com/jackypanster/qwen-omni\n环境配置 推荐使用conda管理Python环境，确保依赖安装的稳定性：\n# 创建并激活环境 conda create -n qwen-tts python=3.10 conda activate qwen-tts # 安装PyTorch（GPU版本） conda install pytorch=2.5.1 pytorch-cuda=12.1 -c pytorch -c nvidia conda install torchvision torchaudio -c pytorch # 安装其他依赖 conda install streamlit python-soundfile -c conda-forge pip install git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview pip install qwen-omni-utils 核心功能实现 1. 文本转语音（TTS） def text_to_speech(text_input, output_audio_path=\u0026#34;output/output.wav\u0026#34;, speaker=\u0026#34;Chelsie\u0026#34;): # 加载模型和处理器 model = Qwen2_5OmniForConditionalGeneration.from_pretrained( model_path, config=config, torch_dtype=\u0026#34;auto\u0026#34;, device_map=\u0026#34;auto\u0026#34; ) processor = Qwen2_5OmniProcessor.from_pretrained(model_path) # 构造对话 conversation = [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;You are Qwen...\u0026#34;}]}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: text_input}]} ] # 生成语音 with torch.no_grad(): text_ids, audio = model.generate( **inputs, speaker=speaker, do_sample=True, temperature=0.8, top_p=0.95, max_new_tokens=1024 ) 2. 语音转文本（ASR） def audio_to_text(audio_path: str) -\u0026gt; str: # 标准ASR模式 conversation = [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;你是Qwen...\u0026#34;}]}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;audio\u0026#34;, \u0026#34;audio\u0026#34;: audio_path}]} ] # 生成文本 with torch.no_grad(): text_ids = model.generate( **inputs, do_sample=False, max_new_tokens=1024, return_audio=False ) Web界面实现 使用Streamlit构建了简洁的Web界面：\n# 文本输入 text_input = st.text_area(\u0026#34;请输入要合成的文本：\u0026#34;, height=120, max_chars=200) # 发音人选择 speaker = st.selectbox(\u0026#34;请选择发音人：\u0026#34;, [\u0026#34;Chelsie\u0026#34;, \u0026#34;Ethan\u0026#34;], index=0) # 生成按钮 if st.button(\u0026#34;生成语音\u0026#34;): # 生成语音并播放 audio_path = os.path.join(OUTPUT_DIR, f\u0026#34;tts_{uuid.uuid4().hex}.wav\u0026#34;) text_to_speech(text_input, output_audio_path=audio_path, speaker=speaker) st.audio(audio_path, format=\u0026#34;audio/wav\u0026#34;) RESTful API实现 使用FastAPI构建了RESTful API接口：\n@app.post(\u0026#34;/tts\u0026#34;) async def tts(request: TTSRequest): audio_filename = f\u0026#34;tts_{uuid.uuid4().hex}.wav\u0026#34; audio_path = os.path.join(OUTPUT_DIR, audio_filename) text_to_speech(request.text, audio_path, request.speaker) return {\u0026#34;audio_url\u0026#34;: f\u0026#34;/output/{audio_filename}\u0026#34;} @app.post(\u0026#34;/asr\u0026#34;) async def asr(file: UploadFile = File(...)): # 处理上传的音频文件 audio_path = os.path.join(OUTPUT_DIR, f\u0026#34;asr_{uuid.uuid4().hex}.wav\u0026#34;) with open(audio_path, \u0026#34;wb\u0026#34;) as buffer: shutil.copyfileobj(file.file, buffer) text = audio_to_text(audio_path) return {\u0026#34;text\u0026#34;: text} 使用说明 启动Web界面： streamlit run app_text2audio.py 启动API服务： uvicorn fastapi_app:app --host 0.0.0.0 --port 8000 注意事项 模型文件较大，建议提前下载并配置好模型路径 使用conda安装依赖可以避免大多数环境问题 音频文件会保存在output目录下 API接口支持文件上传和文本转写 后续优化方向 支持更多发音人选项 优化模型加载速度 添加批量处理功能 支持更多音频格式 添加历史记录功能 参考资源 Qwen2.5-Omni-7B官方文档 Streamlit文档 FastAPI文档 ","permalink":"https://jackypanster.github.io/ai-stream/posts/how-to-use-qwen-omni-tts-asr/","summary":"\u003ch1 id=\"如何使用qwen25-omni实现文本转语音tts和语音转文本asr\"\u003e如何使用Qwen2.5-Omni实现文本转语音(TTS)和语音转文本(ASR)\u003c/h1\u003e\n\u003ch2 id=\"项目概述\"\u003e项目概述\u003c/h2\u003e\n\u003cp\u003e本项目基于Qwen2.5-Omni-7B模型，实现了两个核心功能：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e文本转语音（TTS）：将输入文本转换为自然流畅的语音\u003c/li\u003e\n\u003cli\u003e语音转文本（ASR）：将语音文件转换为文本，支持标准ASR和纯ASR两种模式\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e项目地址：\u003ca href=\"https://github.com/jackypanster/qwen-omni\"\u003ehttps://github.com/jackypanster/qwen-omni\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"环境配置\"\u003e环境配置\u003c/h2\u003e\n\u003cp\u003e推荐使用conda管理Python环境，确保依赖安装的稳定性：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 创建并激活环境\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econda create -n qwen-tts python\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e3.10\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econda activate qwen-tts\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 安装PyTorch（GPU版本）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econda install pytorch\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e2.5.1 pytorch-cuda\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e12.1 -c pytorch -c nvidia\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econda install torchvision torchaudio -c pytorch\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# 安装其他依赖\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econda install streamlit python-soundfile -c conda-forge\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip install git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip install qwen-omni-utils\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"核心功能实现\"\u003e核心功能实现\u003c/h2\u003e\n\u003ch3 id=\"1-文本转语音tts\"\u003e1. 文本转语音（TTS）\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etext_to_speech\u003c/span\u003e(text_input, output_audio_path\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;output/output.wav\u0026#34;\u003c/span\u003e, speaker\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Chelsie\u0026#34;\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 加载模型和处理器\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    model \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Qwen2_5OmniForConditionalGeneration\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efrom_pretrained(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        model_path, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003econfig, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        torch_dtype\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;auto\u0026#34;\u003c/span\u003e, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        device_map\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;auto\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    processor \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Qwen2_5OmniProcessor\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efrom_pretrained(model_path)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 构造对话\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    conversation \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;system\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e: [{\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;You are Qwen...\u0026#34;\u003c/span\u003e}]},\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;user\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e: [{\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e: text_input}]}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 生成语音\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ewith\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eno_grad():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        text_ids, audio \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egenerate(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003einputs,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            speaker\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003espeaker,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            do_sample\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            temperature\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.8\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            top_p\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.95\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            max_new_tokens\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1024\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        )\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"2-语音转文本asr\"\u003e2. 语音转文本（ASR）\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eaudio_to_text\u003c/span\u003e(audio_path: str) \u003cspan style=\"color:#f92672\"\u003e-\u0026gt;\u003c/span\u003e str:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 标准ASR模式\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    conversation \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;system\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e: [{\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;你是Qwen...\u0026#34;\u003c/span\u003e}]},\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;user\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e: [{\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;audio\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;audio\u0026#34;\u003c/span\u003e: audio_path}]}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 生成文本\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ewith\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eno_grad():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        text_ids \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egenerate(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003einputs,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            do_sample\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            max_new_tokens\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1024\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            return_audio\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        )\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"web界面实现\"\u003eWeb界面实现\u003c/h2\u003e\n\u003cp\u003e使用Streamlit构建了简洁的Web界面：\u003c/p\u003e","title":"如何使用Qwen2.5-Omni实现文本转语音(TTS)和语音转文本(ASR)"},{"content":"\n概述 本脚本基于 Qwen2.5-Omni-7B 多模态模型实现文本转语音（TTS）功能，支持生成自然流畅的中文 / 英文语音，并提供两种语音类型（女性 “Chelsie”、男性 “Ethan”）。脚本可将输入文本转换为音频文件（.wav格式），适用于语音助手、内容创作、无障碍服务等场景。\n主要特性 🎙️ 支持自然流畅的 中文/英文 语音合成 👥 提供两种语音类型选择： 女性声线：\u0026ldquo;Chelsie\u0026rdquo; 男性声线：\u0026ldquo;Ethan\u0026rdquo; 💾 输出格式：标准 .wav 音频文件 🚀 高性能推理，适用于生产环境 应用场景 智能语音助手开发 内容创作与播客制作 无障碍服务 教育类应用 多媒体内容生成 开始使用 💡 在开始之前，请确保您的系统满足以下要求：\nPython 3.8+ CUDA 11.7+ (如需GPU加速) 至少16GB可用内存 安装依赖库\nuv init uv add git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview uv add accelerate uv add qwen-omni-utils[decord] uv add soundfile uv add torchvision uv sync 完整脚本代码（main_text2audio.py）\nimport os import soundfile as sf import torch from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor from qwen_omni_utils import process_mm_info from transformers import AutoConfig def text_to_speech( text_input: str, output_audio_path: str = \u0026#34;output/test_audio.wav\u0026#34;, speaker: str = \u0026#34;Chelsie\u0026#34;, model_path: str = \u0026#34;/home/llm/model/qwen/Omni/\u0026#34; # 改为本地路径或远程路径 ): \u0026#34;\u0026#34;\u0026#34; 文本转语音核心函数 :param text_input: 输入文本（支持中文/英文） :param output_audio_path: 音频输出路径（含文件名） :param speaker: 语音类型（\u0026#34;Chelsie\u0026#34;女性/\u0026#34;Ethan\u0026#34;男性） :param model_path: 模型路径（本地/远程） \u0026#34;\u0026#34;\u0026#34; # 1. 加载模型配置（修复ROPE参数兼容性） config = AutoConfig.from_pretrained(model_path, local_files_only=True) if hasattr(config, \u0026#34;rope_scaling\u0026#34;) and \u0026#34;mrope_section\u0026#34; in config.rope_scaling: config.rope_scaling.pop(\u0026#34;mrope_section\u0026#34;) # 2. 加载模型（支持GPU自动分配） model = Qwen2_5OmniForConditionalGeneration.from_pretrained( model_path, config=config, torch_dtype=\u0026#34;auto\u0026#34;, device_map=\u0026#34;auto\u0026#34;, local_files_only=(model_path != \u0026#34;Qwen/Qwen2.5-Omni-7B\u0026#34;) ) processor = Qwen2_5OmniProcessor.from_pretrained(model_path) # 3. 系统提示（必须包含语音生成能力声明） system_prompt = [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\u0026#34;} ] } ] # 4. 构建对话（纯文本输入） conversation = system_prompt + [ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: text_input}]} ] # 5. 处理输入数据 text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) audios, images, videos = process_mm_info(conversation, use_audio_in_video=False) # 6. 生成语音 inputs = processor( text=text, audio=audios, images=images, videos=videos, return_tensors=\u0026#34;pt\u0026#34;, padding=True, use_audio_in_video=False ).to(model.device, model.dtype) with torch.no_grad(): text_ids, audio = model.generate( **inputs, speaker=speaker, do_sample=True, # 启用采样模式以使用temperature/top_p temperature=0.8, # 控制随机性（0.5-1.0较自然） top_p=0.95, # 核采样参数 max_new_tokens=1024, # 控制语音时长（约15秒） use_audio_in_video=False ) # 7. 保存结果 os.makedirs(os.path.dirname(output_audio_path), exist_ok=True) sf.write(output_audio_path, audio.reshape(-1).cpu().numpy(), samplerate=24000) print(f\u0026#34;✅ 生成完成：{output_audio_path}\u0026#34;) print(f\u0026#34;📄 生成文本：{processor.batch_decode(text_ids, skip_special_tokens=True)[0]}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: # 示例输入（可替换为任意文本） input_text = \u0026#34;你好，这是Qwen2.5-Omni的文本转语音示例。祝你使用愉快！\u0026#34; # 调用函数（指定输出路径和语音类型） text_to_speech( input_text, output_audio_path=\u0026#34;output/hello_qwen.wav\u0026#34;, speaker=\u0026#34;Chelsie\u0026#34; # 可选\u0026#34;Ethan\u0026#34; ) 运行脚本\nuv run main.py ","permalink":"https://jackypanster.github.io/ai-stream/posts/how-to-setup-qwen-omni/","summary":"\u003cp\u003e\u003cimg alt=\"Qwen2.5-Omni-7B TTS\" loading=\"lazy\" src=\"https://via.placeholder.com/800x400.png/007bff/ffffff?text=Qwen2.5-Omni-7B+TTS\"\u003e\u003c/p\u003e\n\u003ch2 id=\"概述\"\u003e概述\u003c/h2\u003e\n\u003cp\u003e本脚本基于 Qwen2.5-Omni-7B 多模态模型实现文本转语音（TTS）功能，支持生成自然流畅的中文 / 英文语音，并提供两种语音类型（女性 “Chelsie”、男性 “Ethan”）。脚本可将输入文本转换为音频文件（.wav格式），适用于语音助手、内容创作、无障碍服务等场景。\u003c/p\u003e\n\u003ch2 id=\"主要特性\"\u003e主要特性\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e🎙️ 支持自然流畅的 \u003cstrong\u003e中文/英文\u003c/strong\u003e 语音合成\u003c/li\u003e\n\u003cli\u003e👥 提供两种语音类型选择：\n\u003cul\u003e\n\u003cli\u003e女性声线：\u0026ldquo;Chelsie\u0026rdquo;\u003c/li\u003e\n\u003cli\u003e男性声线：\u0026ldquo;Ethan\u0026rdquo;\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e💾 输出格式：标准 \u003ccode\u003e.wav\u003c/code\u003e 音频文件\u003c/li\u003e\n\u003cli\u003e🚀 高性能推理，适用于生产环境\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"应用场景\"\u003e应用场景\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e智能语音助手开发\u003c/li\u003e\n\u003cli\u003e内容创作与播客制作\u003c/li\u003e\n\u003cli\u003e无障碍服务\u003c/li\u003e\n\u003cli\u003e教育类应用\u003c/li\u003e\n\u003cli\u003e多媒体内容生成\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"开始使用\"\u003e开始使用\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e💡 在开始之前，请确保您的系统满足以下要求：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePython 3.8+\u003c/li\u003e\n\u003cli\u003eCUDA 11.7+ (如需GPU加速)\u003c/li\u003e\n\u003cli\u003e至少16GB可用内存\u003c/li\u003e\n\u003c/ul\u003e\u003c/blockquote\u003e\n\u003cp\u003e安装依赖库\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv init\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv add git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv add accelerate\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv add qwen-omni-utils\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003edecord\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv add soundfile\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv add torchvision\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euv sync\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e完整脚本代码（main_text2audio.py）\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e os\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e soundfile \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e sf\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e torch\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e transformers \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e qwen_omni_utils \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e process_mm_info\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e transformers \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e AutoConfig\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003etext_to_speech\u003c/span\u003e(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    text_input: str,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    output_audio_path: str \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;output/test_audio.wav\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    speaker: str \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Chelsie\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    model_path: str \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;/home/llm/model/qwen/Omni/\u0026#34;\u003c/span\u003e  \u003cspan style=\"color:#75715e\"\u003e# 改为本地路径或远程路径\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    文本转语音核心函数\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    :param text_input: 输入文本（支持中文/英文）\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    :param output_audio_path: 音频输出路径（含文件名）\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    :param speaker: 语音类型（\u0026#34;Chelsie\u0026#34;女性/\u0026#34;Ethan\u0026#34;男性）\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    :param model_path: 模型路径（本地/远程）\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003e    \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 1. 加载模型配置（修复ROPE参数兼容性）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    config \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e AutoConfig\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efrom_pretrained(model_path, local_files_only\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e hasattr(config, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rope_scaling\u0026#34;\u003c/span\u003e) \u003cspan style=\"color:#f92672\"\u003eand\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;mrope_section\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e config\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erope_scaling:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erope_scaling\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epop(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;mrope_section\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 2. 加载模型（支持GPU自动分配）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    model \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Qwen2_5OmniForConditionalGeneration\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efrom_pretrained(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        model_path,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        config\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003econfig,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        torch_dtype\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;auto\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        device_map\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;auto\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        local_files_only\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e(model_path \u003cspan style=\"color:#f92672\"\u003e!=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Qwen/Qwen2.5-Omni-7B\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    processor \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Qwen2_5OmniProcessor\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003efrom_pretrained(model_path)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 3. 系统提示（必须包含语音生成能力声明）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    system_prompt \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;system\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e: [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\u0026#34;\u003c/span\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 4. 构建对话（纯文本输入）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    conversation \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e system_prompt \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e [\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;role\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;user\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e: [{\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;text\u0026#34;\u003c/span\u003e: text_input}]}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 5. 处理输入数据\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    text \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e processor\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eapply_chat_template(conversation, add_generation_prompt\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e, tokenize\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    audios, images, videos \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e process_mm_info(conversation, use_audio_in_video\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 6. 生成语音\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    inputs \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e processor(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        text\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003etext, audio\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eaudios, images\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eimages, videos\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003evideos,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        return_tensors\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;pt\u0026#34;\u003c/span\u003e, padding\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e, use_audio_in_video\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    )\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto(model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edevice, model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edtype)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ewith\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eno_grad():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        text_ids, audio \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egenerate(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003einputs,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            speaker\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003espeaker,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            do_sample\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e,  \u003cspan style=\"color:#75715e\"\u003e# 启用采样模式以使用temperature/top_p\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            temperature\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.8\u003c/span\u003e,  \u003cspan style=\"color:#75715e\"\u003e# 控制随机性（0.5-1.0较自然）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            top_p\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.95\u003c/span\u003e,       \u003cspan style=\"color:#75715e\"\u003e# 核采样参数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            max_new_tokens\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1024\u003c/span\u003e,  \u003cspan style=\"color:#75715e\"\u003e# 控制语音时长（约15秒）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            use_audio_in_video\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 7. 保存结果\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    os\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emakedirs(os\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epath\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edirname(output_audio_path), exist_ok\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    sf\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ewrite(output_audio_path, audio\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ereshape(\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecpu()\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enumpy(), samplerate\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e24000\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    print(\u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;✅ 生成完成：\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003eoutput_audio_path\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    print(\u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;📄 生成文本：\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003eprocessor\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebatch_decode(text_ids, skip_special_tokens\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e)[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e __name__ \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;__main__\u0026#34;\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 示例输入（可替换为任意文本）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    input_text \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;你好，这是Qwen2.5-Omni的文本转语音示例。祝你使用愉快！\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# 调用函数（指定输出路径和语音类型）\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    text_to_speech(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        input_text,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        output_audio_path\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;output/hello_qwen.wav\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        speaker\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Chelsie\u0026#34;\u003c/span\u003e  \u003cspan style=\"color:#75715e\"\u003e# 可选\u0026#34;Ethan\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    )\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e运行脚本\u003c/p\u003e","title":"Qwen2.5-Omni-7B 文本转语音部署指南"}]