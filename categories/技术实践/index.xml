<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>技术实践 on Code Whispers</title><link>https://jackypanster.github.io/ai-stream/categories/%E6%8A%80%E6%9C%AF%E5%AE%9E%E8%B7%B5/</link><description>Recent content in 技术实践 on Code Whispers</description><generator>Hugo -- 0.148.1</generator><language>zh-cn</language><lastBuildDate>Wed, 04 Jun 2025 14:30:00 +0800</lastBuildDate><atom:link href="https://jackypanster.github.io/ai-stream/categories/%E6%8A%80%E6%9C%AF%E5%AE%9E%E8%B7%B5/index.xml" rel="self" type="application/rss+xml"/><item><title>Qwen3-30B 技术优化实践（二）：思考模式控制与15-20%性能提升</title><link>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-part2/</link><pubDate>Wed, 04 Jun 2025 14:30:00 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-part2/</guid><description>&lt;h1 id="qwen3-30b-技术优化实践二思考模式控制与性能提升">Qwen3-30B 技术优化实践（二）：思考模式控制与性能提升&lt;/h1>
&lt;blockquote>
&lt;p>本文是&lt;a href="blog-post.md">《从32K到131K：Qwen3-30B大模型上下文扩展实践》&lt;/a>的续篇，聚焦于模型性能调优特别是思考模式（reasoning mode）控制的技术细节与实践经验。&lt;/p>&lt;/blockquote>
&lt;p>在前文中，我们详细介绍了如何使用YaRN技术将Qwen3-30B的上下文长度从32K扩展到131K。今天，我们将深入探讨另一个关键优化维度：&lt;strong>思考模式控制&lt;/strong>及其对性能的影响。通过一系列实验和调优，我们发现禁用思考模式可以显著提升模型响应速度和内存效率，特别适合编程和直接输出类任务场景。&lt;/p>
&lt;h2 id="-思考模式reasoning-mode解析">🔍 思考模式（Reasoning Mode）解析&lt;/h2>
&lt;h3 id="什么是思考模式">什么是思考模式？&lt;/h3>
&lt;p>思考模式（Reasoning Mode，也称为Thinking Mode）是Qwen3系列模型的一个特性，让模型能够生成中间思考步骤，这些步骤被包含在&lt;code>&amp;lt;think&amp;gt;...&amp;lt;/think&amp;gt;&lt;/code>标签内。理论上，这种&amp;quot;思考过程&amp;quot;有助于模型进行更复杂的推理，但同时也引入了额外的计算和内存开销。&lt;/p>
&lt;p>在默认配置下，Qwen3模型会启用思考模式，产生类似以下的输出：&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;think&amp;gt;
首先，我需要分析用户的问题：如何实现一个简单的文件读写功能。
我应该使用Python的内置文件操作功能。
基本步骤应该是：
1. 打开文件（可以使用with语句自动管理资源）
2. 读取或写入内容
3. 确保文件正确关闭
&amp;lt;/think&amp;gt;
以下是一个简单的Python文件读写示例：
```python
# 写入文件
with open(&amp;#39;example.txt&amp;#39;, &amp;#39;w&amp;#39;) as file:
file.write(&amp;#39;Hello, World!&amp;#39;)
# 读取文件
with open(&amp;#39;example.txt&amp;#39;, &amp;#39;r&amp;#39;) as file:
content = file.read()
print(content)
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>
### 思考模式实现机制
vLLM部署Qwen3模型时，思考模式通过两种方式实现控制：
1. **服务器级控制**：通过部署参数`--enable-reasoning`和`--reasoning-parser deepseek_r1`启用
2. **API级控制**：通过API调用中的`chat_template_kwargs`参数或`enable_thinking`参数动态控制
我们的发现是，**仅删除服务器级别的参数并不足够完全禁用思考模式**，模型在某些情况下仍会产生思考过程。更彻底的解决方案是使用自定义聊天模板。
## 💡 禁用思考模式的技术实现
### 自定义聊天模板方案
经过研究Qwen官方文档和实验，我们发现使用自定义聊天模板是完全禁用思考模式的最可靠方法。我们创建了一个名为`qwen3_nonthinking.jinja`的模板文件：
```jinja
{% if messages %}
{% set loop_messages = messages %}
{% else %}
{% set loop_messages = [{&amp;#39;role&amp;#39;: &amp;#39;system&amp;#39;, &amp;#39;content&amp;#39;: &amp;#39;&amp;#39;}] %}
{% endif %}
{% for message in loop_messages %}
{% if message[&amp;#39;role&amp;#39;] == &amp;#39;user&amp;#39; %}
&amp;lt;|im_start|&amp;gt;user
{{ message[&amp;#39;content&amp;#39;] }}&amp;lt;|im_end|&amp;gt;
{% elif message[&amp;#39;role&amp;#39;] == &amp;#39;assistant&amp;#39; %}
&amp;lt;|im_start|&amp;gt;assistant
{{ message[&amp;#39;content&amp;#39;] }}&amp;lt;|im_end|&amp;gt;
{% elif message[&amp;#39;role&amp;#39;] == &amp;#39;system&amp;#39; %}
&amp;lt;|im_start|&amp;gt;system
{{ message[&amp;#39;content&amp;#39;] }}&amp;lt;|im_end|&amp;gt;
{% endif %}
{% endfor %}
&amp;lt;|im_start|&amp;gt;assistant
{% if add_generation_prompt is defined and add_generation_prompt %}{{ generation_prompt }}{% endif %}
&lt;/code>&lt;/pre>&lt;p>这个模板的关键点是&lt;strong>移除了所有与思考模式相关的标签和处理逻辑&lt;/strong>，确保模型无法生成&lt;code>&amp;lt;think&amp;gt;...&amp;lt;/think&amp;gt;&lt;/code>块，即使API请求中尝试启用思考模式。&lt;/p></description></item><item><title>AI辅助编程实战：从需求到实现的完整对话记录</title><link>https://jackypanster.github.io/ai-stream/posts/coding-with-ai/</link><pubDate>Tue, 03 Jun 2025 13:28:50 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/coding-with-ai/</guid><description>&lt;h1 id="ai辅助编程实战从需求到实现的完整对话记录">AI辅助编程实战：从需求到实现的完整对话记录&lt;/h1>
&lt;h2 id="引言">引言&lt;/h2>
&lt;p>本文真实还原了我与AI助手协作实现Docker镜像大小监控功能的全过程。通过这次实践，可以看到AI不仅能辅助开发者梳理需求、设计方案，还能在代码实现、文档编写等环节提供高效帮助。整个流程自然流畅，极大提升了开发体验。&lt;/p>
&lt;h2 id="1-需求分析与方案设计">1. 需求分析与方案设计&lt;/h2>
&lt;h3 id="初始需求描述">初始需求描述&lt;/h3>
&lt;pre tabindex="0">&lt;code>用户：
我们需要在Jenkins共享库中增加Docker镜像大小监控功能，当镜像超过4GB时发出警告。请帮忙分析一下实现思路。
&lt;/code>&lt;/pre>&lt;h3 id="ai的初步分析与建议">AI的初步分析与建议&lt;/h3>
&lt;pre tabindex="0">&lt;code>AI：
让我先看看项目的结构……
这是一个Jenkins共享库项目，核心代码都在`vars/`目录。针对镜像大小监控，我建议直接在`buildDocker`方法里加逻辑：
1. 镜像构建完成后用`du`命令获取大小
2. 与4GB阈值比较
3. 超过阈值就输出警告
4. 相关指标写日志，方便后续分析
这个方案您觉得合适吗？有无补充？
&lt;/code>&lt;/pre>&lt;h3 id="需求确认与细化">需求确认与细化&lt;/h3>
&lt;pre tabindex="0">&lt;code>用户：
这个思路可以，不过还需要：
1. 阈值要可配置，默认4GB
2. 日志里要有镜像名、大小、时间戳等详细信息
3. 输出优化建议
另外，异常处理也要考虑得更优雅些。
&lt;/code>&lt;/pre>&lt;h2 id="2-代码实现">2. 代码实现&lt;/h2>
&lt;h3 id="初始代码实现">初始代码实现&lt;/h3>
&lt;pre tabindex="0">&lt;code>AI：
明白，我来实现。会在`buildDocker`方法里：
1. 用`getEnvOrDefault`支持阈值配置
2. 详细记录指标
3. 加强错误处理
4. 输出优化建议
实现逻辑您看还有什么要调整的吗？
&lt;/code>&lt;/pre>&lt;h3 id="代码审查与优化">代码审查与优化&lt;/h3>
&lt;pre tabindex="0">&lt;code>用户：
整体不错，不过警告要改成抛异常直接中断构建。另外，能不能顺便加上基础镜像大小检测？
&lt;/code>&lt;/pre>&lt;h3 id="异常处理优化">异常处理优化&lt;/h3>
&lt;pre tabindex="0">&lt;code>AI：
明白您的需求，代码我会这样优化：
1. 超标直接抛异常中断流水线
2. 增加基础镜像大小检测
3. 明确区分业务异常（比如镜像过大）和技术异常（比如命令失败）
基础镜像检测这块，我建议：
- 解析Dockerfile的FROM
- 拉取并检测基础镜像大小
- 超限就给出alpine等轻量镜像建议
这样处理您觉得OK吗？
&lt;/code>&lt;/pre>&lt;h2 id="3-测试与验证">3. 测试与验证&lt;/h2>
&lt;h3 id="测试方案讨论">测试方案讨论&lt;/h3>
&lt;pre tabindex="0">&lt;code>用户：
怎么验证这个功能？
AI：
建议这样测试：
1. 镜像大小正常
2. 恰好等于阈值
3. 超过阈值
4. Dockerfile解析异常
也可以临时把阈值调小，快速验证告警逻辑。
&lt;/code>&lt;/pre>&lt;h2 id="4-文档与沟通">4. 文档与沟通&lt;/h2>
&lt;h3 id="变更日志生成">变更日志生成&lt;/h3>
&lt;pre tabindex="0">&lt;code>AI：
我已为您生成详细CHANGELOG，包括：
- 功能说明
- 技术实现细节
- 配置参数说明
- 后续优化计划
如需调整，随时告诉我。
&lt;/code>&lt;/pre>&lt;h3 id="团队通知">团队通知&lt;/h3>
&lt;pre tabindex="0">&lt;code>用户：
要发个通知给团队，说明我们要限制镜像大小。
AI：
建议分阶段推进：
1. 先监控和通知
2. 再进入警告期
3. 最后强制限制
这样大家有缓冲期，可以逐步优化Dockerfile。需要帮忙起草通知吗？
&lt;/code>&lt;/pre>&lt;h2 id="5-经验总结">5. 经验总结&lt;/h2>
&lt;h3 id="技术收获">技术收获&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>AI辅助效率&lt;/strong>&lt;/p></description></item></channel></rss>