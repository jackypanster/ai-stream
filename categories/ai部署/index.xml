<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>AI部署 on Code Whispers</title><link>https://jackypanster.github.io/ai-stream/categories/ai%E9%83%A8%E7%BD%B2/</link><description>Recent content in AI部署 on Code Whispers</description><generator>Hugo -- 0.148.1</generator><language>zh-cn</language><lastBuildDate>Sat, 07 Jun 2025 17:50:00 +0800</lastBuildDate><atom:link href="https://jackypanster.github.io/ai-stream/categories/ai%E9%83%A8%E7%BD%B2/index.xml" rel="self" type="application/rss+xml"/><item><title>DeepSeek-R1-0528-Qwen3-8B部署优化实践</title><link>https://jackypanster.github.io/ai-stream/posts/deploy-deepseek-r1-qwen3-8b-optimization/</link><pubDate>Sat, 07 Jun 2025 17:50:00 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/deploy-deepseek-r1-qwen3-8b-optimization/</guid><description>&lt;h1 id="deepseek-r1-0528-qwen3-8b部署优化实践性能与稳定性的平衡艺术">DeepSeek-R1-0528-Qwen3-8B部署优化实践：性能与稳定性的平衡艺术&lt;/h1>
&lt;p>在AI大模型部署领域，本文详细记录对DeepSeek-R1-0528-Qwen3-8B模型使用vLLM进行部署优化的全过程，重点关注上下文窗口长度与硬件资源利用的平衡调优。&lt;/p>
&lt;h2 id="环境与基础设施">环境与基础设施&lt;/h2>
&lt;p>我们的部署环境具备以下配置：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>GPU&lt;/strong>: 4 x NVIDIA RTX 2080 Ti（每张22GB显存，总计88GB显存）
&lt;ul>
&lt;li>架构: Turing&lt;/li>
&lt;li>计算能力: 7.5&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>CPU&lt;/strong>: 56核&lt;/li>
&lt;li>&lt;strong>内存&lt;/strong>: 512GB RAM&lt;/li>
&lt;li>&lt;strong>存储&lt;/strong>: 2TB SSD&lt;/li>
&lt;li>&lt;strong>操作系统&lt;/strong>: Ubuntu 24.04&lt;/li>
&lt;li>&lt;strong>容器镜像&lt;/strong>: &lt;code>vllm/vllm-openai:v0.8.5&lt;/code>&lt;/li>
&lt;li>&lt;strong>NVIDIA驱动&lt;/strong>: 570.153.02（CUDA 12.8）&lt;/li>
&lt;/ul>
&lt;h2 id="优化前的部署脚本分析">优化前的部署脚本分析&lt;/h2>
&lt;p>我们最初的部署脚本如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -d &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --gpus all &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --name coder &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --shm-size 16g &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ulimit memlock&lt;span style="color:#f92672">=&lt;/span>-1 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --restart always &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ipc&lt;span style="color:#f92672">=&lt;/span>host &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -p 8000:8000 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -e CUDA_MODULE_LOADING&lt;span style="color:#f92672">=&lt;/span>LAZY &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> vllm/vllm-openai:v0.8.5 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --model /models &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --served-model-name coder &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --tensor-parallel-size &lt;span style="color:#ae81ff">4&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --gpu-memory-utilization 0.93 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --dtype float16 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --max-model-len &lt;span style="color:#ae81ff">65536&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --trust-remote-code &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --load-format safetensors &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --disable-custom-all-reduce
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>通过分析，我们发现几个可以优化的关键点：&lt;/p></description></item><item><title>Qwen3-30B 技术优化实践（二）：思考模式控制与15-20%性能提升</title><link>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-part2/</link><pubDate>Wed, 04 Jun 2025 14:30:00 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3-part2/</guid><description>&lt;h1 id="qwen3-30b-技术优化实践二思考模式控制与性能提升">Qwen3-30B 技术优化实践（二）：思考模式控制与性能提升&lt;/h1>
&lt;blockquote>
&lt;p>本文是&lt;a href="blog-post.md">《从32K到131K：Qwen3-30B大模型上下文扩展实践》&lt;/a>的续篇，聚焦于模型性能调优特别是思考模式（reasoning mode）控制的技术细节与实践经验。&lt;/p>&lt;/blockquote>
&lt;p>在前文中，我们详细介绍了如何使用YaRN技术将Qwen3-30B的上下文长度从32K扩展到131K。今天，我们将深入探讨另一个关键优化维度：&lt;strong>思考模式控制&lt;/strong>及其对性能的影响。通过一系列实验和调优，我们发现禁用思考模式可以显著提升模型响应速度和内存效率，特别适合编程和直接输出类任务场景。&lt;/p>
&lt;h2 id="-思考模式reasoning-mode解析">🔍 思考模式（Reasoning Mode）解析&lt;/h2>
&lt;h3 id="什么是思考模式">什么是思考模式？&lt;/h3>
&lt;p>思考模式（Reasoning Mode，也称为Thinking Mode）是Qwen3系列模型的一个特性，让模型能够生成中间思考步骤，这些步骤被包含在&lt;code>&amp;lt;think&amp;gt;...&amp;lt;/think&amp;gt;&lt;/code>标签内。理论上，这种&amp;quot;思考过程&amp;quot;有助于模型进行更复杂的推理，但同时也引入了额外的计算和内存开销。&lt;/p>
&lt;p>在默认配置下，Qwen3模型会启用思考模式，产生类似以下的输出：&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;think&amp;gt;
首先，我需要分析用户的问题：如何实现一个简单的文件读写功能。
我应该使用Python的内置文件操作功能。
基本步骤应该是：
1. 打开文件（可以使用with语句自动管理资源）
2. 读取或写入内容
3. 确保文件正确关闭
&amp;lt;/think&amp;gt;
以下是一个简单的Python文件读写示例：
```python
# 写入文件
with open(&amp;#39;example.txt&amp;#39;, &amp;#39;w&amp;#39;) as file:
file.write(&amp;#39;Hello, World!&amp;#39;)
# 读取文件
with open(&amp;#39;example.txt&amp;#39;, &amp;#39;r&amp;#39;) as file:
content = file.read()
print(content)
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>
### 思考模式实现机制
vLLM部署Qwen3模型时，思考模式通过两种方式实现控制：
1. **服务器级控制**：通过部署参数`--enable-reasoning`和`--reasoning-parser deepseek_r1`启用
2. **API级控制**：通过API调用中的`chat_template_kwargs`参数或`enable_thinking`参数动态控制
我们的发现是，**仅删除服务器级别的参数并不足够完全禁用思考模式**，模型在某些情况下仍会产生思考过程。更彻底的解决方案是使用自定义聊天模板。
## 💡 禁用思考模式的技术实现
### 自定义聊天模板方案
经过研究Qwen官方文档和实验，我们发现使用自定义聊天模板是完全禁用思考模式的最可靠方法。我们创建了一个名为`qwen3_nonthinking.jinja`的模板文件：
```jinja
{% if messages %}
{% set loop_messages = messages %}
{% else %}
{% set loop_messages = [{&amp;#39;role&amp;#39;: &amp;#39;system&amp;#39;, &amp;#39;content&amp;#39;: &amp;#39;&amp;#39;}] %}
{% endif %}
{% for message in loop_messages %}
{% if message[&amp;#39;role&amp;#39;] == &amp;#39;user&amp;#39; %}
&amp;lt;|im_start|&amp;gt;user
{{ message[&amp;#39;content&amp;#39;] }}&amp;lt;|im_end|&amp;gt;
{% elif message[&amp;#39;role&amp;#39;] == &amp;#39;assistant&amp;#39; %}
&amp;lt;|im_start|&amp;gt;assistant
{{ message[&amp;#39;content&amp;#39;] }}&amp;lt;|im_end|&amp;gt;
{% elif message[&amp;#39;role&amp;#39;] == &amp;#39;system&amp;#39; %}
&amp;lt;|im_start|&amp;gt;system
{{ message[&amp;#39;content&amp;#39;] }}&amp;lt;|im_end|&amp;gt;
{% endif %}
{% endfor %}
&amp;lt;|im_start|&amp;gt;assistant
{% if add_generation_prompt is defined and add_generation_prompt %}{{ generation_prompt }}{% endif %}
&lt;/code>&lt;/pre>&lt;p>这个模板的关键点是&lt;strong>移除了所有与思考模式相关的标签和处理逻辑&lt;/strong>，确保模型无法生成&lt;code>&amp;lt;think&amp;gt;...&amp;lt;/think&amp;gt;&lt;/code>块，即使API请求中尝试启用思考模式。&lt;/p></description></item><item><title>高性能部署Qwen3-30B：vLLM优化实践指南</title><link>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3/</link><pubDate>Tue, 03 Jun 2025 16:00:00 +0800</pubDate><guid>https://jackypanster.github.io/ai-stream/posts/deploy-qwen3/</guid><description>&lt;h1 id="高性能部署qwen3-30bvllm优化实践指南">高性能部署Qwen3-30B：vLLM优化实践指南&lt;/h1>
&lt;h2 id="-概述">📋 概述&lt;/h2>
&lt;p>本文详细介绍如何使用vLLM高效部署Qwen3-30B-A3B模型，实现32K上下文窗口和OpenAI兼容API，适用于生产环境。通过精细调整部署参数，我们能够在有限的GPU资源下最大化模型性能。&lt;/p>
&lt;h2 id="-系统要求">🖥️ 系统要求&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>硬件配置&lt;/strong>
&lt;ul>
&lt;li>4块NVIDIA GPU (每块22GB显存，总计88GB)&lt;/li>
&lt;li>512GB系统内存&lt;/li>
&lt;li>2TB SSD存储&lt;/li>
&lt;li>56核CPU&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>软件环境&lt;/strong>
&lt;ul>
&lt;li>Ubuntu 24.04&lt;/li>
&lt;li>NVIDIA驱动 550.144.03&lt;/li>
&lt;li>CUDA 12.4&lt;/li>
&lt;li>Docker + NVIDIA Container Toolkit&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="-模型与架构">🧠 模型与架构&lt;/h2>
&lt;p>Qwen3-30B-A3B是阿里云发布的通用大语言模型，具有以下特点：&lt;/p>
&lt;ul>
&lt;li>30B参数量&lt;/li>
&lt;li>原生支持32K上下文长度&lt;/li>
&lt;li>支持思考模式(Chain-of-Thought)&lt;/li>
&lt;li>优异的多语言与代码能力&lt;/li>
&lt;/ul>
&lt;p>我们使用vLLM作为推理引擎，主要基于以下考量：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>高效内存管理&lt;/strong>：通过PagedAttention技术优化KV缓存&lt;/li>
&lt;li>&lt;strong>张量并行&lt;/strong>：自动跨多GPU分布模型权重&lt;/li>
&lt;li>&lt;strong>OpenAI兼容API&lt;/strong>：直接替代OpenAI API，无需修改现有应用&lt;/li>
&lt;li>&lt;strong>动态批处理&lt;/strong>：自动批处理多请求，提高吞吐量&lt;/li>
&lt;/ol>
&lt;h2 id="-部署脚本">🐳 部署脚本&lt;/h2>
&lt;p>以下是我们用于部署的Docker命令，经过精心调优以平衡性能与资源利用：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run -d &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --runtime&lt;span style="color:#f92672">=&lt;/span>nvidia &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --gpus&lt;span style="color:#f92672">=&lt;/span>all &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --name coder &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -v /home/llm/model/qwen/qwen3-30b-a3b:/qwen/qwen3-30b-a3b &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -p 8000:8000 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --cpuset-cpus 0-55 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ulimit memlock&lt;span style="color:#f92672">=&lt;/span>-1 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ulimit stack&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">67108864&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --restart always &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --ipc&lt;span style="color:#f92672">=&lt;/span>host &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> vllm/vllm-openai:v0.8.5 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --model /qwen/qwen3-30b-a3b &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --served-model-name coder &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --tensor-parallel-size &lt;span style="color:#ae81ff">4&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --dtype half &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --max-model-len &lt;span style="color:#ae81ff">32768&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --max-num-batched-tokens &lt;span style="color:#ae81ff">4096&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --gpu-memory-utilization 0.93 &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --block-size &lt;span style="color:#ae81ff">32&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --enable-chunked-prefill &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --swap-space &lt;span style="color:#ae81ff">16&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --tokenizer-pool-size &lt;span style="color:#ae81ff">56&lt;/span> &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> --disable-custom-all-reduce
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="-参数详解与优化策略">🔧 参数详解与优化策略&lt;/h2>
&lt;h3 id="docker容器配置">Docker容器配置&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>参数&lt;/th>
&lt;th>值&lt;/th>
&lt;th>作用&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>--runtime=nvidia&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;td>启用NVIDIA容器运行时&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>--gpus=all&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;td>将所有GPU暴露给容器&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>--cpuset-cpus&lt;/code>&lt;/td>
&lt;td>&lt;code>0-55&lt;/code>&lt;/td>
&lt;td>限制容器使用0-55号CPU核心&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>--ulimit memlock=-1&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;td>移除内存锁定限制，提高性能&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>--ipc=host&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;td>使用主机IPC命名空间，对共享内存很重要&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="vllm引擎配置">vLLM引擎配置&lt;/h3>
&lt;h4 id="1-张量并行策略">1. 张量并行策略&lt;/h4>
&lt;pre tabindex="0">&lt;code>--tensor-parallel-size 4
&lt;/code>&lt;/pre>&lt;p>我们使用4路张量并行，将模型分布在4块GPU上。这是基于实验得出的最佳配置 - 在我们的硬件上，每块22GB显存的GPU无法单独加载完整的30B模型。&lt;/p></description></item></channel></rss>